{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/10000], Loss: 798.9494\n",
      "Epoch [2000/10000], Loss: 733.3624\n",
      "Epoch [3000/10000], Loss: 509.8066\n",
      "Epoch [4000/10000], Loss: 376.6555\n",
      "Epoch [5000/10000], Loss: 311.6450\n",
      "Epoch [6000/10000], Loss: 356.1513\n",
      "Epoch [7000/10000], Loss: 336.5067\n",
      "Epoch [8000/10000], Loss: 341.1594\n",
      "Epoch [9000/10000], Loss: 303.1803\n",
      "Epoch [10000/10000], Loss: 299.5796\n",
      "Input: [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "Output: [49, 49, 6, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DNC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, memory_size, memory_vector_dim, num_read_heads=1):\n",
    "        super(DNC, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.memory_size = memory_size\n",
    "        self.memory_vector_dim = memory_vector_dim\n",
    "        self.num_read_heads = num_read_heads\n",
    "\n",
    "        self.controller = nn.LSTMCell(input_size + num_read_heads * memory_vector_dim, hidden_size)\n",
    "        self.memory = nn.Parameter(torch.zeros(memory_size, memory_vector_dim))\n",
    "        self.read_heads = nn.ModuleList([nn.Linear(hidden_size, memory_size) for _ in range(num_read_heads)])\n",
    "        self.write_head = nn.Linear(hidden_size, memory_size)\n",
    "        self.erase_head = nn.Linear(hidden_size, memory_vector_dim)\n",
    "        self.add_head = nn.Linear(hidden_size, memory_vector_dim)\n",
    "        self.output = nn.Linear(hidden_size + num_read_heads * memory_vector_dim, input_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "\n",
    "        if hidden is None:\n",
    "            h = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "            c = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "            hidden = (h, c)\n",
    "\n",
    "        outputs = []\n",
    "        read_vectors = [torch.zeros(batch_size, self.memory_vector_dim, device=x.device) for _ in range(self.num_read_heads)]\n",
    "\n",
    "        for i in range(seq_length):\n",
    "            xi = x[:, i, :]\n",
    "            controller_input = torch.cat([xi] + read_vectors, dim=1)\n",
    "            hidden = self.controller(controller_input, hidden)\n",
    "            h, c = hidden\n",
    "\n",
    "            read_weights = [torch.softmax(head(h), dim=1) for head in self.read_heads]\n",
    "            read_vectors = [torch.bmm(w.unsqueeze(1), self.memory.expand(batch_size, -1, -1)).squeeze(1) for w in read_weights]\n",
    "\n",
    "            write_weights = torch.softmax(self.write_head(h), dim=1)\n",
    "            erase_vector = torch.sigmoid(self.erase_head(h))\n",
    "            add_vector = self.add_head(h)\n",
    "\n",
    "            erase = torch.bmm(write_weights.unsqueeze(2), erase_vector.unsqueeze(1))\n",
    "            add = torch.bmm(write_weights.unsqueeze(2), add_vector.unsqueeze(1))\n",
    "            self.memory.data = self.memory * (1 - erase.mean(0)) + add.mean(0)\n",
    "\n",
    "            output = self.output(torch.cat([h] + read_vectors, dim=1))\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "memory_size = 128\n",
    "memory_vector_dim = 32\n",
    "num_read_heads = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "seq_length = 5\n",
    "\n",
    "# Create the model\n",
    "model = DNC(input_size, hidden_size, memory_size, memory_vector_dim, num_read_heads)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Generate a batch of random lists of integers\n",
    "    input_batch = torch.randint(1, 101, (batch_size, seq_length, input_size)).float()\n",
    "    target_batch = input_batch.flip(1)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "test_input = torch.tensor([[[1], [2], [3], [4], [5]]]).float()\n",
    "test_output = model(test_input)\n",
    "print(\"Input:\", test_input.squeeze().tolist())\n",
    "print(\"Output:\", [round(x) for x in test_output.squeeze().tolist()])\n",
    "\n",
    "\n",
    "\n",
    "# Generate 100 lists of 5 random integers\n",
    "# data = []\n",
    "# for _ in range(10):\n",
    "#     data.append(torch.tensor([[random.randint(1,100)] for _ in range(5)], dtype=torch.int64))\n",
    "\n",
    "# targets = [item.flip(0) for item in data]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
