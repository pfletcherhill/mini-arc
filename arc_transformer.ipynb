{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.train import ARCModelState, ARCTrainParams\n",
    "from arc_prize.vis import visualize_epochs\n",
    "import modal\n",
    "import torch\n",
    "import petname\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.data import ARCDatasetParams, ReARCDataset, make_re_arc_data_loaders\n",
    "\n",
    "config = ARCDatasetParams(max_grid_size=30, max_train_grids=10, color_offset=1)\n",
    "dataset = ReARCDataset(\"data/re_arc/ff805c23.json\", config)\n",
    "train_loader, val_loader = make_re_arc_data_loaders([\"data/re_arc/ff805c23.json\"], 10, config)\n",
    "len(val_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from arc_prize.train import ARCTrainParams\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "import petname\n",
    "\n",
    "model_type = \"encoder_decoder\"\n",
    "\n",
    "\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=12,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=16,\n",
    "  num_decoder_layers=16,\n",
    "  num_heads=16,\n",
    "  d_model=512,\n",
    "  d_ff=512*6,\n",
    "  dropout=0.2\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=12,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/chunked/html_dim_12_20241108\", \"/vol/data/chunked/re_arc_dim_12\", \"/vol/data/chunked/barc_1_dim_12\", \"/vol/data/chunked/barc_2_dim_12\"],\n",
    "  loss_class_weights={0: 0.2},\n",
    "  train_steps_per_epoch=1000,\n",
    "  eval_steps_per_epoch=150,\n",
    "  warmup_epochs=8,\n",
    "  refinement_ratio=0.25\n",
    ")\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "model_names = []\n",
    "\n",
    "num_runs = 1\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"train\")\n",
    "for i in range(num_runs):\n",
    "  model_name = petname.generate(words=3, separator='_')\n",
    "  fn_call = fn.spawn(model_name, num_epochs, model_type, model_params, train_params)\n",
    "  print(\"Model name\", model_name, model_type, fn_call.object_id)\n",
    "  model_names.append(model_name)\n",
    "\n",
    "print(model_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from arc_prize.train import ARCTrainParams, train_on_mac\n",
    "import petname\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "\n",
    "\n",
    "model_type = \"vision_encoder\"\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=20,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=2,\n",
    "  num_decoder_layers=0,\n",
    "  num_heads=2,\n",
    "  d_model=16,\n",
    "  d_ff=16*2,\n",
    "  dropout=0.1\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=12,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/Users/pfh/work/arc-data/chunked/html_dim_12_20241023\"],\n",
    "  train_steps_per_epoch=3,\n",
    "  eval_steps_per_epoch=2,\n",
    ")\n",
    "\n",
    "num_epochs = 4\n",
    "\n",
    "model_name = petname.generate(words=3, separator='_')  \n",
    "print(model_name)\n",
    "train_on_mac(model_name, num_epochs, model_type, model_params, train_params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"vision\"\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=12,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=1,\n",
    "  num_decoder_layers=1,\n",
    "  num_heads=4,\n",
    "  d_model=32,\n",
    "  d_ff=32*4,\n",
    "  dropout=0.2\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=32,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/move_random_small\", \"/vol/data/flip\"],\n",
    "  loss_class_weights={0: 0.2},\n",
    "  meta_num_epochs=2,\n",
    "  meta_batch_size=10,\n",
    "  meta_learning_rate=1e-4,\n",
    "  meta_weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "model_names = []\n",
    "\n",
    "num_runs = 1\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize-meta\", \"meta_train\")\n",
    "for i in range(num_runs):\n",
    "  model_name = petname.generate(words=3, separator='_')\n",
    "  fn_call = fn.spawn(model_name, num_epochs, model_type, model_params, train_params)\n",
    "  print(\"Model name\", model_name, model_type, fn_call.object_id)\n",
    "  model_names.append(model_name)\n",
    "\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from arc_prize.train import ARCTrainParams\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "# model_names = ['subtly_moral_bee', 'newly_mint_kite']\n",
    "# model_names = [\"vastly_intent_frog_2\"]\n",
    "model_names = [\"vastly_intent_frog_2\"]\n",
    "\n",
    "model_type = \"vision_encoder\"\n",
    "\n",
    "# subtly_full_bull\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=8,\n",
    "  learning_rate=1e-5,\n",
    "  weight_decay=1e-5,\n",
    "  dataset_dir=[\"/vol/data/chunked/html_dim_20_20241103\", \"/vol/data/chunked/re_arc_dim_20\", \"/vol/data/chunked/barc_1_dim_20\", \"/vol/data/chunked/barc_2_dim_20\"],\n",
    "  loss_class_weights={0: 0.2},\n",
    "  train_steps_per_epoch=1000,\n",
    "  eval_steps_per_epoch=100,\n",
    "  warmup_epochs=10,\n",
    "  refinement_ratio=0.25\n",
    ")\n",
    "\n",
    "# kindly_exact_beagle\n",
    "# train_params = ARCTrainParams(\n",
    "#   batch_size=16,\n",
    "#   learning_rate=1e-5,\n",
    "#   weight_decay=1e-5,\n",
    "#   dataset_dir=[\"/vol/data/chunked/html_dim_12_20241108\", \"/vol/data/chunked/re_arc_dim_12\", \"/vol/data/chunked/barc_1_dim_12\", \"/vol/data/chunked/barc_2_dim_12\"],\n",
    "#   loss_class_weights={0: 0.2},\n",
    "#   train_steps_per_epoch=1000,\n",
    "#   eval_steps_per_epoch=200,\n",
    "#   warmup_epochs=8,\n",
    "#   refinement_ratio=0.25\n",
    "# )\n",
    "\n",
    "# train_params = ARCTrainParams(\n",
    "#   batch_size=64,\n",
    "#   learning_rate=5e-5,\n",
    "#   weight_decay=1e-4,\n",
    "#   dataset_dir=[\"/vol/data/chunked/html_dim_12_20241108\", \"/vol/data/chunked/re_arc_dim_12\", \"/vol/data/chunked/barc_1_dim_12\", \"/vol/data/chunked/barc_2_dim_12\"],\n",
    "#   loss_class_weights={0: 0.2},\n",
    "#   train_steps_per_epoch=1000,\n",
    "#   eval_steps_per_epoch=150,\n",
    "#   warmup_epochs=8,\n",
    "#   refinement_ratio=0.25\n",
    "# )\n",
    "# train_params = None\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"train_80gb\")\n",
    "for model_name in model_names:\n",
    "  fn_call = fn.spawn(model_name, num_epochs, model_type, None, train_params)\n",
    "  print(\"Model name\", model_name, fn_call.object_id)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from arc_prize.train import ARCModelState, EpochState\n",
    "from arc_prize.vis import visualize_epochs\n",
    "\n",
    "def visualize_group(model_names: list[str]):\n",
    "  epochs = {}\n",
    "  get_model = modal.Function.lookup(\"arc-prize\", \"get_model\")\n",
    "  for name in model_names:\n",
    "    checkpoint_dict = get_model.remote(name)\n",
    "    # checkpoint = ARCModelState(**get_model.remote(name))\n",
    "    print(name, checkpoint_dict.get(\"model_type\", None), checkpoint_dict[\"best_val_loss\"], len(checkpoint_dict[\"epochs\"]), checkpoint_dict[\"epochs\"][-1], checkpoint_dict[\"model_params\"])\n",
    "    print(checkpoint_dict[\"train_params\"])\n",
    "    epochs[name] = checkpoint_dict[\"epochs\"]\n",
    "\n",
    "\n",
    "    # print(len(checkpoint.encoder_attn_weights))\n",
    "    # for b, batch in enumerate(checkpoint.encoder_attn_weights):\n",
    "    #   for i, layer in enumerate(batch):\n",
    "    #     visualize_all_heads(layer, title=f\"Batch {b}, layer {i}\")\n",
    "    \n",
    "\n",
    "  visualize_epochs(epochs)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "groups = [\n",
    "  # ['kindly_huge_jennet', 'lovely_tidy_lab', 'solely_living_leech'], # BEST\n",
    "  # ['weekly_enough_moose', 'gently_known_beagle', 'nicely_robust_rhino'], # 20x20 too slow\n",
    "  # ['wildly_firm_husky', 'surely_brief_bug', 'fully_better_dodo'], # Amazing\n",
    "  # ['wildly_steady_iguana', 'yearly_smart_donkey', 'mainly_polite_bison'], # Includes scale dataset\n",
    "  # ['partly_vocal_piglet', 'neatly_needed_liger', 'firmly_game_weevil'], # Scale and diagonal\n",
    "  # ['wholly_tops_heron', 'solely_eager_foal', 'deeply_one_skink'], # Tons of data\n",
    "  # ['unduly_glad_swift', 'purely_steady_hornet', 'humbly_civil_donkey'], # Basic data\n",
    "  # [\"early_civil_beetle\"],\n",
    "  # [\"really_fancy_kitten\", \"mildly_humble_tahr\"],\n",
    "  # ['solely_brief_shad', 'fairly_amazed_hyena', 'vastly_amazed_bobcat'],\n",
    "  # ['barely_sound_viper', 'lively_key_goblin', 'wildly_fancy_glider'], # patch size 3\n",
    "  # ['mildly_able_horse', 'vastly_normal_rhino', 'oddly_mint_clam'], # patch size 2\n",
    "  # ['firmly_tops_adder', 'yearly_normal_puma', 'slowly_more_caiman'], # patch size 2, simpler embedding\n",
    "  # ['safely_poetic_adder', 'vastly_close_horse', 'fairly_legal_insect', 'daily_actual_monkey'], # Patch embedding with ARC pos encoding\n",
    "  # ['nicely_wired_mouse', 'freely_up_shrimp', 'hardly_loving_mullet', 'gladly_active_muskox']\n",
    "  # ['mostly_normal_dog', 'lively_pure_hawk', 'rarely_tender_roughy'], # HUGE dataset re_arc_dim_12\n",
    "  # ['daily_pro_cattle', 'newly_suited_finch', 'rarely_tender_roughy'], # 32 batch size, 64 dim\n",
    "  # ['solely_sound_sponge', 'lively_sacred_egret'], # 32 batch size, 128 dim, 3+3 layers\n",
    "  # ['nicely_pure_leech', 'namely_sure_emu'], # 128 dim, 4+4 layers\n",
    "  # ['subtly_moral_bee', 'newly_mint_kite', 'solely_busy_skunk'], # Vision transformer large models\n",
    "  # ['newly_mint_kite', 'namely_caring_mite', 'overly_enough_tomcat', 'subtly_known_panda', 'mildly_ruling_hog', 'simply_tight_fowl'],\n",
    "  # ['subtly_known_panda', 'barely_clean_cicada', 'sadly_real_viper'], # 512 dim (12x12, 20x20, 30x30)\n",
    "  # ['lively_fleet_goat'], # vision 20x20\n",
    "  # [\"kindly_living_spider\", \"gladly_prompt_koi\"] # encoder vs normal\n",
    "  # [\"kindly_living_spider\", \"unduly_worthy_zebra\", \"daily_useful_squid\", \"vastly_intent_frog\"],\n",
    "  # [\"kindly_exact_beagle\", \"jolly_picked_stud\", \"badly_moral_pika\"],\n",
    "  # [\"kindly_exact_beagle\", \"jolly_picked_stud\", \"badly_moral_pika\", \"weekly_needed_buck\", \"lively_key_ape\", \"purely_ready_burro\"]\n",
    "  [\"vastly_intent_frog_2\", \"kindly_exact_beagle\", \"jolly_picked_stud\"]\n",
    "]\n",
    "\n",
    "# print([group for sublist in groups for group in sublist])\n",
    "for group in groups:\n",
    "  visualize_group(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "\n",
    "\n",
    "eval_model = modal.Function.lookup(\"arc-eval\", \"evaluate_model\")\n",
    "\n",
    "output = eval_model.remote(\"vastly_intent_frog_2\", [\"/vol/data/eval_dim_12\"], False, [0.2, 0.4])\n",
    "# output = eval_model.remote(\"daily_useful_squid\", [\"/vol/data/re_arc_dim_12_small\"], False, [0.8, 0.2], 100)\n",
    "# output = eval_model.remote(\"overly_hip_egret\", [\"/vol/data/re_arc/00d62c1b.json\"], True, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def score_output(output) -> tuple:\n",
    "    correct = 0\n",
    "    refined_correct = 0\n",
    "    total = 0\n",
    "    accuracy = 0.0\n",
    "    refined_accuracy = 0\n",
    "    close = 0\n",
    "    for item in output:\n",
    "        if np.array_equal(item[\"output_grid\"], item[\"predictions\"].squeeze(0)) is True:\n",
    "            correct += 1\n",
    "        if np.array_equal(item[\"output_grid\"], item[\"refined_predictions\"].squeeze(0)) is True:\n",
    "            refined_correct += 1\n",
    "        item_accuracy = (item[\"predictions\"].squeeze(0) == item[\"output_grid\"]).astype(float).mean()\n",
    "        accuracy += item_accuracy\n",
    "        if item_accuracy >= 0.95:\n",
    "            close += 1\n",
    "        refined_accuracy += (item[\"refined_predictions\"].squeeze(0) == item[\"output_grid\"]).astype(float).mean()\n",
    "        total += 1\n",
    "\n",
    "    refined_accuracy /= total\n",
    "    accuracy /= total\n",
    "\n",
    "    return (total, correct, accuracy, refined_correct, refined_accuracy, close)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*score_output(output))\n",
    "\n",
    "\n",
    "# print(f\"Correct: {correct}/{total} (Accuracy: {accuracy:.4f}%)\")\n",
    "# print(f\"Refined: {refined_correct}/{total} (Accuracy: {refined_accuracy:.4f}%)\")\n",
    "# print(f\"Close: {close}/{total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "temps = [0.0, 0.1, 0.4, 0.7, 0.9]\n",
    "for first_temp in temps:\n",
    "    for second_temp in temps:\n",
    "        output = eval_model.remote(\"kindly_exact_beagle\", [\"/vol/data/eval_dim_12\"], False, [first_temp, second_temp])\n",
    "        print(first_temp, second_temp, *score_output(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "fn = modal.Function.lookup(\"arc-eval\", \"finetune_and_predict\")\n",
    "output = fn.remote(\"jolly_picked_stud\", [\"/vol/data/eval_dim_12\"], 15, None, [0.1, 0.2], 4, 1e-5, 1e-5, 0.995, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_list(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_list(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_list(item) for item in obj]\n",
    "    elif hasattr(obj, 'tolist'):  \n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "with open(\"experiments/jolly_picked_stud-eval_dim_12-finetune-2.json\", \"w\") as file:\n",
    "  json.dump(convert_to_list(output), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiments/jolly_picked_stud-eval_dim_12-finetune-1.json\", \"r\") as file:\n",
    "  experiment_dict = json.load(file)\n",
    "  experiment = [{\n",
    "    \"task_id\": item[\"task_id\"],\n",
    "    \"grids\": np.array(item[\"grids\"]),\n",
    "    \"output_grid\": np.array(item[\"output_grid\"]),\n",
    "    \"predictions\": np.array(item[\"predictions\"]),\n",
    "    \"finetune_predictions\": np.array(item[\"finetune_predictions\"]),\n",
    "    \"refined_predictions\": np.array(item[\"refined_predictions\"])\n",
    "  } for item in experiment_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "correct = 0\n",
    "finetune_correct = 0\n",
    "refined_correct = 0\n",
    "total = 0\n",
    "accuracy = 0.0\n",
    "finetune_accuracy = 0\n",
    "refined_accuracy = 0\n",
    "close = 0\n",
    "finetune_close = 0\n",
    "refined_close = 0\n",
    "\n",
    "correct_ids = []\n",
    "incorrect_ids = []\n",
    "\n",
    "trained_ids = [\"03560426\", \"00dbd492\", \"0c786b71\", \"195ba7dc\", \"5ffb2104\", \"17b80ad2\", \"15696249\", \"12422b43\", \"4364c1c4\", \"0bb8deee\", \"0a1d4ef5\", \"0b17323b\", \"292dd178\", \"137f0df0\", \"0c9aba6e\", \"14754a24\", \"08573cc6\", \"17cae0c1\", \"12997ef3\", \"11e1fe23\", \"0f63c0b9\", \"1d398264\", \"351d6448\", \"0becf7df\", \"00576224\", \"332efdb3\", \"140c817e\", \"32e9702f\", \"0e671a1a\", \"12eac192\"]\n",
    "\n",
    "\n",
    "for item in output:\n",
    "    if item[\"task_id\"] not in trained_ids:\n",
    "        continue\n",
    "    if np.array_equal(item[\"output_grid\"], item[\"predictions\"][0]) is True:\n",
    "        correct += 1\n",
    "    if np.array_equal(item[\"output_grid\"], item[\"finetune_predictions\"][0]) is True:\n",
    "        finetune_correct += 1\n",
    "    if np.array_equal(item[\"output_grid\"], item[\"refined_predictions\"][0][0]) is True or np.array_equal(item[\"output_grid\"], item[\"refined_predictions\"][1][0]) is True:\n",
    "        refined_correct += 1\n",
    "    a = (item[\"predictions\"][0] == item[\"output_grid\"]).astype(float).mean()\n",
    "    accuracy += a\n",
    "    if a >= 0.95:\n",
    "        close += 1\n",
    "    f = (item[\"finetune_predictions\"][0] == item[\"output_grid\"]).astype(float).mean()\n",
    "    finetune_accuracy += f\n",
    "    if f >= 0.95:\n",
    "        finetune_close += 1\n",
    "    r = max((item[\"refined_predictions\"][0][0] == item[\"output_grid\"]).astype(float).mean(),\n",
    "         (item[\"refined_predictions\"][1][0] == item[\"output_grid\"]).astype(float).mean())\n",
    "    refined_accuracy += r\n",
    "    if r >= 0.95:\n",
    "        refined_close += 1\n",
    "    total += 1\n",
    "\n",
    "finetune_accuracy /= total\n",
    "accuracy /= total\n",
    "refined_accuracy /= total\n",
    "\n",
    "print(\"Zero-shot Correct\", correct, total, correct / total)\n",
    "print(\"Zero-shot acc\", accuracy)\n",
    "print(\"Zero-shot close\", close, total, close / total)\n",
    "\n",
    "print(\"TTT Correct\", finetune_correct, total, finetune_correct / total)\n",
    "print(\"TTT acc\", finetune_accuracy)\n",
    "print(\"TTT close\", finetune_close, total, finetune_close / total)\n",
    "\n",
    "print(\"TTT + Refined Correct\", refined_correct, total, refined_correct / total)\n",
    "print(\"TTT + Refined acc\", refined_accuracy)\n",
    "print(\"TTT + Refined close\", refined_close, total, refined_close / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ids = [\"03560426\", \"00dbd492\", \"0c786b71\", \"195ba7dc\", \"5ffb2104\", \"17b80ad2\", \"15696249\", \"12422b43\", \"4364c1c4\", \"0bb8deee\", \"0a1d4ef5\", \"0b17323b\", \"292dd178\", \"137f0df0\", \"0c9aba6e\", \"14754a24\", \"08573cc6\", \"17cae0c1\", \"12997ef3\", \"11e1fe23\", \"0f63c0b9\", \"1d398264\", \"351d6448\", \"0becf7df\", \"00576224\", \"332efdb3\", \"140c817e\", \"32e9702f\", \"0e671a1a\", \"12eac192\"]\n",
    "task_ids = [item[\"task_id\"] for item in experiment]\n",
    "included = 0\n",
    "for id in incorrect_ids:\n",
    "  if trained_ids.__contains__(id):\n",
    "    print(id)\n",
    "    included += 1\n",
    "len(correct_ids)\n",
    "print(included)\n",
    "print(task_ids)\n",
    "overlap = list(set(trained_ids) & set(task_ids))\n",
    "print(\", \".join(task_ids))\n",
    "len(task_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from arc_prize.vis import visualize_tensors, visualize_all_heads\n",
    "from arc_prize.vis import visualize_mean_mha_attention\n",
    "\n",
    "\n",
    "def visualize_mean_attention(attention_weights: torch.Tensor, num_grids: int, grid_size: int):\n",
    "    # Reshape the attention weights\n",
    "    # From [4, 100, 900] to [4, 100, 9, 10, 10]\n",
    "    num_heads = attention_weights.shape[0]\n",
    "    reshaped_attention = attention_weights.view(\n",
    "        num_heads, num_grids, grid_size, grid_size\n",
    "    )\n",
    "\n",
    "    # Calculate mean attention across the target sequence (dim=1)\n",
    "    # mean_attention = reshaped_attention.mean(dim=1)  # Shape: [4, 9, 10, 10]\n",
    "    mean_attention = reshaped_attention\n",
    "\n",
    "    # Create a figure with subplots for each head\n",
    "    fig, axes = plt.subplots(num_heads, num_grids, figsize=(20, 10))\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        # Create a 3x3 grid of heatmaps\n",
    "        for i in range(num_grids):\n",
    "            grid_attention = mean_attention[head, i]\n",
    "\n",
    "            # Add subplot within the head's subplot\n",
    "            # sub_ax = ax.inset_axes([1/9])\n",
    "            ax = axes[head, i]\n",
    "            im = ax.imshow(grid_attention, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "            # ax.axis(\"off\")\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(f\"Head {head + 1}\", rotation=0, ha=\"right\", va=\"center\")\n",
    "\n",
    "            # Add colorbar for each grid\n",
    "            # plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Remove ticks from the main subplot\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for item in output:\n",
    "#   if np.array_equal(item[\"output_grid\"], item[\"predictions\"]) is True:\n",
    "    # visualize_tensors(torch.Tensor(item[\"grids\"]).squeeze(0), torch.Tensor(item[\"output_grid\"]).squeeze(0), torch.Tensor(item[\"predictions\"]).squeeze(0).squeeze(0), torch.Tensor(item[\"refined_predictions\"]).squeeze(0).squeeze(0))\n",
    "    visualize_tensors(torch.Tensor(item[\"grids\"]).squeeze(0), torch.Tensor(item[\"output_grid\"]).squeeze(0), torch.Tensor(item[\"predictions\"]).squeeze(0), torch.Tensor(item[\"refined_predictions\"])[0][0].squeeze(0))\n",
    "# print(torch.Tensor(item[\"decoder_sa_attn_weights\"]).shape)\n",
    "  # for i, layer in enumerate(torch.Tensor(item[\"decoder_mha_attn_weights\"]).squeeze(0)):\n",
    "  #   print(layer.shape)\n",
    "  #   visualize_mean_attention(layer, 9, 30)\n",
    "    \n",
    "\n",
    "  # visualize_all_heads(layer, title=f\"Layer {i}\")\n",
    "# for i, layer in enumerate(torch.Tensor(item[\"decoder_sa_attn_weights\"]).squeeze(0)):\n",
    "#     visualize_mean_sa_attention(layer)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
