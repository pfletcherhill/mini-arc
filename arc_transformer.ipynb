{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.train import ARCModelState, ARCTrainParams\n",
    "from arc_prize.vis import visualize_epochs\n",
    "import modal\n",
    "import torch\n",
    "import petname\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.data import ARCDatasetParams, ReARCDataset, make_re_arc_data_loaders\n",
    "\n",
    "config = ARCDatasetParams(max_grid_size=30, max_train_grids=10, color_offset=1)\n",
    "dataset = ReARCDataset(\"data/re_arc/ff805c23.json\", config)\n",
    "train_loader, val_loader = make_re_arc_data_loaders([\"data/re_arc/ff805c23.json\"], 10, config)\n",
    "len(val_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"vision\"\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=20,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=6,\n",
    "  num_decoder_layers=6,\n",
    "  num_heads=16,\n",
    "  d_model=512,\n",
    "  d_ff=512*4,\n",
    "  dropout=0.3\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=32,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/re_arc_dim_20\", \"/vol/data/html_dim_20_20240925\"],\n",
    "  loss_class_weights={0: 0.2}\n",
    ")\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "model_names = []\n",
    "\n",
    "num_runs = 1\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"train\")\n",
    "for i in range(num_runs):\n",
    "  model_name = petname.generate(words=3, separator='_')\n",
    "  fn_call = fn.spawn(model_name, num_epochs, model_type, model_params, train_params)\n",
    "  print(\"Model name\", model_name, model_type, fn_call.object_id)\n",
    "  model_names.append(model_name)\n",
    "\n",
    "print(model_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new model namely_clear_bear\n",
      "Using 0 GPUs in parallel\n",
      "Model odict_keys(['output_query', 'embedding.conv_embed.weight', 'embedding.conv_embed.bias', 'pos_encoding.row_embedding.weight', 'pos_encoding.col_embedding.weight', 'pos_encoding.input_output_embedding.weight', 'pos_encoding.pair_embedding.weight', 'encoder.layers.0.self_attn.in_proj_weight', 'encoder.layers.0.self_attn.in_proj_bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.linear1.weight', 'encoder.layers.0.linear1.bias', 'encoder.layers.0.linear2.weight', 'encoder.layers.0.linear2.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.0.norm1.bias', 'encoder.layers.0.norm2.weight', 'encoder.layers.0.norm2.bias', 'decoder.layers.0.self_attn.in_proj_weight', 'decoder.layers.0.self_attn.in_proj_bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.multihead_attn.in_proj_weight', 'decoder.layers.0.multihead_attn.in_proj_bias', 'decoder.layers.0.multihead_attn.out_proj.weight', 'decoder.layers.0.multihead_attn.out_proj.bias', 'decoder.layers.0.linear1.weight', 'decoder.layers.0.linear1.bias', 'decoder.layers.0.linear2.weight', 'decoder.layers.0.linear2.bias', 'decoder.layers.0.norm1.weight', 'decoder.layers.0.norm1.bias', 'decoder.layers.0.norm2.weight', 'decoder.layers.0.norm2.bias', 'decoder.layers.0.norm3.weight', 'decoder.layers.0.norm3.bias', 'output_layer.weight', 'output_layer.bias'])\n",
      "Starting training run with dataset of 1000 training items and 200 evaluation items: /Users/pfh/work/arc-data/flip\n",
      "Using batch size of 10\n",
      "Starting outer loop batch 1/100\n",
      "Starting inner loop with 600 tasks\n",
      "Inner Loop parameters\n",
      "output_query torch.Size([1, 144, 16]) cpu True\n",
      "embedding.conv_embed.weight torch.Size([16, 11, 2, 2]) cpu True\n",
      "embedding.conv_embed.bias torch.Size([16]) cpu True\n",
      "pos_encoding.row_embedding.weight torch.Size([6, 4]) cpu True\n",
      "pos_encoding.col_embedding.weight torch.Size([6, 4]) cpu True\n",
      "pos_encoding.input_output_embedding.weight torch.Size([2, 4]) cpu True\n",
      "pos_encoding.pair_embedding.weight torch.Size([5, 4]) cpu True\n",
      "encoder.layers.0.self_attn.in_proj_weight torch.Size([48, 16]) cpu True\n",
      "encoder.layers.0.self_attn.in_proj_bias torch.Size([48]) cpu True\n",
      "encoder.layers.0.self_attn.out_proj.weight torch.Size([16, 16]) cpu True\n",
      "encoder.layers.0.self_attn.out_proj.bias torch.Size([16]) cpu True\n",
      "encoder.layers.0.linear1.weight torch.Size([32, 16]) cpu True\n",
      "encoder.layers.0.linear1.bias torch.Size([32]) cpu True\n",
      "encoder.layers.0.linear2.weight torch.Size([16, 32]) cpu True\n",
      "encoder.layers.0.linear2.bias torch.Size([16]) cpu True\n",
      "encoder.layers.0.norm1.weight torch.Size([16]) cpu True\n",
      "encoder.layers.0.norm1.bias torch.Size([16]) cpu True\n",
      "encoder.layers.0.norm2.weight torch.Size([16]) cpu True\n",
      "encoder.layers.0.norm2.bias torch.Size([16]) cpu True\n",
      "decoder.layers.0.self_attn.in_proj_weight torch.Size([48, 16]) cpu True\n",
      "decoder.layers.0.self_attn.in_proj_bias torch.Size([48]) cpu True\n",
      "decoder.layers.0.self_attn.out_proj.weight torch.Size([16, 16]) cpu True\n",
      "decoder.layers.0.self_attn.out_proj.bias torch.Size([16]) cpu True\n",
      "decoder.layers.0.multihead_attn.in_proj_weight torch.Size([48, 16]) cpu True\n",
      "decoder.layers.0.multihead_attn.in_proj_bias torch.Size([48]) cpu True\n",
      "decoder.layers.0.multihead_attn.out_proj.weight torch.Size([16, 16]) cpu True\n",
      "decoder.layers.0.multihead_attn.out_proj.bias torch.Size([16]) cpu True\n",
      "decoder.layers.0.linear1.weight torch.Size([32, 16]) cpu True\n",
      "decoder.layers.0.linear1.bias torch.Size([32]) cpu True\n",
      "decoder.layers.0.linear2.weight torch.Size([16, 32]) cpu True\n",
      "decoder.layers.0.linear2.bias torch.Size([16]) cpu True\n",
      "decoder.layers.0.norm1.weight torch.Size([16]) cpu True\n",
      "decoder.layers.0.norm1.bias torch.Size([16]) cpu True\n",
      "decoder.layers.0.norm2.weight torch.Size([16]) cpu True\n",
      "decoder.layers.0.norm2.bias torch.Size([16]) cpu True\n",
      "decoder.layers.0.norm3.weight torch.Size([16]) cpu True\n",
      "decoder.layers.0.norm3.bias torch.Size([16]) cpu True\n",
      "output_layer.weight torch.Size([11, 16]) cpu True\n",
      "output_layer.bias torch.Size([11]) cpu True\n",
      "Fine-tuning finished - Train Loss: 2.4262, Train Accuracy: 0.1581, Total duration: 40.94s (0.68m)\n",
      "Starting outer loop batch 2/100\n",
      "Starting inner loop with 600 tasks\n",
      "Inner Loop parameters\n",
      "output_query torch.Size([1, 144, 16]) cpu True\n",
      "embedding.conv_embed.weight torch.Size([16, 11, 2, 2]) cpu True\n",
      "embedding.conv_embed.bias torch.Size([16]) cpu True\n",
      "pos_encoding.row_embedding.weight torch.Size([6, 4]) cpu True\n",
      "pos_encoding.col_embedding.weight torch.Size([6, 4]) cpu True\n",
      "pos_encoding.input_output_embedding.weight torch.Size([2, 4]) cpu True\n",
      "pos_encoding.pair_embedding.weight torch.Size([5, 4]) cpu True\n",
      "encoder.layers.0.self_attn.in_proj_weight torch.Size([48, 16]) cpu True\n",
      "encoder.layers.0.self_attn.in_proj_bias torch.Size([48]) cpu True\n",
      "encoder.layers.0.self_attn.out_proj.weight torch.Size([16, 16]) cpu True\n",
      "encoder.layers.0.self_attn.out_proj.bias torch.Size([16]) cpu True\n",
      "encoder.layers.0.linear1.weight torch.Size([32, 16]) cpu True\n",
      "encoder.layers.0.linear1.bias torch.Size([32]) cpu True\n",
      "encoder.layers.0.linear2.weight torch.Size([16, 32]) cpu True\n",
      "encoder.layers.0.linear2.bias torch.Size([16]) cpu True\n",
      "encoder.layers.0.norm1.weight torch.Size([16]) cpu True\n",
      "encoder.layers.0.norm1.bias torch.Size([16]) cpu True\n",
      "encoder.layers.0.norm2.weight torch.Size([16]) cpu True\n",
      "encoder.layers.0.norm2.bias torch.Size([16]) cpu True\n",
      "decoder.layers.0.self_attn.in_proj_weight torch.Size([48, 16]) cpu True\n",
      "decoder.layers.0.self_attn.in_proj_bias torch.Size([48]) cpu True\n",
      "decoder.layers.0.self_attn.out_proj.weight torch.Size([16, 16]) cpu True\n",
      "decoder.layers.0.self_attn.out_proj.bias torch.Size([16]) cpu True\n",
      "decoder.layers.0.multihead_attn.in_proj_weight torch.Size([48, 16]) cpu True\n",
      "decoder.layers.0.multihead_attn.in_proj_bias torch.Size([48]) cpu True\n",
      "decoder.layers.0.multihead_attn.out_proj.weight torch.Size([16, 16]) cpu True\n",
      "decoder.layers.0.multihead_attn.out_proj.bias torch.Size([16]) cpu True\n",
      "decoder.layers.0.linear1.weight torch.Size([32, 16]) cpu True\n",
      "decoder.layers.0.linear1.bias torch.Size([32]) cpu True\n",
      "decoder.layers.0.linear2.weight torch.Size([16, 32]) cpu True\n",
      "decoder.layers.0.linear2.bias torch.Size([16]) cpu True\n",
      "decoder.layers.0.norm1.weight torch.Size([16]) cpu True\n",
      "decoder.layers.0.norm1.bias torch.Size([16]) cpu True\n",
      "decoder.layers.0.norm2.weight torch.Size([16]) cpu True\n",
      "decoder.layers.0.norm2.bias torch.Size([16]) cpu True\n",
      "decoder.layers.0.norm3.weight torch.Size([16]) cpu True\n",
      "decoder.layers.0.norm3.bias torch.Size([16]) cpu True\n",
      "output_layer.weight torch.Size([11, 16]) cpu True\n",
      "output_layer.bias torch.Size([11]) cpu True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_runs):\n\u001b[1;32m     36\u001b[0m   model_name \u001b[38;5;241m=\u001b[39m petname\u001b[38;5;241m.\u001b[39mgenerate(words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[0;32m---> 37\u001b[0m   \u001b[43mmeta_train_on_mac\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m   \u001b[38;5;66;03m# print(\"Model name\", model_name, fn_call.object_id)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m   model_names\u001b[38;5;241m.\u001b[39mappend(model_name)\n",
      "File \u001b[0;32m~/work/arc-prize/arc_prize/meta.py:366\u001b[0m, in \u001b[0;36mmeta_train_on_mac\u001b[0;34m(model_name, num_epochs, model_type, model_params, train_params)\u001b[0m\n\u001b[1;32m    355\u001b[0m     model_state \u001b[38;5;241m=\u001b[39m ARCModelState(\n\u001b[1;32m    356\u001b[0m         model_type\u001b[38;5;241m=\u001b[39mmodel_type,\n\u001b[1;32m    357\u001b[0m         model_state_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m         best_val_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    363\u001b[0m     )\n\u001b[1;32m    364\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model_state\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, model_filename)\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeta_train_arc_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_params\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/arc-prize/arc_prize/meta.py:222\u001b[0m, in \u001b[0;36mmeta_train_arc_transformer\u001b[0;34m(model_filename, num_epochs, patience, train_params)\u001b[0m\n\u001b[1;32m    218\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    220\u001b[0m finetune_dataset \u001b[38;5;241m=\u001b[39m make_finetune_dataset(grids, dataset_params)\n\u001b[0;32m--> 222\u001b[0m adapted_params \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_fine_tune_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune_dataset\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m#     for name, param in parallel_model.module.named_parameters():\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m#         param.copy_(adapted_params[name])\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(device\u001b[38;5;241m.\u001b[39mtype):\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# output = parallel_model.forward(grids, masks)[0]\u001b[39;00m\n",
      "File \u001b[0;32m~/work/arc-prize/arc_prize/meta.py:102\u001b[0m, in \u001b[0;36mmeta_fine_tune_transformer\u001b[0;34m(model, train_params, dataset)\u001b[0m\n\u001b[1;32m     94\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39mfunctional_call(\n\u001b[1;32m     95\u001b[0m         model, adapted_params, (grids, masks), {}\n\u001b[1;32m     96\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     97\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     98\u001b[0m         output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m.\u001b[39mnum_classes),\n\u001b[1;32m     99\u001b[0m         target_grid\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong(),\n\u001b[1;32m    100\u001b[0m     )\n\u001b[0;32m--> 102\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(adapted_params\u001b[38;5;241m.\u001b[39mvalues(), grads):\n\u001b[1;32m    107\u001b[0m     param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m grad\n",
      "File \u001b[0;32m~/work/arc-prize/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:436\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    432\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    433\u001b[0m         grad_outputs_\n\u001b[1;32m    434\u001b[0m     )\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 436\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    447\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    449\u001b[0m     ):\n",
      "File \u001b[0;32m~/work/arc-prize/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "from arc_prize.meta import meta_train_on_mac\n",
    "\n",
    "model_type = \"vision\"\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=12,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=1,\n",
    "  num_decoder_layers=1,\n",
    "  num_heads=2,\n",
    "  d_model=16,\n",
    "  d_ff=16*2,\n",
    "  dropout=0.1\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=10,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/Users/pfh/work/arc-data/flip\"],\n",
    "  meta_batch_size=2,\n",
    "  meta_learning_rate=1e-5,\n",
    "  meta_weight_decay=1e-5,\n",
    "  meta_num_epochs=3\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "model_names = []\n",
    "\n",
    "num_runs = 1\n",
    "\n",
    "\n",
    "for i in range(num_runs):\n",
    "  model_name = petname.generate(words=3, separator='_')  \n",
    "  meta_train_on_mac(model_name, num_epochs, model_type, model_params, train_params)\n",
    "  # print(\"Model name\", model_name, fn_call.object_id)\n",
    "  model_names.append(model_name)\n",
    "\n",
    "print(model_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name fairly_awake_camel vision fc-01J9219VGHBYN7FNCTCQQEC5N1\n",
      "['fairly_awake_camel']\n"
     ]
    }
   ],
   "source": [
    "model_type = \"vision\"\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=12,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=1,\n",
    "  num_decoder_layers=1,\n",
    "  num_heads=4,\n",
    "  d_model=32,\n",
    "  d_ff=32*4,\n",
    "  dropout=0.2\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=32,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/re_arc_dim_12_small\"],\n",
    "  loss_class_weights={0: 0.2},\n",
    "  meta_num_epochs=3,\n",
    "  meta_batch_size=5,\n",
    "  meta_learning_rate=1e-5,\n",
    "  meta_weight_decay=1e-5,\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "model_names = []\n",
    "\n",
    "num_runs = 1\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize-meta\", \"meta_train\")\n",
    "for i in range(num_runs):\n",
    "  model_name = petname.generate(words=3, separator='_')\n",
    "  fn_call = fn.spawn(model_name, num_epochs, model_type, model_params, train_params)\n",
    "  print(\"Model name\", model_name, model_type, fn_call.object_id)\n",
    "  model_names.append(model_name)\n",
    "\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# model_names = ['subtly_moral_bee', 'newly_mint_kite']\n",
    "model_names = ['lively_fleet_goat']\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=20,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/html_dim_20_20240925\", \"/vol/data/move_many_random\", \"/vol/data/rotate\", \"/vol/data/scale\", \"/vol/data/flip\"],\n",
    "  loss_class_weights={0: 0.2}\n",
    ")\n",
    "train_params = None\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"train\")\n",
    "for model_name in model_names:\n",
    "  fn_call = fn.spawn(model_name, num_epochs, None, train_params)\n",
    "  print(\"Model name\", model_name, fn_call.object_id)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_group(model_names: list[str]):\n",
    "  epochs = {}\n",
    "  get_model = modal.Function.lookup(\"arc-prize\", \"get_model\")\n",
    "  for name in model_names:\n",
    "    checkpoint = ARCModelState(**get_model.remote(name))\n",
    "    print(name, len(checkpoint.epochs), checkpoint.epochs[-1], checkpoint.model_params)\n",
    "    epochs[name] = checkpoint.epochs\n",
    "\n",
    "\n",
    "    # print(len(checkpoint.encoder_attn_weights))\n",
    "    # for b, batch in enumerate(checkpoint.encoder_attn_weights):\n",
    "    #   for i, layer in enumerate(batch):\n",
    "    #     visualize_all_heads(layer, title=f\"Batch {b}, layer {i}\")\n",
    "    \n",
    "\n",
    "  visualize_epochs(epochs)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "groups = [\n",
    "  # ['kindly_huge_jennet', 'lovely_tidy_lab', 'solely_living_leech'], # BEST\n",
    "  # ['weekly_enough_moose', 'gently_known_beagle', 'nicely_robust_rhino'], # 20x20 too slow\n",
    "  # ['wildly_firm_husky', 'surely_brief_bug', 'fully_better_dodo'], # Amazing\n",
    "  # ['wildly_steady_iguana', 'yearly_smart_donkey', 'mainly_polite_bison'], # Includes scale dataset\n",
    "  # ['partly_vocal_piglet', 'neatly_needed_liger', 'firmly_game_weevil'], # Scale and diagonal\n",
    "  # ['wholly_tops_heron', 'solely_eager_foal', 'deeply_one_skink'], # Tons of data\n",
    "  # ['unduly_glad_swift', 'purely_steady_hornet', 'humbly_civil_donkey'], # Basic data\n",
    "  # [\"early_civil_beetle\"],\n",
    "  # [\"really_fancy_kitten\", \"mildly_humble_tahr\"],\n",
    "  # ['solely_brief_shad', 'fairly_amazed_hyena', 'vastly_amazed_bobcat'],\n",
    "  # ['barely_sound_viper', 'lively_key_goblin', 'wildly_fancy_glider'], # patch size 3\n",
    "  # ['mildly_able_horse', 'vastly_normal_rhino', 'oddly_mint_clam'], # patch size 2\n",
    "  # ['firmly_tops_adder', 'yearly_normal_puma', 'slowly_more_caiman'], # patch size 2, simpler embedding\n",
    "  # ['safely_poetic_adder', 'vastly_close_horse', 'fairly_legal_insect', 'daily_actual_monkey'], # Patch embedding with ARC pos encoding\n",
    "  # ['nicely_wired_mouse', 'freely_up_shrimp', 'hardly_loving_mullet', 'gladly_active_muskox']\n",
    "  # ['mostly_normal_dog', 'lively_pure_hawk', 'rarely_tender_roughy'], # HUGE dataset re_arc_dim_12\n",
    "  # ['daily_pro_cattle', 'newly_suited_finch', 'rarely_tender_roughy'], # 32 batch size, 64 dim\n",
    "  # ['solely_sound_sponge', 'lively_sacred_egret'], # 32 batch size, 128 dim, 3+3 layers\n",
    "  # ['nicely_pure_leech', 'namely_sure_emu'], # 128 dim, 4+4 layers\n",
    "  # ['subtly_moral_bee', 'newly_mint_kite', 'solely_busy_skunk'], # Vision transformer large models\n",
    "  # ['newly_mint_kite', 'namely_caring_mite', 'overly_enough_tomcat', 'subtly_known_panda', 'mildly_ruling_hog', 'simply_tight_fowl'],\n",
    "  ['subtly_known_panda', 'barely_clean_cicada', 'sadly_real_viper'], # 512 dim (12x12, 20x20, 30x30)\n",
    "  ['lively_fleet_goat'], # vision 20x20\n",
    "]\n",
    "\n",
    "# print([group for sublist in groups for group in sublist])\n",
    "for group in groups:\n",
    "  visualize_group(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "eval_model = modal.Function.lookup(\"arc-prize\", \"evaluate_model\")\n",
    "output = eval_model.remote(\"lively_fleet_goat\", [\"/vol/data/eval_dim_20\"])\n",
    "# output = eval_model.remote(\"overly_hip_egret\", [\"/vol/data/re_arc/00d62c1b.json\"], True, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = modal.Function.lookup(\"arc-prize-meta\", \"finetune_and_predict\")\n",
    "output = fn.remote(\"barely_clean_cicada\", [\"/vol/data/eval_dim_20\"], 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "correct = 0\n",
    "for item in output:\n",
    "    if np.array_equal(item[\"output_grid\"], item[\"predictions\"]) is True:\n",
    "        correct += 1\n",
    "\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from arc_prize.vis import visualize_tensors, visualize_all_heads\n",
    "from arc_prize.vis import visualize_mean_mha_attention\n",
    "\n",
    "\n",
    "def visualize_mean_attention(attention_weights: torch.Tensor, num_grids: int, grid_size: int):\n",
    "    # Reshape the attention weights\n",
    "    # From [4, 100, 900] to [4, 100, 9, 10, 10]\n",
    "    num_heads = attention_weights.shape[0]\n",
    "    reshaped_attention = attention_weights.view(\n",
    "        num_heads, num_grids, grid_size, grid_size\n",
    "    )\n",
    "\n",
    "    # Calculate mean attention across the target sequence (dim=1)\n",
    "    # mean_attention = reshaped_attention.mean(dim=1)  # Shape: [4, 9, 10, 10]\n",
    "    mean_attention = reshaped_attention\n",
    "\n",
    "    # Create a figure with subplots for each head\n",
    "    fig, axes = plt.subplots(num_heads, num_grids, figsize=(20, 10))\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        # Create a 3x3 grid of heatmaps\n",
    "        for i in range(num_grids):\n",
    "            grid_attention = mean_attention[head, i]\n",
    "\n",
    "            # Add subplot within the head's subplot\n",
    "            # sub_ax = ax.inset_axes([1/9])\n",
    "            ax = axes[head, i]\n",
    "            im = ax.imshow(grid_attention, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "            # ax.axis(\"off\")\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(f\"Head {head + 1}\", rotation=0, ha=\"right\", va=\"center\")\n",
    "\n",
    "            # Add colorbar for each grid\n",
    "            # plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Remove ticks from the main subplot\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for item in output:\n",
    "#   if np.array_equal(item[\"output_grid\"], item[\"predictions\"]) is True:\n",
    "    visualize_tensors(torch.Tensor(item[\"grids\"]).squeeze(0), torch.Tensor(item[\"output_grid\"]).squeeze(0), torch.Tensor(item[\"predictions\"]).squeeze(0), None)\n",
    "# print(torch.Tensor(item[\"decoder_sa_attn_weights\"]).shape)\n",
    "  # for i, layer in enumerate(torch.Tensor(item[\"decoder_mha_attn_weights\"]).squeeze(0)):\n",
    "  #   print(layer.shape)\n",
    "  #   visualize_mean_attention(layer, 9, 30)\n",
    "    \n",
    "\n",
    "  # visualize_all_heads(layer, title=f\"Layer {i}\")\n",
    "# for i, layer in enumerate(torch.Tensor(item[\"decoder_sa_attn_weights\"]).squeeze(0)):\n",
    "#     visualize_mean_sa_attention(layer)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "\n",
    "model_file_path = \"models/subtly_known_panda.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_dict = torch.load(model_file_path, weights_only=False, map_location=device)\n",
    "checkpoint = ARCModelState(**checkpoint_dict)\n",
    "\n",
    "# ARCTransformerEncoderDecoderParams(grid_dim=12, num_train_pairs=4, num_colors=10, num_encoder_layers=6, num_decoder_layers=6, num_heads=16, d_model=512, d_ff=2048, dropout=0.3)\n",
    "\n",
    "kaggle_model_file_path = \"kaggle/models/subtly_known_panda.pth\"\n",
    "torch.save(checkpoint.model_state_dict, kaggle_model_file_path)\n",
    "# kaggle_checkpoint = ARCKaggleModelState(model_params=checkpoint.model_params, model_state_dict=checkpoint.model_state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_output(output: torch.Tensor) -> torch.Tensor:\n",
    "    output = output - 1\n",
    "\n",
    "    filtered_rows = [row[row != -1] for row in output]\n",
    "    \n",
    "    max_length = max(len(row) for row in filtered_rows)\n",
    "\n",
    "    padded_rows = [torch.cat([row, torch.zeros(max_length - len(row), dtype=row.dtype, device=device)]) for row in filtered_rows]\n",
    "\n",
    "    return torch.stack(padded_rows)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
