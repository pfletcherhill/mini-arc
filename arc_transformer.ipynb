{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.train import ARCModelState, ARCTrainParams\n",
    "from arc_prize.vis import visualize_epochs\n",
    "import modal\n",
    "import torch\n",
    "import petname\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.data import ARCDatasetParams, ReARCDataset, make_re_arc_data_loaders\n",
    "\n",
    "config = ARCDatasetParams(max_grid_size=30, max_train_grids=10, color_offset=1)\n",
    "dataset = ReARCDataset(\"data/re_arc/ff805c23.json\", config)\n",
    "train_loader, val_loader = make_re_arc_data_loaders([\"data/re_arc/ff805c23.json\"], 10, config)\n",
    "len(val_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name barely_nice_ox vision_encoder fc-01JBQNGRMP1EJSG4XB8CKASNGQ\n",
      "['barely_nice_ox']\n"
     ]
    }
   ],
   "source": [
    "import modal\n",
    "from arc_prize.train import ARCTrainParams\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "import petname\n",
    "\n",
    "model_type = \"vision_encoder\"\n",
    "\n",
    "\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=20,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=12,\n",
    "  num_decoder_layers=0,\n",
    "  num_heads=16,\n",
    "  d_model=1024,\n",
    "  d_ff=1024*4,\n",
    "  dropout=0.1\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=32,\n",
    "  learning_rate=1e-5,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/html_dim_20_20240925\", \"/vol/data/re_arc_dim_20\"],\n",
    "  loss_class_weights={0: 0.2},\n",
    "  train_steps_per_epoch=1000,\n",
    "  eval_steps_per_epoch=150\n",
    ")\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "model_names = []\n",
    "\n",
    "num_runs = 1\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"train\")\n",
    "for i in range(num_runs):\n",
    "  model_name = petname.generate(words=3, separator='_')\n",
    "  fn_call = fn.spawn(model_name, num_epochs, model_type, model_params, train_params)\n",
    "  print(\"Model name\", model_name, model_type, fn_call.object_id)\n",
    "  model_names.append(model_name)\n",
    "\n",
    "print(model_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_deep_grub\n",
      "Starting new model daily_deep_grub\n",
      "Using single CPU\n",
      "Starting training run with dataset of 1000 training items and 200 evaluation items: /Users/pfh/work/arc-data/flip\n",
      "Using batch size of 12\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Epoch 1/5 for models/daily_deep_grub.pth:\n",
      "Train Loss: 3.2492, Train Accuracy: 0.0235, Train Steps: 3\n",
      "Val Loss: 3.2123, Val Accuracy: 0.0266, Val Steps: 2\n",
      "Epoch duration: 8.66s (0.14m)\n",
      "New best val loss 3.2123231887817383\n",
      "Saved checkpoint models/daily_deep_grub.pth\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Epoch 2/5 for models/daily_deep_grub.pth:\n",
      "Train Loss: 3.1984, Train Accuracy: 0.0210, Train Steps: 3\n",
      "Val Loss: 3.1902, Val Accuracy: 0.0336, Val Steps: 2\n",
      "Epoch duration: 1.56s (0.03m)\n",
      "New best val loss 3.1901580095291138\n",
      "Saved checkpoint models/daily_deep_grub.pth\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Epoch 3/5 for models/daily_deep_grub.pth:\n",
      "Train Loss: 3.1719, Train Accuracy: 0.0258, Train Steps: 3\n",
      "Val Loss: 3.2026, Val Accuracy: 0.0278, Val Steps: 2\n",
      "Epoch duration: 1.54s (0.03m)\n",
      "Saved checkpoint models/daily_deep_grub.pth\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Epoch 4/5 for models/daily_deep_grub.pth:\n",
      "Train Loss: 3.1509, Train Accuracy: 0.0235, Train Steps: 3\n",
      "Val Loss: 3.1517, Val Accuracy: 0.0284, Val Steps: 2\n",
      "Epoch duration: 1.70s (0.03m)\n",
      "New best val loss 3.151740550994873\n",
      "Saved checkpoint models/daily_deep_grub.pth\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Testing torch.Size([12, 9, 12, 12]) torch.Size([12, 9, 12, 12])\n",
      "pos emb torch.Size([10, 12, 12, 16])\n",
      "src torch.Size([12, 108, 12]) src_mask torch.Size([12, 108, 12])\n",
      "input pos torch.Size([9, 12, 12, 16])\n",
      "patched pos emb torch.Size([324, 16])\n",
      "input_seq torch.Size([12, 324, 16])\n",
      "output query torch.Size([12, 1, 12, 12, 16]) torch.Size([1, 12, 12, 16])\n",
      "combined torch.Size([12, 468, 16])\n",
      "causal mask torch.Size([468, 468])\n",
      "padding mask torch.Size([12, 468])\n",
      "Epoch 5/5 for models/daily_deep_grub.pth:\n",
      "Train Loss: 3.1577, Train Accuracy: 0.0189, Train Steps: 3\n",
      "Val Loss: 3.1300, Val Accuracy: 0.0356, Val Steps: 2\n",
      "Epoch duration: 2.38s (0.04m)\n",
      "New best val loss 3.130020260810852\n",
      "Saved checkpoint models/daily_deep_grub.pth\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from arc_prize.train import ARCTrainParams, train_on_mac\n",
    "import petname\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "\n",
    "\n",
    "model_type = \"vision_encoder\"\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=12,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=2,\n",
    "  num_decoder_layers=1,\n",
    "  num_heads=2,\n",
    "  d_model=16,\n",
    "  d_ff=16*2,\n",
    "  dropout=0.1\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=12,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/Users/pfh/work/arc-data/flip\"],\n",
    "  train_steps_per_epoch=3,\n",
    "  eval_steps_per_epoch=2,\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "model_name = petname.generate(words=3, separator='_')  \n",
    "print(model_name)\n",
    "train_on_mac(model_name, num_epochs, model_type, model_params, train_params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"vision\"\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=12,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=1,\n",
    "  num_decoder_layers=1,\n",
    "  num_heads=4,\n",
    "  d_model=32,\n",
    "  d_ff=32*4,\n",
    "  dropout=0.2\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=32,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/move_random_small\", \"/vol/data/flip\"],\n",
    "  loss_class_weights={0: 0.2},\n",
    "  meta_num_epochs=2,\n",
    "  meta_batch_size=10,\n",
    "  meta_learning_rate=1e-4,\n",
    "  meta_weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "model_names = []\n",
    "\n",
    "num_runs = 1\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize-meta\", \"meta_train\")\n",
    "for i in range(num_runs):\n",
    "  model_name = petname.generate(words=3, separator='_')\n",
    "  fn_call = fn.spawn(model_name, num_epochs, model_type, model_params, train_params)\n",
    "  print(\"Model name\", model_name, model_type, fn_call.object_id)\n",
    "  model_names.append(model_name)\n",
    "\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name hugely_key_moth fc-01JBPY878SXPE7X4HMAV53XET5\n",
      "Model name purely_one_llama fc-01JBPY87BBV80R8RSSDB5P6TP2\n"
     ]
    }
   ],
   "source": [
    "import modal\n",
    "from arc_prize.train import ARCTrainParams\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# model_names = ['subtly_moral_bee', 'newly_mint_kite']\n",
    "model_names = [\"hugely_key_moth\", \"purely_one_llama\"]\n",
    "model_type = \"vision_encoder\"\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=32,\n",
    "  learning_rate=1e-3,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/html_dim_20_20240925\", \"/vol/data/re_arc_dim_20\"],\n",
    "  loss_class_weights={0: 0.2},\n",
    "  train_steps_per_epoch=1000,\n",
    "  eval_steps_per_epoch=150\n",
    ")\n",
    "# train_params = None\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"train\")\n",
    "for model_name in model_names:\n",
    "  fn_call = fn.spawn(model_name, num_epochs, model_type, None, train_params)\n",
    "  print(\"Model name\", model_name, fn_call.object_id)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ARCModelState.__init__() missing 2 required positional arguments: 'model_state_dict' and 'optimizer_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print([group for sublist in groups for group in sublist])\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m groups:\n\u001b[0;32m---> 54\u001b[0m   \u001b[43mvisualize_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mvisualize_group\u001b[0;34m(model_names)\u001b[0m\n\u001b[1;32m      6\u001b[0m get_model \u001b[38;5;241m=\u001b[39m modal\u001b[38;5;241m.\u001b[39mFunction\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marc-prize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m model_names:\n\u001b[0;32m----> 8\u001b[0m   checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mARCModelState\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28mprint\u001b[39m(name, \u001b[38;5;28mlen\u001b[39m(checkpoint\u001b[38;5;241m.\u001b[39mepochs), checkpoint\u001b[38;5;241m.\u001b[39mepochs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], checkpoint\u001b[38;5;241m.\u001b[39mmodel_params)\n\u001b[1;32m     10\u001b[0m   epochs[name] \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mepochs\n",
      "\u001b[0;31mTypeError\u001b[0m: ARCModelState.__init__() missing 2 required positional arguments: 'model_state_dict' and 'optimizer_state_dict'"
     ]
    }
   ],
   "source": [
    "from arc_prize.train import ARCModelState, EpochState\n",
    "from arc_prize.vis import visualize_epochs\n",
    "\n",
    "def visualize_group(model_names: list[str]):\n",
    "  epochs = {}\n",
    "  get_model = modal.Function.lookup(\"arc-prize\", \"get_model\")\n",
    "  for name in model_names:\n",
    "    checkpoint_dict = get_model.remote(name)\n",
    "    # checkpoint = ARCModelState(**get_model.remote(name))\n",
    "    print(name, len(checkpoint_dict[\"epochs\"]), checkpoint_dict[\"epochs\"][-1], checkpoint_dict[\"model_params\"])\n",
    "    epochs[name] = checkpoint_dict[\"epochs\"]\n",
    "\n",
    "\n",
    "    # print(len(checkpoint.encoder_attn_weights))\n",
    "    # for b, batch in enumerate(checkpoint.encoder_attn_weights):\n",
    "    #   for i, layer in enumerate(batch):\n",
    "    #     visualize_all_heads(layer, title=f\"Batch {b}, layer {i}\")\n",
    "    \n",
    "\n",
    "  visualize_epochs(epochs)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "groups = [\n",
    "  # ['kindly_huge_jennet', 'lovely_tidy_lab', 'solely_living_leech'], # BEST\n",
    "  # ['weekly_enough_moose', 'gently_known_beagle', 'nicely_robust_rhino'], # 20x20 too slow\n",
    "  # ['wildly_firm_husky', 'surely_brief_bug', 'fully_better_dodo'], # Amazing\n",
    "  # ['wildly_steady_iguana', 'yearly_smart_donkey', 'mainly_polite_bison'], # Includes scale dataset\n",
    "  # ['partly_vocal_piglet', 'neatly_needed_liger', 'firmly_game_weevil'], # Scale and diagonal\n",
    "  # ['wholly_tops_heron', 'solely_eager_foal', 'deeply_one_skink'], # Tons of data\n",
    "  # ['unduly_glad_swift', 'purely_steady_hornet', 'humbly_civil_donkey'], # Basic data\n",
    "  # [\"early_civil_beetle\"],\n",
    "  # [\"really_fancy_kitten\", \"mildly_humble_tahr\"],\n",
    "  # ['solely_brief_shad', 'fairly_amazed_hyena', 'vastly_amazed_bobcat'],\n",
    "  # ['barely_sound_viper', 'lively_key_goblin', 'wildly_fancy_glider'], # patch size 3\n",
    "  # ['mildly_able_horse', 'vastly_normal_rhino', 'oddly_mint_clam'], # patch size 2\n",
    "  # ['firmly_tops_adder', 'yearly_normal_puma', 'slowly_more_caiman'], # patch size 2, simpler embedding\n",
    "  # ['safely_poetic_adder', 'vastly_close_horse', 'fairly_legal_insect', 'daily_actual_monkey'], # Patch embedding with ARC pos encoding\n",
    "  # ['nicely_wired_mouse', 'freely_up_shrimp', 'hardly_loving_mullet', 'gladly_active_muskox']\n",
    "  # ['mostly_normal_dog', 'lively_pure_hawk', 'rarely_tender_roughy'], # HUGE dataset re_arc_dim_12\n",
    "  # ['daily_pro_cattle', 'newly_suited_finch', 'rarely_tender_roughy'], # 32 batch size, 64 dim\n",
    "  # ['solely_sound_sponge', 'lively_sacred_egret'], # 32 batch size, 128 dim, 3+3 layers\n",
    "  # ['nicely_pure_leech', 'namely_sure_emu'], # 128 dim, 4+4 layers\n",
    "  # ['subtly_moral_bee', 'newly_mint_kite', 'solely_busy_skunk'], # Vision transformer large models\n",
    "  # ['newly_mint_kite', 'namely_caring_mite', 'overly_enough_tomcat', 'subtly_known_panda', 'mildly_ruling_hog', 'simply_tight_fowl'],\n",
    "  # ['subtly_known_panda', 'barely_clean_cicada', 'sadly_real_viper'], # 512 dim (12x12, 20x20, 30x30)\n",
    "  # ['lively_fleet_goat'], # vision 20x20\n",
    "  # [\"kindly_living_spider\", \"gladly_prompt_koi\"] # encoder vs normal\n",
    "  [\"hugely_humble_falcon\", \"hugely_key_moth\", \"purely_one_llama\"],\n",
    "]\n",
    "\n",
    "# print([group for sublist in groups for group in sublist])\n",
    "for group in groups:\n",
    "  visualize_group(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "\n",
    "\n",
    "eval_model = modal.Function.lookup(\"arc-prize\", \"evaluate_model\")\n",
    "output = eval_model.remote(\"kindly_living_spider\", [\"/vol/data/re_arc_dim_12_small\"])\n",
    "# output = eval_model.remote(\"overly_hip_egret\", [\"/vol/data/re_arc/00d62c1b.json\"], True, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"finetune_and_predict\")\n",
    "output = fn.remote(\"kindly_living_spider\", [\"/vol/data/re_arc_dim_12_small\"], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for item in output:\n",
    "    if np.array_equal(item[\"output_grid\"], item[\"predictions\"]) is True:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(correct, total, correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from arc_prize.vis import visualize_tensors, visualize_all_heads\n",
    "from arc_prize.vis import visualize_mean_mha_attention\n",
    "\n",
    "\n",
    "def visualize_mean_attention(attention_weights: torch.Tensor, num_grids: int, grid_size: int):\n",
    "    # Reshape the attention weights\n",
    "    # From [4, 100, 900] to [4, 100, 9, 10, 10]\n",
    "    num_heads = attention_weights.shape[0]\n",
    "    reshaped_attention = attention_weights.view(\n",
    "        num_heads, num_grids, grid_size, grid_size\n",
    "    )\n",
    "\n",
    "    # Calculate mean attention across the target sequence (dim=1)\n",
    "    # mean_attention = reshaped_attention.mean(dim=1)  # Shape: [4, 9, 10, 10]\n",
    "    mean_attention = reshaped_attention\n",
    "\n",
    "    # Create a figure with subplots for each head\n",
    "    fig, axes = plt.subplots(num_heads, num_grids, figsize=(20, 10))\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        # Create a 3x3 grid of heatmaps\n",
    "        for i in range(num_grids):\n",
    "            grid_attention = mean_attention[head, i]\n",
    "\n",
    "            # Add subplot within the head's subplot\n",
    "            # sub_ax = ax.inset_axes([1/9])\n",
    "            ax = axes[head, i]\n",
    "            im = ax.imshow(grid_attention, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "            # ax.axis(\"off\")\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(f\"Head {head + 1}\", rotation=0, ha=\"right\", va=\"center\")\n",
    "\n",
    "            # Add colorbar for each grid\n",
    "            # plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Remove ticks from the main subplot\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for item in output:\n",
    "#   if np.array_equal(item[\"output_grid\"], item[\"predictions\"]) is True:\n",
    "    visualize_tensors(torch.Tensor(item[\"grids\"]).squeeze(0), torch.Tensor(item[\"output_grid\"]).squeeze(0), torch.Tensor(item[\"predictions\"]).squeeze(0), torch.Tensor(item[\"finetune_predictions\"]))\n",
    "# print(torch.Tensor(item[\"decoder_sa_attn_weights\"]).shape)\n",
    "  # for i, layer in enumerate(torch.Tensor(item[\"decoder_mha_attn_weights\"]).squeeze(0)):\n",
    "  #   print(layer.shape)\n",
    "  #   visualize_mean_attention(layer, 9, 30)\n",
    "    \n",
    "\n",
    "  # visualize_all_heads(layer, title=f\"Layer {i}\")\n",
    "# for i, layer in enumerate(torch.Tensor(item[\"decoder_sa_attn_weights\"]).squeeze(0)):\n",
    "#     visualize_mean_sa_attention(layer)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "\n",
    "model_file_path = \"models/subtly_known_panda.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_dict = torch.load(model_file_path, weights_only=False, map_location=device)\n",
    "checkpoint = ARCModelState(**checkpoint_dict)\n",
    "\n",
    "# ARCTransformerEncoderDecoderParams(grid_dim=12, num_train_pairs=4, num_colors=10, num_encoder_layers=6, num_decoder_layers=6, num_heads=16, d_model=512, d_ff=2048, dropout=0.3)\n",
    "\n",
    "kaggle_model_file_path = \"kaggle/models/subtly_known_panda.pth\"\n",
    "torch.save(checkpoint.model_state_dict, kaggle_model_file_path)\n",
    "# kaggle_checkpoint = ARCKaggleModelState(model_params=checkpoint.model_params, model_state_dict=checkpoint.model_state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_output(output: torch.Tensor) -> torch.Tensor:\n",
    "    output = output - 1\n",
    "\n",
    "    filtered_rows = [row[row != -1] for row in output]\n",
    "    \n",
    "    max_length = max(len(row) for row in filtered_rows)\n",
    "\n",
    "    padded_rows = [torch.cat([row, torch.zeros(max_length - len(row), dtype=row.dtype, device=device)]) for row in filtered_rows]\n",
    "\n",
    "    return torch.stack(padded_rows)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
