{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.train import ARCModelState, ARCTrainParams\n",
    "from arc_prize.vis import visualize_epochs\n",
    "import modal\n",
    "import torch\n",
    "import petname\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.data import ARCDatasetParams, ReARCDataset, make_re_arc_data_loaders\n",
    "\n",
    "config = ARCDatasetParams(max_grid_size=30, max_train_grids=10, color_offset=1)\n",
    "dataset = ReARCDataset(\"data/re_arc/ff805c23.json\", config)\n",
    "train_loader, val_loader = make_re_arc_data_loaders([\"data/re_arc/ff805c23.json\"], 10, config)\n",
    "len(val_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "\n",
    "model_type = \"normal\"\n",
    "\n",
    "\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=12,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=6,\n",
    "  num_decoder_layers=6,\n",
    "  num_heads=16,\n",
    "  d_model=512,\n",
    "  d_ff=512*4,\n",
    "  dropout=0.2\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=32,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/html_dim_12_20241023\", \"/vol/data/re_arc_dim_12\"],\n",
    "  loss_class_weights={0: 0.2}\n",
    ")\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "model_names = []\n",
    "\n",
    "num_runs = 1\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"train\")\n",
    "for i in range(num_runs):\n",
    "  model_name = petname.generate(words=3, separator='_')\n",
    "  fn_call = fn.spawn(model_name, num_epochs, model_type, model_params, train_params)\n",
    "  print(\"Model name\", model_name, model_type, fn_call.object_id)\n",
    "  model_names.append(model_name)\n",
    "\n",
    "print(model_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from arc_prize.train import ARCTrainParams, train_on_mac\n",
    "import petname\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "\n",
    "\n",
    "model_type = \"encoder\"\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=12,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=1,\n",
    "  num_decoder_layers=1,\n",
    "  num_heads=2,\n",
    "  d_model=16,\n",
    "  d_ff=16*2,\n",
    "  dropout=0.1\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=20,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/Users/pfh/work/arc-data/flip\"],\n",
    "  train_steps_per_epoch=5,\n",
    "  eval_steps_per_epoch=2,\n",
    ")\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "model_name = petname.generate(words=3, separator='_')  \n",
    "print(model_name)\n",
    "train_on_mac(model_name, num_epochs, model_type, model_params, train_params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"vision\"\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=12,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=1,\n",
    "  num_decoder_layers=1,\n",
    "  num_heads=4,\n",
    "  d_model=32,\n",
    "  d_ff=32*4,\n",
    "  dropout=0.2\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=32,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/move_random_small\", \"/vol/data/flip\"],\n",
    "  loss_class_weights={0: 0.2},\n",
    "  meta_num_epochs=2,\n",
    "  meta_batch_size=10,\n",
    "  meta_learning_rate=1e-4,\n",
    "  meta_weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "model_names = []\n",
    "\n",
    "num_runs = 1\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize-meta\", \"meta_train\")\n",
    "for i in range(num_runs):\n",
    "  model_name = petname.generate(words=3, separator='_')\n",
    "  fn_call = fn.spawn(model_name, num_epochs, model_type, model_params, train_params)\n",
    "  print(\"Model name\", model_name, model_type, fn_call.object_id)\n",
    "  model_names.append(model_name)\n",
    "\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "# model_names = ['subtly_moral_bee', 'newly_mint_kite']\n",
    "model_names = ['kindly_living_spider']\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=20,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/html_dim_20_20240925\", \"/vol/data/move_many_random\", \"/vol/data/rotate\", \"/vol/data/scale\", \"/vol/data/flip\"],\n",
    "  loss_class_weights={0: 0.2}\n",
    ")\n",
    "train_params = None\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"train\")\n",
    "for model_name in model_names:\n",
    "  fn_call = fn.spawn(model_name, num_epochs, None, train_params)\n",
    "  print(\"Model name\", model_name, fn_call.object_id)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.train import ARCModelState\n",
    "from arc_prize.vis import visualize_epochs\n",
    "\n",
    "def visualize_group(model_names: list[str]):\n",
    "  epochs = {}\n",
    "  get_model = modal.Function.lookup(\"arc-prize\", \"get_model\")\n",
    "  for name in model_names:\n",
    "    checkpoint = ARCModelState(**get_model.remote(name))\n",
    "    print(name, len(checkpoint.epochs), checkpoint.epochs[-1], checkpoint.model_params)\n",
    "    epochs[name] = checkpoint.epochs\n",
    "\n",
    "\n",
    "    # print(len(checkpoint.encoder_attn_weights))\n",
    "    # for b, batch in enumerate(checkpoint.encoder_attn_weights):\n",
    "    #   for i, layer in enumerate(batch):\n",
    "    #     visualize_all_heads(layer, title=f\"Batch {b}, layer {i}\")\n",
    "    \n",
    "\n",
    "  visualize_epochs(epochs)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "groups = [\n",
    "  # ['kindly_huge_jennet', 'lovely_tidy_lab', 'solely_living_leech'], # BEST\n",
    "  # ['weekly_enough_moose', 'gently_known_beagle', 'nicely_robust_rhino'], # 20x20 too slow\n",
    "  # ['wildly_firm_husky', 'surely_brief_bug', 'fully_better_dodo'], # Amazing\n",
    "  # ['wildly_steady_iguana', 'yearly_smart_donkey', 'mainly_polite_bison'], # Includes scale dataset\n",
    "  # ['partly_vocal_piglet', 'neatly_needed_liger', 'firmly_game_weevil'], # Scale and diagonal\n",
    "  # ['wholly_tops_heron', 'solely_eager_foal', 'deeply_one_skink'], # Tons of data\n",
    "  # ['unduly_glad_swift', 'purely_steady_hornet', 'humbly_civil_donkey'], # Basic data\n",
    "  # [\"early_civil_beetle\"],\n",
    "  # [\"really_fancy_kitten\", \"mildly_humble_tahr\"],\n",
    "  # ['solely_brief_shad', 'fairly_amazed_hyena', 'vastly_amazed_bobcat'],\n",
    "  # ['barely_sound_viper', 'lively_key_goblin', 'wildly_fancy_glider'], # patch size 3\n",
    "  # ['mildly_able_horse', 'vastly_normal_rhino', 'oddly_mint_clam'], # patch size 2\n",
    "  # ['firmly_tops_adder', 'yearly_normal_puma', 'slowly_more_caiman'], # patch size 2, simpler embedding\n",
    "  # ['safely_poetic_adder', 'vastly_close_horse', 'fairly_legal_insect', 'daily_actual_monkey'], # Patch embedding with ARC pos encoding\n",
    "  # ['nicely_wired_mouse', 'freely_up_shrimp', 'hardly_loving_mullet', 'gladly_active_muskox']\n",
    "  # ['mostly_normal_dog', 'lively_pure_hawk', 'rarely_tender_roughy'], # HUGE dataset re_arc_dim_12\n",
    "  # ['daily_pro_cattle', 'newly_suited_finch', 'rarely_tender_roughy'], # 32 batch size, 64 dim\n",
    "  # ['solely_sound_sponge', 'lively_sacred_egret'], # 32 batch size, 128 dim, 3+3 layers\n",
    "  # ['nicely_pure_leech', 'namely_sure_emu'], # 128 dim, 4+4 layers\n",
    "  # ['subtly_moral_bee', 'newly_mint_kite', 'solely_busy_skunk'], # Vision transformer large models\n",
    "  # ['newly_mint_kite', 'namely_caring_mite', 'overly_enough_tomcat', 'subtly_known_panda', 'mildly_ruling_hog', 'simply_tight_fowl'],\n",
    "  # ['subtly_known_panda', 'barely_clean_cicada', 'sadly_real_viper'], # 512 dim (12x12, 20x20, 30x30)\n",
    "  # ['lively_fleet_goat'], # vision 20x20\n",
    "  [\"kindly_living_spider\", \"gladly_prompt_koi\"] # encoder vs normal\n",
    "]\n",
    "\n",
    "# print([group for sublist in groups for group in sublist])\n",
    "for group in groups:\n",
    "  visualize_group(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "\n",
    "\n",
    "eval_model = modal.Function.lookup(\"arc-prize\", \"evaluate_model\")\n",
    "output = eval_model.remote(\"kindly_living_spider\", [\"/vol/data/re_arc_dim_12_small\"])\n",
    "# output = eval_model.remote(\"overly_hip_egret\", [\"/vol/data/re_arc/00d62c1b.json\"], True, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"finetune_and_predict\")\n",
    "output = fn.remote(\"kindly_living_spider\", [\"/vol/data/re_arc_dim_12_small\"], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for item in output:\n",
    "    if np.array_equal(item[\"output_grid\"], item[\"predictions\"]) is True:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(correct, total, correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from arc_prize.vis import visualize_tensors, visualize_all_heads\n",
    "from arc_prize.vis import visualize_mean_mha_attention\n",
    "\n",
    "\n",
    "def visualize_mean_attention(attention_weights: torch.Tensor, num_grids: int, grid_size: int):\n",
    "    # Reshape the attention weights\n",
    "    # From [4, 100, 900] to [4, 100, 9, 10, 10]\n",
    "    num_heads = attention_weights.shape[0]\n",
    "    reshaped_attention = attention_weights.view(\n",
    "        num_heads, num_grids, grid_size, grid_size\n",
    "    )\n",
    "\n",
    "    # Calculate mean attention across the target sequence (dim=1)\n",
    "    # mean_attention = reshaped_attention.mean(dim=1)  # Shape: [4, 9, 10, 10]\n",
    "    mean_attention = reshaped_attention\n",
    "\n",
    "    # Create a figure with subplots for each head\n",
    "    fig, axes = plt.subplots(num_heads, num_grids, figsize=(20, 10))\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        # Create a 3x3 grid of heatmaps\n",
    "        for i in range(num_grids):\n",
    "            grid_attention = mean_attention[head, i]\n",
    "\n",
    "            # Add subplot within the head's subplot\n",
    "            # sub_ax = ax.inset_axes([1/9])\n",
    "            ax = axes[head, i]\n",
    "            im = ax.imshow(grid_attention, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "            # ax.axis(\"off\")\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(f\"Head {head + 1}\", rotation=0, ha=\"right\", va=\"center\")\n",
    "\n",
    "            # Add colorbar for each grid\n",
    "            # plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Remove ticks from the main subplot\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for item in output:\n",
    "#   if np.array_equal(item[\"output_grid\"], item[\"predictions\"]) is True:\n",
    "    visualize_tensors(torch.Tensor(item[\"grids\"]).squeeze(0), torch.Tensor(item[\"output_grid\"]).squeeze(0), torch.Tensor(item[\"predictions\"]).squeeze(0), torch.Tensor(item[\"finetune_predictions\"]))\n",
    "# print(torch.Tensor(item[\"decoder_sa_attn_weights\"]).shape)\n",
    "  # for i, layer in enumerate(torch.Tensor(item[\"decoder_mha_attn_weights\"]).squeeze(0)):\n",
    "  #   print(layer.shape)\n",
    "  #   visualize_mean_attention(layer, 9, 30)\n",
    "    \n",
    "\n",
    "  # visualize_all_heads(layer, title=f\"Layer {i}\")\n",
    "# for i, layer in enumerate(torch.Tensor(item[\"decoder_sa_attn_weights\"]).squeeze(0)):\n",
    "#     visualize_mean_sa_attention(layer)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "\n",
    "model_file_path = \"models/subtly_known_panda.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_dict = torch.load(model_file_path, weights_only=False, map_location=device)\n",
    "checkpoint = ARCModelState(**checkpoint_dict)\n",
    "\n",
    "# ARCTransformerEncoderDecoderParams(grid_dim=12, num_train_pairs=4, num_colors=10, num_encoder_layers=6, num_decoder_layers=6, num_heads=16, d_model=512, d_ff=2048, dropout=0.3)\n",
    "\n",
    "kaggle_model_file_path = \"kaggle/models/subtly_known_panda.pth\"\n",
    "torch.save(checkpoint.model_state_dict, kaggle_model_file_path)\n",
    "# kaggle_checkpoint = ARCKaggleModelState(model_params=checkpoint.model_params, model_state_dict=checkpoint.model_state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_output(output: torch.Tensor) -> torch.Tensor:\n",
    "    output = output - 1\n",
    "\n",
    "    filtered_rows = [row[row != -1] for row in output]\n",
    "    \n",
    "    max_length = max(len(row) for row in filtered_rows)\n",
    "\n",
    "    padded_rows = [torch.cat([row, torch.zeros(max_length - len(row), dtype=row.dtype, device=device)]) for row in filtered_rows]\n",
    "\n",
    "    return torch.stack(padded_rows)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
