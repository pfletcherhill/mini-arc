{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.train import ARCModelState, ARCTrainParams\n",
    "from arc_prize.vis import visualize_epochs\n",
    "import modal\n",
    "import torch\n",
    "import petname\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.data import ARCDatasetParams, ReARCDataset, make_re_arc_data_loaders\n",
    "\n",
    "config = ARCDatasetParams(max_grid_size=30, max_train_grids=10, color_offset=1)\n",
    "dataset = ReARCDataset(\"data/re_arc/ff805c23.json\", config)\n",
    "train_loader, val_loader = make_re_arc_data_loaders([\"data/re_arc/ff805c23.json\"], 10, config)\n",
    "len(val_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name neatly_nice_minnow encoder fc-01JBJ18SKXEDS6BE1JVN5XPS7H\n",
      "Model name jolly_neat_imp encoder fc-01JBJ18SP5S0VZFR8PK876463M\n",
      "['neatly_nice_minnow', 'jolly_neat_imp']\n"
     ]
    }
   ],
   "source": [
    "import modal\n",
    "from arc_prize.train import ARCTrainParams\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "import petname\n",
    "\n",
    "model_type = \"encoder\"\n",
    "\n",
    "\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=12,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=6,\n",
    "  num_decoder_layers=0,\n",
    "  num_heads=8,\n",
    "  d_model=128,\n",
    "  d_ff=128*4,\n",
    "  dropout=0.2\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=32,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/html_dim_12_20241023\", \"/vol/data/re_arc_dim_12\"],\n",
    "  loss_class_weights={0: 0.2},\n",
    "  train_steps_per_epoch=10,\n",
    "  eval_steps_per_epoch=2\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "model_names = []\n",
    "\n",
    "num_runs = 1\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"train\")\n",
    "for i in range(num_runs):\n",
    "  model_name = petname.generate(words=3, separator='_')\n",
    "  fn_call = fn.spawn(model_name, num_epochs, model_type, model_params, train_params)\n",
    "  print(\"Model name\", model_name, model_type, fn_call.object_id)\n",
    "  model_names.append(model_name)\n",
    "\n",
    "print(model_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kindly_kind_asp\n",
      "Starting new model kindly_kind_asp\n",
      "Using single CPU\n",
      "Starting training run with dataset of 1000 training items and 200 evaluation items: /Users/pfh/work/arc-data/flip\n",
      "Using batch size of 20\n",
      "Epoch 1/5 for models/kindly_kind_asp.pth:\n",
      "Train Loss: 2.7747, Train Accuracy: 0.0458, Train Steps: 2\n",
      "Val Loss: 2.7704, Val Accuracy: 0.0503, Val Steps: 1\n",
      "Epoch duration: 15.34s (0.26m)\n",
      "New best val loss 2.7703728675842285\n",
      "Saved checkpoint models/kindly_kind_asp.pth\n",
      "Epoch 2/5 for models/kindly_kind_asp.pth:\n",
      "Train Loss: 2.7634, Train Accuracy: 0.0458, Train Steps: 2\n",
      "Val Loss: 2.7729, Val Accuracy: 0.0486, Val Steps: 1\n",
      "Epoch duration: 16.14s (0.27m)\n",
      "Saved checkpoint models/kindly_kind_asp.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m model_name \u001b[38;5;241m=\u001b[39m petname\u001b[38;5;241m.\u001b[39mgenerate(words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_name)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrain_on_mac\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/arc-prize/arc_prize/train.py:523\u001b[0m, in \u001b[0;36mtrain_on_mac\u001b[0;34m(model_name, num_epochs, model_type, model_params, train_params)\u001b[0m\n\u001b[1;32m    512\u001b[0m     model_state \u001b[38;5;241m=\u001b[39m ARCModelState(\n\u001b[1;32m    513\u001b[0m         model_type\u001b[38;5;241m=\u001b[39mmodel_type,\n\u001b[1;32m    514\u001b[0m         model_state_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         best_val_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    520\u001b[0m     )\n\u001b[1;32m    521\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model_state\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, model_filename)\n\u001b[0;32m--> 523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_arc_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/arc-prize/arc_prize/train.py:390\u001b[0m, in \u001b[0;36mtrain_arc_transformer\u001b[0;34m(model_filename, num_epochs, patience, train_params, force_compile)\u001b[0m\n\u001b[1;32m    386\u001b[0m grids, masks, target_grid \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m    388\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 390\u001b[0m output, loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_grid\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_compilation:\n\u001b[1;32m    395\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/work/arc-prize/arc_prize/train.py:342\u001b[0m, in \u001b[0;36mtrain_arc_transformer.<locals>.training_step\u001b[0;34m(model, grids, masks, target_grid)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(model, grids, masks, target_grid):\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast(device\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m--> 342\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m    344\u001b[0m             output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shapes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    345\u001b[0m             target_grid\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong(),\n\u001b[1;32m    346\u001b[0m         )\n\u001b[1;32m    347\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/work/arc-prize/arc_prize/train.py:309\u001b[0m, in \u001b[0;36mtrain_arc_transformer.<locals>.forward_pass\u001b[0;34m(model, grids, masks)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_pass\u001b[39m(model, grids, masks):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m output\n",
      "File \u001b[0;32m~/work/arc-prize/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/arc-prize/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/arc-prize/arc_prize/model.py:492\u001b[0m, in \u001b[0;36mARCTransformerEncoder.forward\u001b[0;34m(self, src, src_mask, tgt, temperature)\u001b[0m\n\u001b[1;32m    480\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39msrc_mask\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    482\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    483\u001b[0m     [\n\u001b[1;32m    484\u001b[0m         padding_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    490\u001b[0m )\n\u001b[0;32m--> 492\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# Get only the output grid portion\u001b[39;00m\n\u001b[1;32m    497\u001b[0m output_grid_portion \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_seq_len :, :]\n",
      "File \u001b[0;32m~/work/arc-prize/arc_prize/model.py:115\u001b[0m, in \u001b[0;36mEncoderWithAttention.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, need_weights)\u001b[0m\n\u001b[1;32m    112\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 115\u001b[0m     output, layer_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m need_weights:\n\u001b[1;32m    122\u001b[0m         attn_weights\u001b[38;5;241m.\u001b[39mappend(layer_attn_weights)\n",
      "File \u001b[0;32m~/work/arc-prize/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/arc-prize/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/arc-prize/arc_prize/model.py:58\u001b[0m, in \u001b[0;36mEncoderLayerWithAttention.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal, need_weights)\u001b[0m\n\u001b[1;32m     48\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39m_canonical_mask(\n\u001b[1;32m     49\u001b[0m     mask\u001b[38;5;241m=\u001b[39msrc_mask,\n\u001b[1;32m     50\u001b[0m     mask_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     check_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n\u001b[0;32m---> 58\u001b[0m x1, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x1)\n\u001b[1;32m     70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n",
      "File \u001b[0;32m~/work/arc-prize/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/arc-prize/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/arc-prize/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:1275\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1262\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1273\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/work/arc-prize/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5560\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5557\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   5558\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 5560\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5561\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   5563\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from arc_prize.train import ARCTrainParams, train_on_mac\n",
    "import petname\n",
    "from arc_prize.model import ARCTransformerEncoderDecoderParams\n",
    "\n",
    "\n",
    "model_type = \"encoder\"\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=12,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=1,\n",
    "  num_decoder_layers=1,\n",
    "  num_heads=2,\n",
    "  d_model=16,\n",
    "  d_ff=16*2,\n",
    "  dropout=0.1\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=20,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/Users/pfh/work/arc-data/flip\"],\n",
    "  train_steps_per_epoch=2,\n",
    "  eval_steps_per_epoch=1,\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "model_name = petname.generate(words=3, separator='_')  \n",
    "print(model_name)\n",
    "train_on_mac(model_name, num_epochs, model_type, model_params, train_params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"vision\"\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "  grid_dim=12,\n",
    "  num_train_pairs=4,\n",
    "  num_colors=10,\n",
    "  num_encoder_layers=1,\n",
    "  num_decoder_layers=1,\n",
    "  num_heads=4,\n",
    "  d_model=32,\n",
    "  d_ff=32*4,\n",
    "  dropout=0.2\n",
    ")\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=32,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/move_random_small\", \"/vol/data/flip\"],\n",
    "  loss_class_weights={0: 0.2},\n",
    "  meta_num_epochs=2,\n",
    "  meta_batch_size=10,\n",
    "  meta_learning_rate=1e-4,\n",
    "  meta_weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "model_names = []\n",
    "\n",
    "num_runs = 1\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize-meta\", \"meta_train\")\n",
    "for i in range(num_runs):\n",
    "  model_name = petname.generate(words=3, separator='_')\n",
    "  fn_call = fn.spawn(model_name, num_epochs, model_type, model_params, train_params)\n",
    "  print(\"Model name\", model_name, model_type, fn_call.object_id)\n",
    "  model_names.append(model_name)\n",
    "\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "# model_names = ['subtly_moral_bee', 'newly_mint_kite']\n",
    "model_names = ['kindly_living_spider']\n",
    "\n",
    "train_params = ARCTrainParams(\n",
    "  batch_size=20,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=1e-4,\n",
    "  dataset_dir=[\"/vol/data/html_dim_20_20240925\", \"/vol/data/move_many_random\", \"/vol/data/rotate\", \"/vol/data/scale\", \"/vol/data/flip\"],\n",
    "  loss_class_weights={0: 0.2}\n",
    ")\n",
    "train_params = None\n",
    "\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"train\")\n",
    "for model_name in model_names:\n",
    "  fn_call = fn.spawn(model_name, num_epochs, None, train_params)\n",
    "  print(\"Model name\", model_name, fn_call.object_id)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.train import ARCModelState\n",
    "from arc_prize.vis import visualize_epochs\n",
    "\n",
    "def visualize_group(model_names: list[str]):\n",
    "  epochs = {}\n",
    "  get_model = modal.Function.lookup(\"arc-prize\", \"get_model\")\n",
    "  for name in model_names:\n",
    "    checkpoint = ARCModelState(**get_model.remote(name))\n",
    "    print(name, len(checkpoint.epochs), checkpoint.epochs[-1], checkpoint.model_params)\n",
    "    epochs[name] = checkpoint.epochs\n",
    "\n",
    "\n",
    "    # print(len(checkpoint.encoder_attn_weights))\n",
    "    # for b, batch in enumerate(checkpoint.encoder_attn_weights):\n",
    "    #   for i, layer in enumerate(batch):\n",
    "    #     visualize_all_heads(layer, title=f\"Batch {b}, layer {i}\")\n",
    "    \n",
    "\n",
    "  visualize_epochs(epochs)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "groups = [\n",
    "  # ['kindly_huge_jennet', 'lovely_tidy_lab', 'solely_living_leech'], # BEST\n",
    "  # ['weekly_enough_moose', 'gently_known_beagle', 'nicely_robust_rhino'], # 20x20 too slow\n",
    "  # ['wildly_firm_husky', 'surely_brief_bug', 'fully_better_dodo'], # Amazing\n",
    "  # ['wildly_steady_iguana', 'yearly_smart_donkey', 'mainly_polite_bison'], # Includes scale dataset\n",
    "  # ['partly_vocal_piglet', 'neatly_needed_liger', 'firmly_game_weevil'], # Scale and diagonal\n",
    "  # ['wholly_tops_heron', 'solely_eager_foal', 'deeply_one_skink'], # Tons of data\n",
    "  # ['unduly_glad_swift', 'purely_steady_hornet', 'humbly_civil_donkey'], # Basic data\n",
    "  # [\"early_civil_beetle\"],\n",
    "  # [\"really_fancy_kitten\", \"mildly_humble_tahr\"],\n",
    "  # ['solely_brief_shad', 'fairly_amazed_hyena', 'vastly_amazed_bobcat'],\n",
    "  # ['barely_sound_viper', 'lively_key_goblin', 'wildly_fancy_glider'], # patch size 3\n",
    "  # ['mildly_able_horse', 'vastly_normal_rhino', 'oddly_mint_clam'], # patch size 2\n",
    "  # ['firmly_tops_adder', 'yearly_normal_puma', 'slowly_more_caiman'], # patch size 2, simpler embedding\n",
    "  # ['safely_poetic_adder', 'vastly_close_horse', 'fairly_legal_insect', 'daily_actual_monkey'], # Patch embedding with ARC pos encoding\n",
    "  # ['nicely_wired_mouse', 'freely_up_shrimp', 'hardly_loving_mullet', 'gladly_active_muskox']\n",
    "  # ['mostly_normal_dog', 'lively_pure_hawk', 'rarely_tender_roughy'], # HUGE dataset re_arc_dim_12\n",
    "  # ['daily_pro_cattle', 'newly_suited_finch', 'rarely_tender_roughy'], # 32 batch size, 64 dim\n",
    "  # ['solely_sound_sponge', 'lively_sacred_egret'], # 32 batch size, 128 dim, 3+3 layers\n",
    "  # ['nicely_pure_leech', 'namely_sure_emu'], # 128 dim, 4+4 layers\n",
    "  # ['subtly_moral_bee', 'newly_mint_kite', 'solely_busy_skunk'], # Vision transformer large models\n",
    "  # ['newly_mint_kite', 'namely_caring_mite', 'overly_enough_tomcat', 'subtly_known_panda', 'mildly_ruling_hog', 'simply_tight_fowl'],\n",
    "  # ['subtly_known_panda', 'barely_clean_cicada', 'sadly_real_viper'], # 512 dim (12x12, 20x20, 30x30)\n",
    "  # ['lively_fleet_goat'], # vision 20x20\n",
    "  [\"kindly_living_spider\", \"gladly_prompt_koi\"] # encoder vs normal\n",
    "]\n",
    "\n",
    "# print([group for sublist in groups for group in sublist])\n",
    "for group in groups:\n",
    "  visualize_group(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "\n",
    "\n",
    "eval_model = modal.Function.lookup(\"arc-prize\", \"evaluate_model\")\n",
    "output = eval_model.remote(\"kindly_living_spider\", [\"/vol/data/re_arc_dim_12_small\"])\n",
    "# output = eval_model.remote(\"overly_hip_egret\", [\"/vol/data/re_arc/00d62c1b.json\"], True, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "fn = modal.Function.lookup(\"arc-prize\", \"finetune_and_predict\")\n",
    "output = fn.remote(\"kindly_living_spider\", [\"/vol/data/re_arc_dim_12_small\"], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for item in output:\n",
    "    if np.array_equal(item[\"output_grid\"], item[\"predictions\"]) is True:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(correct, total, correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from arc_prize.vis import visualize_tensors, visualize_all_heads\n",
    "from arc_prize.vis import visualize_mean_mha_attention\n",
    "\n",
    "\n",
    "def visualize_mean_attention(attention_weights: torch.Tensor, num_grids: int, grid_size: int):\n",
    "    # Reshape the attention weights\n",
    "    # From [4, 100, 900] to [4, 100, 9, 10, 10]\n",
    "    num_heads = attention_weights.shape[0]\n",
    "    reshaped_attention = attention_weights.view(\n",
    "        num_heads, num_grids, grid_size, grid_size\n",
    "    )\n",
    "\n",
    "    # Calculate mean attention across the target sequence (dim=1)\n",
    "    # mean_attention = reshaped_attention.mean(dim=1)  # Shape: [4, 9, 10, 10]\n",
    "    mean_attention = reshaped_attention\n",
    "\n",
    "    # Create a figure with subplots for each head\n",
    "    fig, axes = plt.subplots(num_heads, num_grids, figsize=(20, 10))\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        # Create a 3x3 grid of heatmaps\n",
    "        for i in range(num_grids):\n",
    "            grid_attention = mean_attention[head, i]\n",
    "\n",
    "            # Add subplot within the head's subplot\n",
    "            # sub_ax = ax.inset_axes([1/9])\n",
    "            ax = axes[head, i]\n",
    "            im = ax.imshow(grid_attention, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "            # ax.axis(\"off\")\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(f\"Head {head + 1}\", rotation=0, ha=\"right\", va=\"center\")\n",
    "\n",
    "            # Add colorbar for each grid\n",
    "            # plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Remove ticks from the main subplot\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for item in output:\n",
    "#   if np.array_equal(item[\"output_grid\"], item[\"predictions\"]) is True:\n",
    "    visualize_tensors(torch.Tensor(item[\"grids\"]).squeeze(0), torch.Tensor(item[\"output_grid\"]).squeeze(0), torch.Tensor(item[\"predictions\"]).squeeze(0), torch.Tensor(item[\"finetune_predictions\"]))\n",
    "# print(torch.Tensor(item[\"decoder_sa_attn_weights\"]).shape)\n",
    "  # for i, layer in enumerate(torch.Tensor(item[\"decoder_mha_attn_weights\"]).squeeze(0)):\n",
    "  #   print(layer.shape)\n",
    "  #   visualize_mean_attention(layer, 9, 30)\n",
    "    \n",
    "\n",
    "  # visualize_all_heads(layer, title=f\"Layer {i}\")\n",
    "# for i, layer in enumerate(torch.Tensor(item[\"decoder_sa_attn_weights\"]).squeeze(0)):\n",
    "#     visualize_mean_sa_attention(layer)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "\n",
    "model_file_path = \"models/subtly_known_panda.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_dict = torch.load(model_file_path, weights_only=False, map_location=device)\n",
    "checkpoint = ARCModelState(**checkpoint_dict)\n",
    "\n",
    "# ARCTransformerEncoderDecoderParams(grid_dim=12, num_train_pairs=4, num_colors=10, num_encoder_layers=6, num_decoder_layers=6, num_heads=16, d_model=512, d_ff=2048, dropout=0.3)\n",
    "\n",
    "kaggle_model_file_path = \"kaggle/models/subtly_known_panda.pth\"\n",
    "torch.save(checkpoint.model_state_dict, kaggle_model_file_path)\n",
    "# kaggle_checkpoint = ARCKaggleModelState(model_params=checkpoint.model_params, model_state_dict=checkpoint.model_state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_output(output: torch.Tensor) -> torch.Tensor:\n",
    "    output = output - 1\n",
    "\n",
    "    filtered_rows = [row[row != -1] for row in output]\n",
    "    \n",
    "    max_length = max(len(row) for row in filtered_rows)\n",
    "\n",
    "    padded_rows = [torch.cat([row, torch.zeros(max_length - len(row), dtype=row.dtype, device=device)]) for row in filtered_rows]\n",
    "\n",
    "    return torch.stack(padded_rows)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
