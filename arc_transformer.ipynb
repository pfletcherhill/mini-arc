{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(\"data/arc-agi_training_challenges.json\", \"r\") as f:\n",
    "  tasks = json.load(f)\n",
    "  max_height = 0\n",
    "  max_width = 0\n",
    "  max_count = 0\n",
    "  for task in tasks.values():\n",
    "    count = len(task[\"train\"])\n",
    "    if count > max_count:\n",
    "      max_count = count\n",
    "    for pair in task[\"train\"]:\n",
    "      input = np.array(pair[\"input\"])\n",
    "      height, width = input.shape\n",
    "      if height > max_height:\n",
    "        max_height = height\n",
    "      if width > max_width:\n",
    "        max_width = width\n",
    "  \n",
    "  print(max_count, max_height, max_width)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "from arc_prize.synth_data import generate_dataset\n",
    "from arc_prize.vis import visualize_grids\n",
    "\n",
    "\n",
    "challenges, solutions = generate_dataset(num_tasks=20)\n",
    "eval_challenges, eval_solutions = generate_dataset(num_tasks=10)\n",
    "\n",
    "file_path_root = \"data/arc-synth_move_right\"\n",
    "with open(f\"{file_path_root}_training_challenges.json\", \"w\") as f:\n",
    "   json.dump(challenges, f)\n",
    "with open(f\"{file_path_root}_training_solutions.json\", \"w\") as f:\n",
    "   json.dump(solutions, f)\n",
    "with open(f\"{file_path_root}_evaluation_challenges.json\", \"w\") as f:\n",
    "   json.dump(eval_challenges, f)\n",
    "with open(f\"{file_path_root}_evaluation_solutions.json\", \"w\") as f:\n",
    "   json.dump(eval_solutions, f)\n",
    "\n",
    "\n",
    "\n",
    "# Print a sample task to verify the format\n",
    "sample_task = next(iter(challenges.values()))\n",
    "\n",
    "for task in iter(challenges.values()):\n",
    "  visualize_grids(task[\"train\"], task[\"test\"][0][\"input\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ARCTransformer(nn.Module):\n",
    "    def __init__(self, grid_dim=30, num_train_pairs=10, num_colors=10, num_layers=4, num_heads=8, d_model=256, d_ff=1024, dropout=0.1):\n",
    "        super(ARCTransformer, self).__init__()\n",
    "        \n",
    "        self.input_dim = grid_dim\n",
    "        self.num_classes = num_colors + 1 # Add padding \n",
    "        self.d_model = d_model\n",
    "        self.num_train_pairs = num_train_pairs\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.num_classes, d_model)\n",
    "        self.pos_encoding = TrainablePositionalEncoding(d_model, max_len=(num_train_pairs * 2 + 1)*grid_dim*grid_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=d_ff, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Enable gradient checkpointing\n",
    "        self.transformer_encoder.use_checkpoint = True\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, self.num_classes)\n",
    "    \n",
    "    def forward(self, grids, masks, output=None):\n",
    "        batch_size, num_grids, height, width = grids.size()\n",
    "        print(\"grids\", grids.size)\n",
    "        \n",
    "        # Apply grid masks\n",
    "        # masked_grids = grids * masks\n",
    "        masked_grids = torch.where(masks, grids, torch.zeros_like(grids))\n",
    "\n",
    "        print(f\"Min value in grids: {grids.min()}\")\n",
    "        print(f\"Max value in grids: {grids.max()}\")\n",
    "        print(f\"Unique values in grids: {torch.unique(grids)}\")\n",
    "        print(\"Masked grids min/max:\", masked_grids.min().item(), masked_grids.max().item())\n",
    "        \n",
    "        # Embed input\n",
    "        x = self.embedding(masked_grids)  # This should now be (batch_size, num_grids, height, width, d_model)\n",
    "        print(f\"After embedding shape: {x.shape}\")\n",
    "        \n",
    "        # Flatten grids\n",
    "        x = x.view(batch_size, num_grids, -1, self.d_model)\n",
    "        print(f\"After flattening shape: {x.shape}\")\n",
    "        \n",
    "        # Add trainable positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        print(f\"After positional encoding shape: {x.shape}\")\n",
    "        \n",
    "        # Flatten the grid_masks to create attention mask\n",
    "        # mask = masks.view(batch_size, num_grids, -1).sum(dim=-1) > 0\n",
    "        mask = masks.view(batch_size, num_grids, -1).bool()  \n",
    "        \n",
    "        # Create attention mask for transformer\n",
    "        # attn_mask = mask.unsqueeze(1) * mask.unsqueeze(2)\n",
    "        seq_len = (self.num_train_pairs * 2 + 1) * self.input_dim * self.input_dim\n",
    "        attn_mask = mask.view(batch_size, -1) \n",
    "        attn_mask = attn_mask.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        attn_mask = ~attn_mask\n",
    "\n",
    "        # Instead of expanding for all heads at once, iterate over each head\n",
    "        attn_mask_final = []\n",
    "        for _ in range(self.num_heads):\n",
    "            attn_mask_final.append(attn_mask)\n",
    "        attn_mask = torch.stack(attn_mask_final) # Shape: (num_heads, batch_size, seq_len, seq_len)\n",
    "        attn_mask = attn_mask.reshape(self.num_heads * batch_size, seq_len, seq_len)\n",
    "        \n",
    "        # attn_mask = attn_mask.unsqueeze(0).expand(self.num_heads, -1, -1, -1) \n",
    "        # attn_mask = attn_mask.reshape(self.num_heads * batch_size, seq_len, seq_len)\n",
    "\n",
    "        key_padding_mask = ~mask.view(batch_size, -1)\n",
    "        print(\"key_padding_mask\", key_padding_mask.shape)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        x = x.view(batch_size, -1, self.d_model)\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
    "        print(f\"Before transformer shape: {x.shape}\")\n",
    "        print(f\"x shape: {x.shape}, dtype: {x.dtype}\")\n",
    "        print(f\"key_padding_mask shape: {key_padding_mask.shape if key_padding_mask is not None else None}, dtype: {key_padding_mask.dtype if key_padding_mask is not None else None}\")\n",
    "        print(f\"attn_mask shape: {attn_mask.shape if attn_mask is not None else None}, dtype: {attn_mask.dtype if attn_mask is not None else None}\")\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n",
    "        print(f\"After transformer shape: {x.shape}\")\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        print(f\"After output layer shape: {x.shape}\")\n",
    "        \n",
    "        # Reshape to match the original grid shape\n",
    "        x = x.view(batch_size, num_grids, height, width, self.num_classes)\n",
    "        print(f\"After reshaping shape: {x.shape}\")\n",
    "        \n",
    "        # If we're in training mode and output is provided, use teacher forcing\n",
    "        if self.training and output is not None:\n",
    "            output_embedded = self.embedding(output)\n",
    "            x[:, -1] = self.output_layer(output_embedded).view(batch_size, height, width, -1)\n",
    "            # x[:, -1] = self.embedding(output).view(batch_size, height, width, -1)\n",
    "        \n",
    "        return x[:, -1]  # Return only the last grid (test output)\n",
    "    \n",
    "\n",
    "class TrainablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(TrainablePositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1) * x.size(2)  # num_grids * (height * width)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(x.size(0), -1)\n",
    "        pos_encodings = self.pos_embedding(positions).view(x.size(0), x.size(1), x.size(2), -1)\n",
    "        return x + pos_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class GridAttention(nn.Module):\n",
    "#     def __init__(self, d_model, nhead):\n",
    "#         super().__init__()\n",
    "#         self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
    "    \n",
    "#     def forward(self, x, mask):\n",
    "#         return self.self_attn(x, x, x, key_padding_mask=mask)[0]\n",
    "\n",
    "# class PairAttention(nn.Module):\n",
    "#     def __init__(self, d_model, nhead):\n",
    "#         super().__init__()\n",
    "#         self.cross_attn = nn.MultiheadAttention(d_model, nhead)\n",
    "    \n",
    "#     def forward(self, x, y, mask_x, mask_y):\n",
    "#         return self.cross_attn(x, y, y, key_padding_mask=mask_y)[0]\n",
    "\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, d_model, nhead, dim_feedforward):\n",
    "#         super().__init__()\n",
    "#         self.self_attn = GridAttention(d_model, nhead)\n",
    "#         self.pair_attn = PairAttention(d_model, nhead)\n",
    "#         self.feed_forward = nn.Sequential(\n",
    "#             nn.Linear(d_model, dim_feedforward),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(dim_feedforward, d_model)\n",
    "#         )\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.norm3 = nn.LayerNorm(d_model)\n",
    "    \n",
    "#     def forward(self, x, y, mask_x, mask_y):\n",
    "#         x = x + self.self_attn(x, mask_x)\n",
    "#         x = self.norm1(x)\n",
    "#         x = x + self.pair_attn(x, y, mask_x, mask_y)\n",
    "#         x = self.norm2(x)\n",
    "#         x = x + self.feed_forward(x)\n",
    "#         x = self.norm3(x)\n",
    "#         return x\n",
    "\n",
    "# class ARCTransformer(nn.Module):\n",
    "#     def __init__(self, d_model, nhead, num_layers, dim_feedforward, max_grid_size, num_colors):\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.embedding = nn.Embedding(num_colors + 1, d_model) # account for offset\n",
    "#         self.pos_encoding = nn.Parameter(torch.randn(1, max_grid_size, max_grid_size, d_model))\n",
    "\n",
    "        \n",
    "#         self.input_layers = nn.ModuleList([TransformerBlock(d_model, nhead, dim_feedforward) for _ in range(num_layers)])\n",
    "#         self.output_layers = nn.ModuleList([TransformerBlock(d_model, nhead, dim_feedforward) for _ in range(num_layers)])\n",
    "#         self.cross_layers = nn.ModuleList([PairAttention(d_model, nhead) for _ in range(num_layers)])\n",
    "        \n",
    "#         self.final_layer = nn.Linear(d_model, num_colors)\n",
    "    \n",
    "#     def embed_grid(self, grid, mask):\n",
    "#         print(f\"Grid shape: {grid.shape}\")\n",
    "#         print(f\"Mask shape: {mask.shape}\")\n",
    "#         print(f\"Pos encoding shape: {self.pos_encoding.shape}\")\n",
    "        \n",
    "#         embedded = self.embedding(grid)\n",
    "#         print(f\"Embedded shape: {embedded.shape}\")\n",
    "        \n",
    "#         embedded = embedded + self.pos_encoding[:, :grid.shape[1], :grid.shape[2], :]\n",
    "#         embedded = embedded.view(embedded.shape[0], -1, self.d_model)\n",
    "#         mask = mask.view(mask.shape[0], -1)\n",
    "#         return embedded, mask\n",
    "\n",
    "    \n",
    "#     def forward(self, batch):\n",
    "#         batch_size = len(batch['task_id'])\n",
    "#         num_train_pairs = 5  # Always 5 pairs, some might be padding\n",
    "\n",
    "#         # Embed train input and output grids\n",
    "#         train_inputs = self.embed_grid(batch['train_input_grids'].view(-1, 30, 30),\n",
    "#                                     batch['train_input_masks'].view(-1, 30, 30))\n",
    "#         train_outputs = self.embed_grid(batch['train_output_grids'].view(-1, 30, 30),\n",
    "#                                         batch['train_output_masks'].view(-1, 30, 30))\n",
    "\n",
    "#         # Reshape to (batch_size, num_train_pairs, 900, d_model)\n",
    "#         train_inputs = train_inputs[0].view(batch_size, num_train_pairs, -1, self.d_model)\n",
    "#         train_outputs = train_outputs[0].view(batch_size, num_train_pairs, -1, self.d_model)\n",
    "\n",
    "#         # Create attention mask for train pairs\n",
    "#         train_pair_mask = batch['train_input_output_grids_mask'].unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "#         # Process train pairs\n",
    "#         for i in range(len(self.input_layers)):\n",
    "#             train_inputs = self.input_layers[i](train_inputs, train_outputs, \n",
    "#                                                 batch['train_input_masks'], batch['train_output_masks'])\n",
    "#             train_outputs = self.output_layers[i](train_outputs, train_inputs, \n",
    "#                                                 batch['train_output_masks'], batch['train_input_masks'])\n",
    "\n",
    "#             # Apply mask to zero out padding pairs\n",
    "#             train_inputs = train_inputs * train_pair_mask\n",
    "#             train_outputs = train_outputs * train_pair_mask\n",
    "\n",
    "#             # Cross-attention between pairs (within each batch item)\n",
    "#             for j in range(num_train_pairs):\n",
    "#                 for k in range(num_train_pairs):\n",
    "#                     if j != k:\n",
    "#                         train_inputs[:, j] = self.cross_layers[i](train_inputs[:, j], train_inputs[:, k], \n",
    "#                                                                 batch['train_input_masks'][:, j], batch['train_input_masks'][:, k])\n",
    "#                         train_outputs[:, j] = self.cross_layers[i](train_outputs[:, j], train_outputs[:, k], \n",
    "#                                                                 batch['train_output_masks'][:, j], batch['train_output_masks'][:, k])\n",
    "\n",
    "#         # Embed and process test input grid\n",
    "#         test_input, test_mask = self.embed_grid(batch['test_input_grid'], batch['test_input_mask'])\n",
    "\n",
    "#         # Process test input with attention to train pairs\n",
    "#         for i in range(len(self.input_layers)):\n",
    "#             test_input = self.input_layers[i](test_input, test_input, test_mask, test_mask)\n",
    "#             for j in range(num_train_pairs):\n",
    "#                 test_input = self.cross_layers[i](test_input, train_inputs[:, j], \n",
    "#                                                 test_mask, batch['train_input_masks'][:, j])\n",
    "#                 test_input = self.cross_layers[i](test_input, train_outputs[:, j], \n",
    "#                                                 test_mask, batch['train_output_masks'][:, j])\n",
    "\n",
    "#         # Generate output\n",
    "#         output = self.final_layer(test_input)\n",
    "#         output = output.view(batch['test_input_grid'].shape)\n",
    "\n",
    "#         return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def masked_cross_entropy_loss(predictions, targets, mask):\n",
    "    B, H, W, C = predictions.shape\n",
    "    predictions = predictions.contiguous().view(B*H*W, C)\n",
    "    targets = targets.contiguous().view(B*H*W)\n",
    "    mask = mask.contiguous().view(B*H*W)\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss(reduction='none')(predictions, targets)\n",
    "    masked_loss = (loss * mask.float()).sum() / mask.float().sum()\n",
    "    return masked_loss\n",
    "\n",
    "def train_arc_transformer(model, train_loader, val_loader, num_epochs, learning_rate, device):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            grids, grid_masks, output_grid = [item.to(device) for item in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            print(\"before predictions\", grids.shape, grid_masks.shape)\n",
    "            if output_grid is not None:\n",
    "                print(\"output shape\", output_grid.shape)\n",
    "            \n",
    "            predictions = model(grids, grid_masks, output_grid)\n",
    "\n",
    "            print(\"predictions\", predictions.shape)\n",
    "            \n",
    "            loss = masked_cross_entropy_loss(predictions, output_grid, grid_masks[:, -1])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                grids, grid_masks, output_grid = [item.to(device) for item in batch]\n",
    "                \n",
    "                predictions = model(grids, grid_masks)\n",
    "                \n",
    "                loss = masked_cross_entropy_loss(predictions, output_grid, grid_masks[:, -1])\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from arc_prize.data import ARCDataset, ARCDatasetConfig, collate_arc_fn\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 32\n",
    "num_layers = 4\n",
    "dim_feedforward = 128\n",
    "max_grid_size = 10 # 30\n",
    "num_heads = 4\n",
    "max_context_pairs = 5 # 10\n",
    "batch_size = 5\n",
    "num_epochs = 5\n",
    "num_colors = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "synth_arc_dataset_config = ARCDatasetConfig(max_grid_size=max_grid_size, max_train_grids=max_context_pairs, color_offset=1)\n",
    "\n",
    "train_dataset = ARCDataset(\"data/arc-synth_move_right_training_challenges.json\", \"data/arc-synth_move_right_training_solutions.json\", config=synth_arc_dataset_config)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_arc_fn, num_workers=0)\n",
    "\n",
    "val_dataset = ARCDataset(\"data/arc-synth_move_right_evaluation_challenges.json\", \"data/arc-synth_move_right_evaluation_solutions.json\", config=synth_arc_dataset_config)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_arc_fn, num_workers=0)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ARCTransformer(d_model=d_model, num_heads=num_heads, num_layers=num_layers, d_ff=dim_feedforward, grid_dim=max_grid_size, num_colors=num_colors, num_train_pairs=max_context_pairs).to(device)\n",
    "\n",
    "# Train the model\n",
    "# train_arc_transformer(model=model, train_loader=train_loader, val_loader=val_loader, num_epochs=num_epochs, learning_rate=learning_rate, device=device)\n",
    "\n",
    "model_file_name = \"arc_synth_transformer_model.pth\"\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), model_file_name)\n",
    "\n",
    "print(\"Training completed and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# arc_dataset_config = ARCDatasetConfig(max_grid_size=max_grid_size, max_train_grids=max_context_pairs, color_offset=1)\n",
    "\n",
    "# train_dataset = ARCDataset(\"data/arc-agi_training_challenges.json\", \"data/arc-agi_training_solutions.json\", config=arc_dataset_config)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_arc_fn, num_workers=0)\n",
    "\n",
    "# val_dataset = ARCDataset(\"data/arc-agi_evaluation_challenges.json\", \"data/arc-agi_evaluation_solutions.json\", config=arc_dataset_config)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_arc_fn, num_workers=0)\n",
    "\n",
    "# # Initialize model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = ARCTransformer(d_model=d_model, num_heads=num_heads, num_layers=num_layers, d_ff=dim_feedforward, grid_dim=max_grid_size, num_colors=num_colors, num_train_pairs=max_context_pairs).to(device)\n",
    "\n",
    "# # Train the model\n",
    "# train_arc_transformer(model=model, train_loader=train_loader, val_loader=val_loader, num_epochs=num_epochs, learning_rate=learning_rate, device=device)\n",
    "\n",
    "# model_file_name = \"arc_transformer_model.pth\"\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), model_file_name)\n",
    "\n",
    "# print(\"Training completed and model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
