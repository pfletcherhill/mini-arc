{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.amp.autocast_mode import autocast\n",
    "from torch.amp.grad_scaler import GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_challenges_file_name = \"data/arc-agi_evaluation_challenges.json\"\n",
    "kaggle_model_file_path = \"kaggle/models/kindly_exact_beagle_4.pth\"\n",
    "kaggle_submission_file_path = \"kaggle/submission.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ARCDatasetParams:\n",
    "    max_grid_size: int = 30\n",
    "    max_train_grids: int = 10\n",
    "    color_offset: int = 1\n",
    "\n",
    "\n",
    "def pad_and_mask_grid(\n",
    "    grid: list[list[int]], config: ARCDatasetParams\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    h, w = len(grid), len(grid[0])\n",
    "    \n",
    "    # Calculate how much of the grid we can use\n",
    "    h_to_use = min(h, config.max_grid_size)\n",
    "    w_to_use = min(w, config.max_grid_size)\n",
    "    \n",
    "    # If grid is too big, take the center portion\n",
    "    if h > config.max_grid_size or w > config.max_grid_size:\n",
    "        print(\"Warning: grid size too large\")\n",
    "        h_start = (h - h_to_use) // 2\n",
    "        w_start = (w - w_to_use) // 2\n",
    "        grid = [row[w_start:w_start + w_to_use] for row in grid[h_start:h_start + h_to_use]]\n",
    "\n",
    "    # Now h_to_use, w_to_use are our actual grid dimensions\n",
    "    h, w = h_to_use, w_to_use\n",
    "\n",
    "    padded = torch.zeros((config.max_grid_size, config.max_grid_size), dtype=torch.int)\n",
    "    mask = torch.zeros((config.max_grid_size, config.max_grid_size), dtype=torch.bool)\n",
    "\n",
    "    # Calculate padding for the portion we're using\n",
    "    pad_h = (config.max_grid_size - h) // 2\n",
    "    pad_w = (config.max_grid_size - w) // 2\n",
    "\n",
    "    # Place the grid in the center\n",
    "    padded[pad_h : pad_h + h, pad_w : pad_w + w] = (\n",
    "        torch.tensor(grid, dtype=torch.int) + config.color_offset\n",
    "    )\n",
    "    mask[pad_h : pad_h + h, pad_w : pad_w + w] = True\n",
    "\n",
    "    return (padded, mask)\n",
    "\n",
    "\n",
    "class ARCKaggleDataset(Dataset):\n",
    "    challenges: dict[str, dict]\n",
    "    task_ids: list[str]\n",
    "    config: ARCDatasetParams\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        challenges_file: str,\n",
    "        config: ARCDatasetParams,\n",
    "    ):\n",
    "        with open(challenges_file, \"r\") as f:\n",
    "            self.challenges = json.load(f)\n",
    "            self.task_ids = list(self.challenges.keys())\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.task_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        task_id = self.task_ids[idx]\n",
    "        challenge = self.challenges[task_id]\n",
    "\n",
    "        all_grids = []\n",
    "        all_masks = []\n",
    "\n",
    "        for test in challenge[\"test\"]:\n",
    "            grids = torch.zeros(\n",
    "                2 * self.config.max_train_grids + 1,\n",
    "                self.config.max_grid_size,\n",
    "                self.config.max_grid_size,\n",
    "                dtype=torch.int,\n",
    "            )\n",
    "            masks = torch.zeros(\n",
    "                2 * self.config.max_train_grids + 1,\n",
    "                self.config.max_grid_size,\n",
    "                self.config.max_grid_size,\n",
    "                dtype=torch.bool,\n",
    "            )\n",
    "\n",
    "            for i, pair in enumerate(challenge[\"train\"]):\n",
    "                if i >= self.config.max_train_grids:\n",
    "                    print(\n",
    "                        \"Training pairs exceed max\", task_id, i, self.config.max_train_grids\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    input_grid, input_mask = pad_and_mask_grid(pair[\"input\"], self.config)\n",
    "                    output_grid, output_mask = pad_and_mask_grid(\n",
    "                        pair[\"output\"], self.config\n",
    "                    )\n",
    "                    grids[2 * i] = input_grid\n",
    "                    masks[2 * i] = input_mask\n",
    "                    grids[2 * i + 1] = output_grid\n",
    "                    masks[2 * i + 1] = output_mask\n",
    "                except Exception as e:\n",
    "                    print(\"Got exception for training pair\", task_id, i, e)\n",
    "\n",
    "            try:\n",
    "                test_input_grid, test_input_mask = pad_and_mask_grid(\n",
    "                    test[\"input\"], self.config\n",
    "                )\n",
    "                grids[-1] = test_input_grid\n",
    "                masks[-1] = test_input_mask\n",
    "            except Exception as e:\n",
    "                print(\"Got exception on test input\", task_id, e)\n",
    "            \n",
    "            all_grids.append(grids)\n",
    "            all_masks.append(masks)\n",
    "\n",
    "        return {\"task_id\": task_id, \"grids\": torch.stack(all_grids), \"masks\": torch.stack(all_masks)}\n",
    "    \n",
    "\n",
    "def collate_arc_fn(\n",
    "    batch: list[dict],\n",
    ") -> tuple[list, torch.Tensor, torch.Tensor]:\n",
    "    task_ids = [item[\"task_id\"] for item in batch]\n",
    "    grids = torch.stack([item[\"grids\"] for item in batch])\n",
    "    masks = torch.stack([item[\"masks\"] for item in batch])\n",
    "\n",
    "    return (task_ids, grids, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ARCTransformerEncoderDecoderParams:\n",
    "    grid_dim: int\n",
    "    num_train_pairs: int\n",
    "    num_colors: int\n",
    "    num_encoder_layers: int\n",
    "    num_decoder_layers: int\n",
    "    num_heads: int\n",
    "    d_model: int\n",
    "    d_ff: int\n",
    "    dropout: float\n",
    "\n",
    "\n",
    "class EncoderLayerWithAttention(nn.TransformerEncoderLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: Optional[torch.Tensor] = None,\n",
    "        src_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        is_causal: bool = False,\n",
    "        need_weights: bool = False,\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        src_key_padding_mask = F._canonical_mask(\n",
    "            mask=src_key_padding_mask,\n",
    "            mask_name=\"src_key_padding_mask\",\n",
    "            other_type=F._none_or_dtype(src_mask),\n",
    "            other_name=\"src_mask\",\n",
    "            target_type=src.dtype,\n",
    "        )\n",
    "\n",
    "        src_mask = F._canonical_mask(\n",
    "            mask=src_mask,\n",
    "            mask_name=\"src_mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=src.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "        x = src\n",
    "        x1, attn_weights = self.self_attn(\n",
    "            x,\n",
    "            x,\n",
    "            x,\n",
    "            attn_mask=src_mask,\n",
    "            key_padding_mask=src_key_padding_mask,\n",
    "            need_weights=need_weights,\n",
    "            is_causal=is_causal,\n",
    "            average_attn_weights=False,\n",
    "        )\n",
    "\n",
    "        x = x + self.dropout1(x1)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x1 = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        x = x + self.dropout2(x1)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "class EncoderWithAttention(nn.TransformerEncoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_layer: \"EncoderLayerWithAttention\",\n",
    "        num_layers: int,\n",
    "    ) -> None:\n",
    "        super().__init__(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        src_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        need_weights: bool = False,\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        src_key_padding_mask = F._canonical_mask(\n",
    "            mask=src_key_padding_mask,\n",
    "            mask_name=\"src_key_padding_mask\",\n",
    "            other_type=F._none_or_dtype(mask),\n",
    "            other_name=\"mask\",\n",
    "            target_type=src.dtype,\n",
    "        )\n",
    "\n",
    "        mask = F._canonical_mask(\n",
    "            mask=mask,\n",
    "            mask_name=\"mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=src.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "        output = src\n",
    "        attn_weights = []\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output, layer_attn_weights = mod(\n",
    "                output,\n",
    "                src_mask=mask,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                need_weights=need_weights,\n",
    "            )\n",
    "            if need_weights:\n",
    "                attn_weights.append(layer_attn_weights)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output, (torch.stack(attn_weights, dim=1) if need_weights else None)\n",
    "\n",
    "\n",
    "class DecoderLayerWithAttention(nn.TransformerDecoderLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: Optional[torch.Tensor] = None,\n",
    "        memory_mask: Optional[torch.Tensor] = None,\n",
    "        tgt_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        memory_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        tgt_is_causal: bool = False,\n",
    "        memory_is_causal: bool = False,\n",
    "        need_weights: bool = False,\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        x = tgt\n",
    "        x_sa, sa_attn_weights = self._sa_block(\n",
    "            x,\n",
    "            tgt_mask,\n",
    "            tgt_key_padding_mask,\n",
    "            is_causal=tgt_is_causal,\n",
    "            need_weights=need_weights,\n",
    "        )\n",
    "        x = x + x_sa\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x_mha, mha_attn_weights = self._mha_block(\n",
    "            x,\n",
    "            memory,\n",
    "            memory_mask,\n",
    "            memory_key_padding_mask,\n",
    "            is_causal=memory_is_causal,\n",
    "            need_weights=need_weights,\n",
    "        )\n",
    "        x = x + x_mha\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        x = x + self._ff_block(x)\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x, sa_attn_weights, mha_attn_weights\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attn_mask: Optional[torch.Tensor],\n",
    "        key_padding_mask: Optional[torch.Tensor],\n",
    "        is_causal: bool = False,\n",
    "        need_weights: bool = False,\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        x, sa_attn_weights = self.self_attn(\n",
    "            x,\n",
    "            x,\n",
    "            x,\n",
    "            attn_mask=attn_mask,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            is_causal=is_causal,\n",
    "            need_weights=need_weights,\n",
    "            average_attn_weights=False,\n",
    "        )\n",
    "        return self.dropout1(x), sa_attn_weights\n",
    "\n",
    "    # multihead attention block\n",
    "    def _mha_block(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mem: torch.Tensor,\n",
    "        attn_mask: Optional[torch.Tensor],\n",
    "        key_padding_mask: Optional[torch.Tensor],\n",
    "        is_causal: bool = False,\n",
    "        need_weights: bool = False,\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        x, mha_attn_weights = self.multihead_attn(\n",
    "            x,\n",
    "            mem,\n",
    "            mem,\n",
    "            attn_mask=attn_mask,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            is_causal=is_causal,\n",
    "            need_weights=need_weights,\n",
    "            average_attn_weights=False,\n",
    "        )\n",
    "        return self.dropout2(x), mha_attn_weights\n",
    "\n",
    "    # feed forward block\n",
    "    def _ff_block(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout3(x)\n",
    "\n",
    "\n",
    "class DecoderWithAttention(nn.TransformerDecoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        decoder_layer: \"DecoderLayerWithAttention\",\n",
    "        num_layers: int,\n",
    "    ) -> None:\n",
    "        super().__init__(decoder_layer, num_layers)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: Optional[torch.Tensor] = None,\n",
    "        memory_mask: Optional[torch.Tensor] = None,\n",
    "        tgt_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        memory_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        tgt_is_causal: Optional[bool] = False,\n",
    "        memory_is_causal: bool = False,\n",
    "        need_weights: bool = False,\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        output = tgt\n",
    "\n",
    "        sa_attn_weights = []\n",
    "        mha_attn_weights = []\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output, layer_sa_attn_weights, layer_mha_attn_weights = mod(\n",
    "                output,\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask,\n",
    "                memory_mask=memory_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask,\n",
    "                tgt_is_causal=tgt_is_causal,\n",
    "                memory_is_causal=memory_is_causal,\n",
    "                need_weights=need_weights,\n",
    "            )\n",
    "            if need_weights:\n",
    "                sa_attn_weights.append(layer_sa_attn_weights)\n",
    "                mha_attn_weights.append(layer_mha_attn_weights)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return (\n",
    "            output,\n",
    "            torch.stack(sa_attn_weights, dim=1) if need_weights else None,\n",
    "            torch.stack(mha_attn_weights, dim=1) if need_weights else None,\n",
    "        )\n",
    "\n",
    "\n",
    "class ARCTransformerEncoderDecoder(nn.Module):\n",
    "    grid_dim: int\n",
    "    num_train_pairs: int\n",
    "    num_classes: int\n",
    "    num_encoder_layers: int\n",
    "    num_decoder_layers: int\n",
    "    num_heads: int\n",
    "    d_model: int\n",
    "    d_ff: int\n",
    "    dropout: float\n",
    "    seq_len: int\n",
    "\n",
    "    def __init__(self, params: ARCTransformerEncoderDecoderParams):\n",
    "        super().__init__()\n",
    "        self.grid_dim = params.grid_dim\n",
    "        self.num_train_pairs = params.num_train_pairs\n",
    "        self.num_classes = params.num_colors + 1\n",
    "        self.d_model = params.d_model\n",
    "        self.num_encoder_layers = params.num_encoder_layers\n",
    "        self.num_decoder_layers = params.num_decoder_layers\n",
    "        self.num_heads = params.num_heads\n",
    "        self.d_ff = params.d_ff\n",
    "        self.dropout = params.dropout\n",
    "        self.seq_len = (self.num_train_pairs * 2 + 1) * self.grid_dim * self.grid_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_classes, self.d_model)\n",
    "        self.pos_encoding = ARCPositionalEncoding(\n",
    "            d_model=self.d_model,\n",
    "            grid_dim=self.grid_dim,\n",
    "            num_train_pairs=self.num_train_pairs,\n",
    "        )\n",
    "\n",
    "        encoder_layer = EncoderLayerWithAttention(\n",
    "            self.d_model, self.num_heads, self.d_ff, self.dropout\n",
    "        )\n",
    "\n",
    "        self.encoder = EncoderWithAttention(encoder_layer, self.num_encoder_layers)\n",
    "\n",
    "        decoder_layer = DecoderLayerWithAttention(\n",
    "            self.d_model, self.num_heads, self.d_ff, self.dropout\n",
    "        )\n",
    "\n",
    "        self.decoder = DecoderWithAttention(decoder_layer, self.num_decoder_layers)\n",
    "\n",
    "        self.output_query = nn.Parameter(torch.randn(1, self.grid_dim**2, self.d_model))\n",
    "        self.output_layer = nn.Linear(self.d_model, self.num_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, src: torch.Tensor, src_mask: torch.Tensor, need_weights: bool = False\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        Optional[torch.Tensor],\n",
    "        Optional[torch.Tensor],\n",
    "        Optional[torch.Tensor],\n",
    "    ]:\n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        src = self.embedding.forward(src)\n",
    "\n",
    "        pos_emb = self.pos_encoding.forward(src)\n",
    "        src.add_(pos_emb)\n",
    "\n",
    "        src = src.view(batch_size, self.seq_len, self.d_model)\n",
    "\n",
    "        padding_mask = ~src_mask.view(batch_size, -1)\n",
    "\n",
    "        memory, encoder_attn_weights = self.encoder.forward(\n",
    "            src, src_key_padding_mask=padding_mask, need_weights=need_weights\n",
    "        )\n",
    "\n",
    "        output_query = self.output_query.expand(batch_size, -1, -1)\n",
    "\n",
    "        (\n",
    "            output,\n",
    "            decoder_sa_attn_weights,\n",
    "            decoder_mha_attn_weights,\n",
    "        ) = self.decoder.forward(\n",
    "            output_query,\n",
    "            memory,\n",
    "            memory_key_padding_mask=padding_mask,\n",
    "            need_weights=need_weights,\n",
    "        )\n",
    "\n",
    "        output = self.output_layer(output)\n",
    "\n",
    "        output = output.view(batch_size, self.grid_dim, self.grid_dim, self.num_classes)\n",
    "\n",
    "        return (\n",
    "            output,\n",
    "            encoder_attn_weights,\n",
    "            decoder_sa_attn_weights,\n",
    "            decoder_mha_attn_weights,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        need_weights: bool = False,\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        Optional[torch.Tensor],\n",
    "        Optional[torch.Tensor],\n",
    "        Optional[torch.Tensor],\n",
    "    ]:\n",
    "        with torch.no_grad():\n",
    "            (\n",
    "                output,\n",
    "                encoder_attn_weights,\n",
    "                decoder_sa_attn_weights,\n",
    "                decoder_mha_attn_weights,\n",
    "            ) = self.forward(src, src_mask, need_weights)\n",
    "            return (\n",
    "                torch.argmax(output, dim=-1),\n",
    "                encoder_attn_weights,\n",
    "                decoder_sa_attn_weights,\n",
    "                decoder_mha_attn_weights,\n",
    "            )\n",
    "\n",
    "\n",
    "class ARCTransformerEncoder(nn.Module):\n",
    "    grid_dim: int\n",
    "    num_train_pairs: int\n",
    "    num_classes: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    d_model: int\n",
    "    d_ff: int\n",
    "    dropout: float\n",
    "    seq_len: int\n",
    "\n",
    "    def __init__(self, params: ARCTransformerEncoderDecoderParams):\n",
    "        super().__init__()\n",
    "        self.grid_dim = params.grid_dim\n",
    "        self.num_train_pairs = params.num_train_pairs\n",
    "        self.num_classes = params.num_colors + 1\n",
    "        self.d_model = params.d_model\n",
    "        self.num_layers = params.num_encoder_layers\n",
    "        self.num_heads = params.num_heads\n",
    "        self.d_ff = params.d_ff\n",
    "        self.dropout = params.dropout\n",
    "\n",
    "        self.input_seq_len = (\n",
    "            (self.num_train_pairs * 2 + 1) * self.grid_dim * self.grid_dim\n",
    "        )\n",
    "        self.output_seq_len = self.grid_dim * self.grid_dim\n",
    "        self.seq_len = self.input_seq_len + self.output_seq_len\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_classes, self.d_model)\n",
    "        self.pos_encoding = ARCPositionalEncoding(\n",
    "            d_model=self.d_model,\n",
    "            grid_dim=self.grid_dim,\n",
    "            num_train_pairs=self.num_train_pairs,\n",
    "        )\n",
    "\n",
    "        encoder_layer = EncoderLayerWithAttention(\n",
    "            self.d_model, self.num_heads, self.d_ff, self.dropout\n",
    "        )\n",
    "\n",
    "        self.encoder = EncoderWithAttention(encoder_layer, self.num_layers)\n",
    "        # encoder_layer = nn.TransformerEncoderLayer(\n",
    "        #     self.d_model, self.num_heads, self.d_ff, self.dropout, batch_first=True\n",
    "        # )\n",
    "        # self.encoder = nn.TransformerEncoder(encoder_layer, self.num_layers)\n",
    "        self.output_query = nn.Parameter(\n",
    "            torch.randn(1, 1, self.grid_dim, self.grid_dim, self.d_model)\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.d_model, self.num_classes)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        tgt: Optional[torch.Tensor] = None,\n",
    "        temperature: float = 0.0,\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        Optional[torch.Tensor],\n",
    "        Optional[torch.Tensor],\n",
    "        Optional[torch.Tensor],\n",
    "    ]:\n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        embedded_src = self.embedding.forward(src)\n",
    "\n",
    "        if tgt is not None:\n",
    "            output_query = self.embedding.forward(tgt).view(\n",
    "                batch_size, 1, self.grid_dim, self.grid_dim, self.d_model\n",
    "            )\n",
    "        else:\n",
    "            output_query = self.output_query.expand(batch_size, -1, -1, -1, -1)\n",
    "\n",
    "        combined_input = torch.cat([embedded_src, output_query], dim=1)\n",
    "\n",
    "        # Add positional encodings\n",
    "        pos_emb = self.pos_encoding(combined_input)\n",
    "        embedded = combined_input + pos_emb\n",
    "\n",
    "        embedded = embedded.view(batch_size, self.seq_len, self.d_model)\n",
    "\n",
    "        causal_mask = torch.zeros(self.seq_len, self.seq_len, device=src.device)\n",
    "        causal_mask[: self.input_seq_len, self.input_seq_len :] = 1\n",
    "        causal_mask = causal_mask.bool()\n",
    "\n",
    "        # Create padding mask\n",
    "        padding_mask = ~src_mask.view(batch_size, -1)\n",
    "\n",
    "        padding_mask = torch.cat(\n",
    "            [\n",
    "                padding_mask,\n",
    "                torch.zeros(\n",
    "                    (batch_size, self.grid_dim**2), dtype=torch.bool, device=src.device\n",
    "                ),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        output = self.encoder.forward(\n",
    "            embedded, mask=causal_mask, src_key_padding_mask=padding_mask\n",
    "        )[0]\n",
    "\n",
    "        # Get only the output grid portion\n",
    "        output_grid_portion = output[:, -self.output_seq_len :, :]\n",
    "\n",
    "        # Project to vocabulary space\n",
    "        logits = self.output_layer(output_grid_portion)\n",
    "\n",
    "        # Reshape to grid format\n",
    "        output = logits.view(batch_size, self.grid_dim, self.grid_dim, self.num_classes)\n",
    "\n",
    "        if temperature > 0:\n",
    "            output = output / temperature\n",
    "\n",
    "        return (output, None, None, None)\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        tgt: Optional[torch.Tensor] = None,\n",
    "        temperature: float = 0.0,\n",
    "        need_weights: bool = False,\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        Optional[torch.Tensor],\n",
    "        Optional[torch.Tensor],\n",
    "        Optional[torch.Tensor],\n",
    "    ]:\n",
    "        with torch.no_grad():\n",
    "            (\n",
    "                output,\n",
    "                encoder_attn_weights,\n",
    "                decoder_sa_attn_weights,\n",
    "                decoder_mha_attn_weights,\n",
    "            ) = self.forward(src, src_mask, tgt=tgt, temperature=temperature)\n",
    "\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(output, dim=-1)\n",
    "            prediction = torch.multinomial(\n",
    "                probs.view(-1, probs.size(-1)),\n",
    "                num_samples=1,\n",
    "                replacement=True,\n",
    "            ).view(-1, *probs.size()[:-1])\n",
    "        else:\n",
    "            prediction = torch.argmax(output, dim=-1)\n",
    "\n",
    "        return (\n",
    "            prediction,\n",
    "            encoder_attn_weights,\n",
    "            decoder_sa_attn_weights,\n",
    "            decoder_mha_attn_weights,\n",
    "        )\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        patch_size: int,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Convolutional layer for patch embedding\n",
    "        self.conv_embed = nn.Conv2d(\n",
    "            in_channels=self.num_classes,\n",
    "            out_channels=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = (\n",
    "            F.one_hot(x.long(), num_classes=self.num_classes)\n",
    "            .permute(0, 3, 1, 2)\n",
    "            .float()\n",
    "        )\n",
    "\n",
    "        x = self.conv_embed(x)\n",
    "\n",
    "        x = x.permute(0, 2, 3, 1).reshape(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        mask = nn.functional.avg_pool2d(\n",
    "            mask.float(),\n",
    "            self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "        )\n",
    "\n",
    "        mask = (mask > 0).reshape(batch_size, -1)\n",
    "\n",
    "        return (x, mask)\n",
    "\n",
    "\n",
    "class ARCVisionEncoderDecoder(nn.Module):\n",
    "    def __init__(self, params: ARCTransformerEncoderDecoderParams):\n",
    "        super().__init__()\n",
    "        self.grid_dim = params.grid_dim\n",
    "        self.num_train_pairs = params.num_train_pairs\n",
    "        self.num_classes = params.num_colors + 1\n",
    "        self.d_model = params.d_model\n",
    "        self.num_encoder_layers = params.num_encoder_layers\n",
    "        self.num_decoder_layers = params.num_decoder_layers\n",
    "        self.num_heads = params.num_heads\n",
    "        self.d_ff = params.d_ff\n",
    "        self.dropout = params.dropout\n",
    "        self.patch_size = 2\n",
    "\n",
    "        num_grids = self.num_train_pairs * 2 + 1\n",
    "        self.patch_grid_dim = self.grid_dim // self.patch_size\n",
    "        self.seq_len = num_grids * self.patch_grid_dim * self.patch_grid_dim\n",
    "\n",
    "        self.embedding = PatchEmbedding(\n",
    "            num_classes=self.num_classes,\n",
    "            patch_size=self.patch_size,\n",
    "            embed_dim=self.d_model,\n",
    "        )\n",
    "        self.pos_encoding = ARCPositionalEncoding(\n",
    "            d_model=self.d_model,\n",
    "            grid_dim=self.patch_grid_dim,\n",
    "            num_train_pairs=self.num_train_pairs,\n",
    "        )\n",
    "\n",
    "        encoder_layer = EncoderLayerWithAttention(\n",
    "            self.d_model, self.num_heads, self.d_ff, self.dropout\n",
    "        )\n",
    "        self.encoder = EncoderWithAttention(encoder_layer, self.num_encoder_layers)\n",
    "\n",
    "        decoder_layer = DecoderLayerWithAttention(\n",
    "            self.d_model, self.num_heads, self.d_ff, self.dropout\n",
    "        )\n",
    "        self.decoder = DecoderWithAttention(decoder_layer, self.num_decoder_layers)\n",
    "\n",
    "        self.output_query = nn.Parameter(\n",
    "            torch.randn(1, self.grid_dim * self.grid_dim, self.d_model)\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.d_model, self.num_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, src: torch.Tensor, src_mask: torch.Tensor, need_weights: bool = False\n",
    "    ):\n",
    "        batch_size, num_grids, grid_dim, grid_dim = src.shape\n",
    "\n",
    "        # Flatten grids\n",
    "        src = src.reshape(batch_size, num_grids * grid_dim, grid_dim)\n",
    "        src_mask = src_mask.reshape(batch_size, num_grids * grid_dim, grid_dim)\n",
    "\n",
    "        # Apply patch embedding\n",
    "        src_patches, mask_patches = self.embedding.forward(src, src_mask)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        pos_emb_patches = self.pos_encoding.forward(\n",
    "            src_patches.reshape(\n",
    "                batch_size,\n",
    "                num_grids,\n",
    "                self.patch_grid_dim,\n",
    "                self.patch_grid_dim,\n",
    "                self.d_model,\n",
    "            )\n",
    "        )\n",
    "        pos_emb_patches = pos_emb_patches.reshape(-1, self.d_model)\n",
    "\n",
    "        src_patches = src_patches.reshape(batch_size, -1, self.d_model)\n",
    "        src_patches = src_patches + pos_emb_patches\n",
    "\n",
    "        # Invert padding mask\n",
    "        padding_mask = ~mask_patches\n",
    "\n",
    "        # Encode input\n",
    "        memory, encoder_attn_weights = self.encoder.forward(\n",
    "            src_patches, src_key_padding_mask=padding_mask, need_weights=need_weights\n",
    "        )\n",
    "\n",
    "        # Prepare output query\n",
    "        output_query = self.output_query.expand(batch_size, -1, -1)\n",
    "\n",
    "        # Decode\n",
    "        (\n",
    "            output,\n",
    "            decoder_sa_attn_weights,\n",
    "            decoder_mha_attn_weights,\n",
    "        ) = self.decoder.forward(\n",
    "            output_query,\n",
    "            memory,\n",
    "            memory_key_padding_mask=padding_mask,\n",
    "            need_weights=need_weights,\n",
    "        )\n",
    "\n",
    "        # Generate output patches\n",
    "        output = self.output_layer.forward(output)\n",
    "\n",
    "        # Reshape to grid\n",
    "        output = output.view(batch_size, self.grid_dim, self.grid_dim, self.num_classes)\n",
    "\n",
    "        return (\n",
    "            output,\n",
    "            encoder_attn_weights,\n",
    "            decoder_sa_attn_weights,\n",
    "            decoder_mha_attn_weights,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self, src: torch.Tensor, src_mask: torch.Tensor, need_weights: bool = False\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            (\n",
    "                output,\n",
    "                encoder_attn_weights,\n",
    "                decoder_sa_attn_weights,\n",
    "                decoder_mha_attn_weights,\n",
    "            ) = self.forward(src, src_mask, need_weights)\n",
    "            return (\n",
    "                torch.argmax(output, dim=-1),\n",
    "                encoder_attn_weights,\n",
    "                decoder_sa_attn_weights,\n",
    "                decoder_mha_attn_weights,\n",
    "            )\n",
    "\n",
    "\n",
    "class ARCVisionEncoder(nn.Module):\n",
    "    def __init__(self, params: ARCTransformerEncoderDecoderParams):\n",
    "        super().__init__()\n",
    "        self.grid_dim = params.grid_dim\n",
    "        self.num_train_pairs = params.num_train_pairs\n",
    "        self.num_classes = params.num_colors + 1\n",
    "        self.d_model = params.d_model\n",
    "        self.num_layers = params.num_encoder_layers\n",
    "        self.num_heads = params.num_heads\n",
    "        self.d_ff = params.d_ff\n",
    "        self.dropout = params.dropout\n",
    "        self.patch_size = 2\n",
    "\n",
    "        self.patch_grid_dim = self.grid_dim // self.patch_size\n",
    "\n",
    "        self.input_seq_len = (\n",
    "            (self.num_train_pairs * 2 + 1) * self.patch_grid_dim * self.patch_grid_dim\n",
    "        )\n",
    "        self.output_seq_len = self.grid_dim * self.grid_dim\n",
    "        self.seq_len = self.input_seq_len + self.output_seq_len\n",
    "\n",
    "        self.embedding = PatchEmbedding(\n",
    "            num_classes=self.num_classes,\n",
    "            patch_size=self.patch_size,\n",
    "            embed_dim=self.d_model,\n",
    "        )\n",
    "        self.tgt_embedding = nn.Embedding(self.num_classes, self.d_model)\n",
    "        self.pos_encoding = ARCPositionalEncodingV2(\n",
    "            d_model=self.d_model,\n",
    "            grid_dim=self.grid_dim,\n",
    "            num_train_pairs=self.num_train_pairs,\n",
    "        )\n",
    "\n",
    "        encoder_layer = EncoderLayerWithAttention(\n",
    "            self.d_model, self.num_heads, self.d_ff, self.dropout\n",
    "        )\n",
    "        self.encoder = EncoderWithAttention(encoder_layer, self.num_layers)\n",
    "\n",
    "        self.output_query = nn.Parameter(\n",
    "            torch.randn(1, 1, self.grid_dim, self.grid_dim, self.d_model)\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.d_model, self.num_classes)\n",
    "\n",
    "    def embed(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        tgt: Optional[torch.Tensor] = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, num_grids, grid_dim, grid_dim = src.shape\n",
    "\n",
    "        pos_emb = self.pos_encoding.forward(\n",
    "            num_grids=num_grids + 1, grid_dim=grid_dim, device=src.device\n",
    "        )\n",
    "\n",
    "        src = src.reshape(batch_size, num_grids * grid_dim, grid_dim)\n",
    "        src_mask = src_mask.reshape(batch_size, num_grids * grid_dim, grid_dim)\n",
    "\n",
    "        src_patched, mask_patched = self.embedding.forward(src, src_mask)\n",
    "\n",
    "        input_pos_emb = pos_emb[:-1, :, :, :]\n",
    "        input_pos_emb_patched = (\n",
    "            input_pos_emb.unfold(1, self.patch_size, self.patch_size)\n",
    "            .unfold(2, self.patch_size, self.patch_size)\n",
    "            .mean(dim=(-2, -1))\n",
    "        )\n",
    "\n",
    "        src_patched = src_patched.reshape(batch_size, -1, self.d_model)\n",
    "        input_pos_emb_patched = input_pos_emb_patched.reshape(-1, self.d_model)\n",
    "        input_seq = src_patched + input_pos_emb_patched\n",
    "\n",
    "        if tgt is not None:\n",
    "            output_query = self.tgt_embedding.forward(tgt).view(\n",
    "                batch_size, 1, self.grid_dim, self.grid_dim, self.d_model\n",
    "            )\n",
    "        else:\n",
    "            output_query = self.output_query.expand(batch_size, -1, -1, -1, -1)\n",
    "        output_pos_emb = pos_emb[-1:, :, :, :]\n",
    "\n",
    "        output_query = output_query.reshape(batch_size, -1, self.d_model)\n",
    "        output_pos_emb = output_pos_emb.reshape(-1, self.d_model)\n",
    "        output_seq = output_query + output_pos_emb\n",
    "\n",
    "        combined_seq = torch.cat([input_seq, output_seq], dim=1)\n",
    "\n",
    "        # Make padding mask\n",
    "        padding_mask = ~mask_patched\n",
    "        padding_mask = torch.cat(\n",
    "            [\n",
    "                padding_mask,\n",
    "                torch.zeros(\n",
    "                    (batch_size, self.grid_dim**2), dtype=torch.bool, device=src.device\n",
    "                ),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return combined_seq, padding_mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        tgt: Optional[torch.Tensor] = None,\n",
    "        temperature: float = 0.0,\n",
    "        need_weights: bool = False,\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        combined_seq, padding_mask = self.embed(src, src_mask, tgt)\n",
    "\n",
    "        # Make causal mask\n",
    "        causal_mask = torch.zeros(self.seq_len, self.seq_len, device=src.device)\n",
    "        causal_mask[: self.input_seq_len, self.input_seq_len :] = 1\n",
    "        causal_mask = causal_mask.bool()\n",
    "\n",
    "        output, attn_weights = self.encoder.forward(\n",
    "            combined_seq,\n",
    "            mask=causal_mask,\n",
    "            src_key_padding_mask=padding_mask,\n",
    "            need_weights=need_weights,\n",
    "        )\n",
    "\n",
    "        output_grid_portion = output[:, -self.output_seq_len :, :]\n",
    "\n",
    "        logits = self.output_layer.forward(output_grid_portion)\n",
    "\n",
    "        output = logits.view(batch_size, self.grid_dim, self.grid_dim, self.num_classes)\n",
    "\n",
    "        if temperature > 0:\n",
    "            output = output / temperature\n",
    "\n",
    "        return (output, attn_weights)\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        tgt: Optional[torch.Tensor] = None,\n",
    "        temperature: float = 0.0,\n",
    "        need_weights: bool = False,\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            (\n",
    "                output,\n",
    "                encoder_attn_weights,\n",
    "            ) = self.forward(src, src_mask, tgt=tgt, temperature=temperature)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(output, dim=-1)\n",
    "            prediction = torch.multinomial(\n",
    "                probs.view(-1, probs.size(-1)),\n",
    "                num_samples=1,\n",
    "                replacement=True,\n",
    "            ).view(-1, *probs.size()[:-1])\n",
    "        else:\n",
    "            prediction = torch.argmax(output, dim=-1)\n",
    "        return prediction, encoder_attn_weights\n",
    "\n",
    "\n",
    "class ARCPositionalEncodingV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        grid_dim: int,\n",
    "        num_train_pairs: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.grid_dim = grid_dim\n",
    "        self.num_train_pairs = num_train_pairs\n",
    "\n",
    "        # Embeddings for row and column positions\n",
    "        self.row_embedding = nn.Embedding(self.grid_dim, self.d_model // 4)\n",
    "        self.col_embedding = nn.Embedding(self.grid_dim, self.d_model // 4)\n",
    "\n",
    "        # Embedding for input vs output\n",
    "        self.input_output_embedding = nn.Embedding(2, d_model // 4)\n",
    "\n",
    "        # Embedding for training pair index\n",
    "        self.pair_embedding = nn.Embedding(\n",
    "            self.num_train_pairs + 1, d_model // 4\n",
    "        )  # +1 for test pair\n",
    "\n",
    "    @torch.compiler.disable\n",
    "    def forward(\n",
    "        self, num_grids: int, grid_dim: int, device: torch.device\n",
    "    ) -> torch.Tensor:\n",
    "        grid_pos = torch.arange(grid_dim, device=device)\n",
    "\n",
    "        # Row pos embedding\n",
    "        row_emb = (\n",
    "            self.row_embedding.forward(grid_pos)\n",
    "            .unsqueeze(1)\n",
    "            .expand(num_grids, -1, grid_dim, -1)\n",
    "        )\n",
    "\n",
    "        # Column pos embedding\n",
    "        col_emb = (\n",
    "            self.col_embedding.forward(grid_pos)\n",
    "            .unsqueeze(0)\n",
    "            .expand(num_grids, grid_dim, -1, -1)\n",
    "        )\n",
    "\n",
    "        # Input/output embedding\n",
    "        grid_indices = torch.arange(num_grids, device=device)\n",
    "        is_output = (grid_indices % 2 == 1).long()\n",
    "        io_emb = (\n",
    "            self.input_output_embedding(is_output)\n",
    "            .unsqueeze(1)\n",
    "            .unsqueeze(1)\n",
    "            .expand(num_grids, grid_dim, grid_dim, -1)\n",
    "        )\n",
    "\n",
    "        # Pair embedding\n",
    "        pair_indices = torch.div(grid_indices, 2, rounding_mode=\"floor\")\n",
    "        pair_emb = (\n",
    "            self.pair_embedding(pair_indices)\n",
    "            .unsqueeze(1)\n",
    "            .unsqueeze(1)\n",
    "            .expand(num_grids, grid_dim, grid_dim, -1)\n",
    "        )\n",
    "\n",
    "        # Combine all embeddings (1, num_grids, height, width, d_model)\n",
    "        combined_emb = torch.cat([row_emb, col_emb, io_emb, pair_emb], dim=-1)\n",
    "\n",
    "        return combined_emb\n",
    "\n",
    "\n",
    "class ARCPositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        grid_dim: int,\n",
    "        num_train_pairs: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.grid_dim = grid_dim\n",
    "        self.num_train_pairs = num_train_pairs\n",
    "\n",
    "        # Embeddings for row and column positions\n",
    "        self.row_embedding = nn.Embedding(self.grid_dim, self.d_model // 4)\n",
    "        self.col_embedding = nn.Embedding(self.grid_dim, self.d_model // 4)\n",
    "\n",
    "        # Embedding for input vs output\n",
    "        self.input_output_embedding = nn.Embedding(2, d_model // 4)\n",
    "\n",
    "        # Embedding for training pair index\n",
    "        self.pair_embedding = nn.Embedding(\n",
    "            self.num_train_pairs + 1, d_model // 4\n",
    "        )  # +1 for test pair\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        _, num_grids, height, width, _ = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        # Row pos embedding\n",
    "        row_pos = torch.arange(height, device=device)\n",
    "        row_emb = (\n",
    "            self.row_embedding.forward(row_pos)\n",
    "            .unsqueeze(1)\n",
    "            .expand(num_grids, -1, width, -1)\n",
    "        )\n",
    "\n",
    "        # Column pos embedding\n",
    "        col_pos = torch.arange(width, device=device)\n",
    "        col_emb = (\n",
    "            self.col_embedding.forward(col_pos)\n",
    "            .unsqueeze(0)\n",
    "            .expand(num_grids, height, -1, -1)\n",
    "        )\n",
    "\n",
    "        # Input/output embedding\n",
    "        grid_indices = torch.arange(num_grids, device=device)\n",
    "        is_output = (grid_indices % 2 == 1).long()\n",
    "        io_emb = (\n",
    "            self.input_output_embedding(is_output)\n",
    "            .unsqueeze(1)\n",
    "            .unsqueeze(1)\n",
    "            .expand(num_grids, height, width, -1)\n",
    "        )\n",
    "\n",
    "        # Pair embedding\n",
    "        pair_indices = torch.div(grid_indices, 2, rounding_mode=\"floor\")\n",
    "        pair_indices[-1] = self.num_train_pairs\n",
    "        pair_emb = (\n",
    "            self.pair_embedding(pair_indices)\n",
    "            .unsqueeze(1)\n",
    "            .unsqueeze(1)\n",
    "            .expand(num_grids, height, width, -1)\n",
    "        )\n",
    "\n",
    "        # Combine all embeddings (1, num_grids, height, width, d_model)\n",
    "        combined_emb = torch.cat([row_emb, col_emb, io_emb, pair_emb], dim=-1)\n",
    "\n",
    "        return combined_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpad_grid(grid: torch.Tensor) -> list[list[int]]:\n",
    "    grid = grid - 1\n",
    "    filtered_rows: list[list] = []\n",
    "    for row in grid:\n",
    "        filtered_row = row[row != -1]\n",
    "        if len(filtered_row) > 0:\n",
    "            filtered_rows.append(filtered_row.tolist())\n",
    "    # Hack to ensure there's always at least one value\n",
    "    if len(filtered_rows) == 0:\n",
    "        filtered_rows.append([0])\n",
    "    max_length = max(len(row) for row in filtered_rows)\n",
    "    padded_rows = [(row + [0] * (max_length - len(row))) for row in filtered_rows]\n",
    "    return padded_rows\n",
    "\n",
    "class FinetuneDataset(Dataset):\n",
    "    tasks: list[list[list[list[list[int]]]]]\n",
    "    config: ARCDatasetParams\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tasks: list[list[list[list[list[int]]]]],\n",
    "        config: ARCDatasetParams,\n",
    "    ):\n",
    "        self.tasks = tasks\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        task = self.tasks[idx]\n",
    "\n",
    "        grids = torch.zeros(\n",
    "            2 * self.config.max_train_grids + 1,\n",
    "            self.config.max_grid_size,\n",
    "            self.config.max_grid_size,\n",
    "            dtype=torch.int,\n",
    "        )\n",
    "        masks = torch.zeros(\n",
    "            2 * self.config.max_train_grids + 1,\n",
    "            self.config.max_grid_size,\n",
    "            self.config.max_grid_size,\n",
    "            dtype=torch.bool,\n",
    "        )\n",
    "\n",
    "        for i, pair in enumerate(task[:-1]):\n",
    "            if i >= self.config.max_train_grids:\n",
    "                print(\"Training pairs exceed max\", i, self.config.max_train_grids)\n",
    "                break\n",
    "\n",
    "            input_grid, input_mask = pad_and_mask_grid(pair[0], self.config)\n",
    "            output_grid, output_mask = pad_and_mask_grid(pair[1], self.config)\n",
    "            grids[2 * i] = input_grid\n",
    "            masks[2 * i] = input_mask\n",
    "            grids[2 * i + 1] = output_grid\n",
    "            masks[2 * i + 1] = output_mask\n",
    "\n",
    "        test_input_grid, test_input_mask = pad_and_mask_grid(task[-1][0], self.config)\n",
    "        grids[-1] = test_input_grid\n",
    "        masks[-1] = test_input_mask\n",
    "\n",
    "        test_output_grid = pad_and_mask_grid(task[-1][1], self.config)[0]\n",
    "\n",
    "        return {\n",
    "            \"grids\": grids,\n",
    "            \"masks\": masks,\n",
    "            \"output\": test_output_grid,\n",
    "        }\n",
    "\n",
    "\n",
    "def make_finetune_dataset(\n",
    "    grids: torch.Tensor, config: ARCDatasetParams\n",
    ") -> FinetuneDataset:\n",
    "    if len(grids.shape) == 3:\n",
    "        grids = grids.unsqueeze(0)\n",
    "    if len(grids.shape) != 4:\n",
    "        raise Exception(\"incorrect grids dimension\")\n",
    "    tasks = []\n",
    "    for task in grids:\n",
    "        pairs = task[:-1].reshape(\n",
    "            config.max_train_grids,\n",
    "            2,\n",
    "            config.max_grid_size,\n",
    "            config.max_grid_size,\n",
    "        )\n",
    "        finetune_pairs: list[list[list[list[int]]]] = []\n",
    "        for pair in pairs:\n",
    "            finetune_pairs.append([unpad_grid(grid) for grid in pair])\n",
    "\n",
    "        for length in range(3, len(finetune_pairs) + 1):\n",
    "            for combination in itertools.combinations(finetune_pairs, length):\n",
    "                for permutation in itertools.permutations(combination):\n",
    "                    tasks.append(list(permutation))\n",
    "\n",
    "    return FinetuneDataset(tasks=tasks, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ARCTrainParams:\n",
    "    batch_size: int\n",
    "    learning_rate: float\n",
    "    weight_decay: float\n",
    "    dataset_dir: list[str]\n",
    "    loss_class_weights: Optional[dict[int, float]] = None\n",
    "    meta_batch_size: Optional[int] = None\n",
    "    meta_learning_rate: Optional[float] = None\n",
    "    meta_weight_decay: Optional[float] = None\n",
    "    meta_num_epochs: Optional[int] = None\n",
    "    train_steps_per_epoch: Optional[int] = None\n",
    "    eval_steps_per_epoch: Optional[int] = None\n",
    "    warmup_epochs: Optional[int] = None\n",
    "    refinement_ratio: Optional[float] = None\n",
    "\n",
    "\n",
    "def finetune_collate_arc_fn(\n",
    "    batch: list[dict],\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    grids = torch.stack([item[\"grids\"] for item in batch])\n",
    "    masks = torch.stack([item[\"masks\"] for item in batch])\n",
    "    output = torch.stack([item[\"output\"] for item in batch])\n",
    "\n",
    "    return (grids, masks, output)\n",
    "\n",
    "def fine_tune_transformer(\n",
    "    model: nn.Module,\n",
    "    finetune_params: ARCTrainParams,\n",
    "    dataset: FinetuneDataset,\n",
    "    num_epochs: int,\n",
    "    accuracy_cutoff: float = 0.99,\n",
    ") -> nn.Module:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    torch.backends.mha.set_fastpath_enabled(False)\n",
    "\n",
    "    model = copy.deepcopy(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=finetune_params.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=finetune_collate_arc_fn,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    print(f\"Starting fine-tuning run with dataset of {len(dataset)} training items\")\n",
    "    print(f\"Using batch size of {finetune_params.batch_size}\")\n",
    "\n",
    "    class_weights = torch.ones(model.num_classes).to(device)\n",
    "    if finetune_params.loss_class_weights is not None:\n",
    "        for cls, weight in finetune_params.loss_class_weights.items():\n",
    "            class_weights[cls] = weight\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=finetune_params.learning_rate,\n",
    "        weight_decay=finetune_params.weight_decay,\n",
    "    )\n",
    "\n",
    "    scaler = GradScaler(device.type)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        train_accuracy = 0.0\n",
    "\n",
    "        for batch in data_loader:\n",
    "            grids, masks, target_grid = [item.to(device) for item in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device.type):\n",
    "                output = model.forward(grids, masks)[0]\n",
    "                loss = criterion(\n",
    "                    output.view(-1, model.num_classes),\n",
    "                    target_grid.view(-1).long(),\n",
    "                )\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(output, dim=-1)\n",
    "            train_accuracy += (predictions == target_grid).float().mean().item()\n",
    "\n",
    "        train_loss /= len(data_loader)\n",
    "        train_accuracy /= len(data_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "        if train_accuracy >= accuracy_cutoff:\n",
    "            print(\"Stopping early because accuracy exceeds accuracy cut-off\")\n",
    "            break\n",
    "\n",
    "    print(\"Fine-tuning completed\")\n",
    "    return model\n",
    "\n",
    "def finetune_and_predict(\n",
    "    model: ARCVisionEncoder | ARCTransformerEncoder,\n",
    "    finetune_params: ARCTrainParams,\n",
    "    dataset_params: ARCDatasetParams,\n",
    "    grids: torch.Tensor,\n",
    "    masks: torch.Tensor,\n",
    "    num_finetune_epochs: int = 2,\n",
    "    temperature: list[float] = [0.0],\n",
    "    accuracy_cutoff: float = 0.99,\n",
    "    num_predictions: int = 1,\n",
    ") -> list[torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    finetune_dataset = make_finetune_dataset(grids, dataset_params)\n",
    "\n",
    "    finetune_model = fine_tune_transformer(\n",
    "        model,\n",
    "        finetune_params,\n",
    "        finetune_dataset,\n",
    "        num_finetune_epochs,\n",
    "        accuracy_cutoff,\n",
    "    )\n",
    "\n",
    "    finetune_predictions = finetune_model.generate(\n",
    "        grids,\n",
    "        masks,\n",
    "        temperature=temperature[0],\n",
    "        need_weights=False\n",
    "    )[0][0]\n",
    "\n",
    "    refine_temperature = temperature[1] if len(temperature) > 1 else temperature[0]\n",
    "\n",
    "    refined_predictions = []\n",
    "    for _ in range(num_predictions):\n",
    "        refined_prediction = finetune_model.generate(\n",
    "            grids,\n",
    "            masks,\n",
    "            temperature=refine_temperature,\n",
    "            tgt=finetune_predictions,\n",
    "            need_weights=False\n",
    "        )[0][0]\n",
    "        refined_predictions.append(refined_prediction)\n",
    "    \n",
    "\n",
    "    return refined_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_params = ARCDatasetParams(max_grid_size=12, max_train_grids=4, color_offset=1)\n",
    "test_dataset = ARCKaggleDataset(challenges_file=test_challenges_file_name, config=data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_params = ARCTransformerEncoderDecoderParams(\n",
    "    grid_dim=12,\n",
    "    num_train_pairs=4,\n",
    "    num_colors=10,\n",
    "    num_encoder_layers=16,\n",
    "    num_decoder_layers=0,\n",
    "    num_heads=16,\n",
    "    d_model=512,\n",
    "    d_ff=3072,\n",
    "    dropout=0.2,\n",
    ")\n",
    "model = ARCTransformerEncoder(model_params).to(device)\n",
    "model.load_state_dict(torch.load(kaggle_model_file_path, map_location=device, weights_only=True))\n",
    "\n",
    "finetune_params = ARCTrainParams(\n",
    "    batch_size=4,\n",
    "    learning_rate=1e-5,\n",
    "    loss_class_weights={0: 0.2},\n",
    "    dataset_dir=[],\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "num_finetune_epochs = 12\n",
    "accuracy_cutoff = 0.99\n",
    "temperature = [0.4, 0.2]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def unpad_output(output: torch.Tensor) -> torch.Tensor:\n",
    "    output = output - 1\n",
    "    filtered_rows = []\n",
    "    for row in output:\n",
    "        filtered_row = row[row != -1]\n",
    "        if len(filtered_row) > 0:\n",
    "            filtered_rows.append(filtered_row)\n",
    "    # Hack to ensure there's always at least one value\n",
    "    if len(filtered_rows) == 0:\n",
    "        filtered_rows.append(torch.zeros(1, device=device, dtype=torch.long))\n",
    "    max_length = max(len(row) for row in filtered_rows)\n",
    "    padded_rows = [torch.cat([row, torch.zeros(max_length - len(row), dtype=row.dtype, device=device)]) for row in filtered_rows]\n",
    "    return torch.stack(padded_rows)\n",
    "\n",
    "kaggle_output = {}\n",
    "\n",
    "\n",
    "for task in test_dataset:\n",
    "    print(f\"Starting {task['task_id']}\")\n",
    "    task_predictions = []\n",
    "    for grids, masks in zip(task[\"grids\"], task[\"masks\"]):\n",
    "        predictions = finetune_and_predict(\n",
    "            model,\n",
    "            finetune_params,\n",
    "            data_params,\n",
    "            grids.unsqueeze(0).to(device),\n",
    "            masks.unsqueeze(0).to(device),\n",
    "            num_finetune_epochs=num_finetune_epochs,\n",
    "            temperature=temperature,\n",
    "            accuracy_cutoff=accuracy_cutoff,\n",
    "            num_predictions=2\n",
    "        )\n",
    "        attempt_1 = unpad_output(predictions[0][0]).cpu().numpy().tolist()\n",
    "        attempt_2 = unpad_output(predictions[1][0]).cpu().numpy().tolist()\n",
    "        task_predictions.append({\n",
    "            \"attempt_1\":  attempt_1,\n",
    "            \"attempt_2\": attempt_2\n",
    "        })\n",
    "    kaggle_output[task[\"task_id\"]] = task_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(kaggle_submission_file_path, \"w\") as f:\n",
    "  json.dump(kaggle_output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grade_submission(submission_filename: str, solutions_filename: str):\n",
    "  with open(solutions_filename, \"r\") as f:\n",
    "    solutions = json.load(f)\n",
    "  with open(submission_filename, \"r\") as f:\n",
    "    submissions = json.load(f)\n",
    "\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  attempted = 0\n",
    "\n",
    "  print(submissions[\"22168020\"])\n",
    "  for k, submission in submissions.items():\n",
    "  # for k, item in solutions.items():\n",
    "      # submission = submissions[k]\n",
    "      \n",
    "      item = solutions[k]\n",
    "      if len(item[0]) <=12 and len(item[0][0]) <= 12:\n",
    "        attempted += 1\n",
    "      # if len(submission[0][\"attempt_1\"]) > 1:\n",
    "      #   attempted += 1\n",
    "      if np.array_equal(item[0], submission[0][\"attempt_1\"]) is True:\n",
    "        correct += 1\n",
    "      total += 1\n",
    "\n",
    "  print(correct, attempted, total, correct / attempted, correct / total)\n",
    "\n",
    "# test_solutions_file_name = \"data/arc-agi_evaluation_solutions.json\"\n",
    "test_solutions_file_name = \"data/arc-agi_training_solutions.json\"\n",
    "kaggle_submission_file_path = \"kaggle/submission_1.json\"\n",
    "grade_submission(kaggle_submission_file_path, test_solutions_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
