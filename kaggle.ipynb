{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_challenges_file_name = \"data/arc-agi_evaluation_challenges.json\"\n",
    "kaggle_model_file_path = \"kaggle/models/subtly_known_panda.pth\"\n",
    "kaggle_submission_file_path = \"kaggle/submission.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ARCDatasetParams:\n",
    "    max_grid_size: int = 30\n",
    "    max_train_grids: int = 10\n",
    "    color_offset: int = 1\n",
    "\n",
    "\n",
    "def pad_and_mask_grid(\n",
    "    grid: list[list[int]], config: ARCDatasetParams\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    h, w = len(grid), len(grid[0])\n",
    "    if h > config.max_grid_size or w > config.max_grid_size:\n",
    "        raise Exception(\"grid size too large\")\n",
    "\n",
    "    padded = torch.zeros((config.max_grid_size, config.max_grid_size), dtype=torch.int)\n",
    "    mask = torch.zeros((config.max_grid_size, config.max_grid_size), dtype=torch.bool)\n",
    "\n",
    "    # Calculate padding\n",
    "    pad_h = (config.max_grid_size - h) // 2\n",
    "    pad_w = (config.max_grid_size - w) // 2\n",
    "\n",
    "    # Place the grid in the center\n",
    "    padded[pad_h : pad_h + h, pad_w : pad_w + w] = (\n",
    "        torch.tensor(grid, dtype=torch.int) + config.color_offset\n",
    "    )\n",
    "    mask[pad_h : pad_h + h, pad_w : pad_w + w] = True\n",
    "\n",
    "    return (padded, mask)\n",
    "\n",
    "\n",
    "class ARCKaggleDataset(Dataset):\n",
    "    challenges: dict[str, dict]\n",
    "    task_ids: list[str]\n",
    "    config: ARCDatasetParams\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        challenges_file: str,\n",
    "        config: ARCDatasetParams,\n",
    "    ):\n",
    "        with open(challenges_file, \"r\") as f:\n",
    "            self.challenges = json.load(f)\n",
    "            self.task_ids = list(self.challenges.keys())\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.task_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        task_id = self.task_ids[idx]\n",
    "        challenge = self.challenges[task_id]\n",
    "\n",
    "        all_grids = []\n",
    "        all_masks = []\n",
    "\n",
    "        for test in challenge[\"test\"]:\n",
    "            grids = torch.zeros(\n",
    "                2 * self.config.max_train_grids + 1,\n",
    "                self.config.max_grid_size,\n",
    "                self.config.max_grid_size,\n",
    "                dtype=torch.int,\n",
    "            )\n",
    "            masks = torch.zeros(\n",
    "                2 * self.config.max_train_grids + 1,\n",
    "                self.config.max_grid_size,\n",
    "                self.config.max_grid_size,\n",
    "                dtype=torch.bool,\n",
    "            )\n",
    "\n",
    "            for i, pair in enumerate(challenge[\"train\"]):\n",
    "                if i >= self.config.max_train_grids:\n",
    "                    print(\n",
    "                        \"Training pairs exceed max\", task_id, i, self.config.max_train_grids\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    input_grid, input_mask = pad_and_mask_grid(pair[\"input\"], self.config)\n",
    "                    output_grid, output_mask = pad_and_mask_grid(\n",
    "                        pair[\"output\"], self.config\n",
    "                    )\n",
    "                    grids[2 * i] = input_grid\n",
    "                    masks[2 * i] = input_mask\n",
    "                    grids[2 * i + 1] = output_grid\n",
    "                    masks[2 * i + 1] = output_mask\n",
    "                except Exception as e:\n",
    "                    print(\"Got exception for training pair\", task_id, i, e)\n",
    "\n",
    "            try:\n",
    "                test_input_grid, test_input_mask = pad_and_mask_grid(\n",
    "                    test[\"input\"], self.config\n",
    "                )\n",
    "                grids[-1] = test_input_grid\n",
    "                masks[-1] = test_input_mask\n",
    "            except Exception as e:\n",
    "                print(\"Got exception on test input\", task_id, e)\n",
    "            \n",
    "            all_grids.append(grids)\n",
    "            all_masks.append(masks)\n",
    "\n",
    "        return {\"task_id\": task_id, \"grids\": torch.stack(all_grids), \"masks\": torch.stack(all_masks)}\n",
    "    \n",
    "\n",
    "def collate_arc_fn(\n",
    "    batch: list[dict],\n",
    ") -> tuple[list, torch.Tensor, torch.Tensor]:\n",
    "    task_ids = [item[\"task_id\"] for item in batch]\n",
    "    grids = torch.stack([item[\"grids\"] for item in batch])\n",
    "    masks = torch.stack([item[\"masks\"] for item in batch])\n",
    "\n",
    "    return (task_ids, grids, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ARCTransformerEncoderDecoderParams:\n",
    "    grid_dim: int\n",
    "    num_train_pairs: int\n",
    "    num_colors: int\n",
    "    num_encoder_layers: int\n",
    "    num_decoder_layers: int\n",
    "    num_heads: int\n",
    "    d_model: int\n",
    "    d_ff: int\n",
    "    dropout: float\n",
    "\n",
    "class ARCPositionalEncodingV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        grid_dim: int,\n",
    "        num_train_pairs: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.grid_dim = grid_dim\n",
    "        self.num_train_pairs = num_train_pairs\n",
    "\n",
    "        # Embeddings for row and column positions\n",
    "        self.row_embedding = nn.Embedding(self.grid_dim, self.d_model // 4)\n",
    "        self.col_embedding = nn.Embedding(self.grid_dim, self.d_model // 4)\n",
    "\n",
    "        # Embedding for input vs output\n",
    "        self.input_output_embedding = nn.Embedding(2, d_model // 4)\n",
    "\n",
    "        # Embedding for training pair index\n",
    "        self.pair_embedding = nn.Embedding(\n",
    "            self.num_train_pairs + 1, d_model // 4\n",
    "        )  # +1 for test pair\n",
    "\n",
    "    @torch.compiler.disable\n",
    "    def forward(\n",
    "        self, num_grids: int, grid_dim: int, device: torch.device\n",
    "    ) -> torch.Tensor:\n",
    "        grid_pos = torch.arange(grid_dim, device=device)\n",
    "\n",
    "        # Row pos embedding\n",
    "        row_emb = (\n",
    "            self.row_embedding.forward(grid_pos)\n",
    "            .unsqueeze(1)\n",
    "            .expand(num_grids, -1, grid_dim, -1)\n",
    "        )\n",
    "\n",
    "        # Column pos embedding\n",
    "        col_emb = (\n",
    "            self.col_embedding.forward(grid_pos)\n",
    "            .unsqueeze(0)\n",
    "            .expand(num_grids, grid_dim, -1, -1)\n",
    "        )\n",
    "\n",
    "        # Input/output embedding\n",
    "        grid_indices = torch.arange(num_grids, device=device)\n",
    "        is_output = (grid_indices % 2 == 1).long()\n",
    "        io_emb = (\n",
    "            self.input_output_embedding(is_output)\n",
    "            .unsqueeze(1)\n",
    "            .unsqueeze(1)\n",
    "            .expand(num_grids, grid_dim, grid_dim, -1)\n",
    "        )\n",
    "\n",
    "        # Pair embedding\n",
    "        pair_indices = torch.div(grid_indices, 2, rounding_mode=\"floor\")\n",
    "        pair_emb = (\n",
    "            self.pair_embedding(pair_indices)\n",
    "            .unsqueeze(1)\n",
    "            .unsqueeze(1)\n",
    "            .expand(num_grids, grid_dim, grid_dim, -1)\n",
    "        )\n",
    "\n",
    "        # Combine all embeddings (1, num_grids, height, width, d_model)\n",
    "        combined_emb = torch.cat([row_emb, col_emb, io_emb, pair_emb], dim=-1)\n",
    "\n",
    "        return combined_emb\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        patch_size: int,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Convolutional layer for patch embedding\n",
    "        self.conv_embed = nn.Conv2d(\n",
    "            in_channels=self.num_classes,\n",
    "            out_channels=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = (\n",
    "            F.one_hot(x.long(), num_classes=self.num_classes)\n",
    "            .permute(0, 3, 1, 2)\n",
    "            .float()\n",
    "        )\n",
    "\n",
    "        x = self.conv_embed(x)\n",
    "\n",
    "        x = x.permute(0, 2, 3, 1).reshape(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        mask = nn.functional.avg_pool2d(\n",
    "            mask.float(),\n",
    "            self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "        )\n",
    "\n",
    "        mask = (mask > 0).reshape(batch_size, -1)\n",
    "\n",
    "        return (x, mask)\n",
    "\n",
    "\n",
    "class EncoderLayerWithAttention(nn.TransformerEncoderLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: Optional[torch.Tensor] = None,\n",
    "        src_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        is_causal: bool = False,\n",
    "        need_weights: bool = False,\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        src_key_padding_mask = F._canonical_mask(\n",
    "            mask=src_key_padding_mask,\n",
    "            mask_name=\"src_key_padding_mask\",\n",
    "            other_type=F._none_or_dtype(src_mask),\n",
    "            other_name=\"src_mask\",\n",
    "            target_type=src.dtype,\n",
    "        )\n",
    "\n",
    "        src_mask = F._canonical_mask(\n",
    "            mask=src_mask,\n",
    "            mask_name=\"src_mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=src.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "        x = src\n",
    "        x1, attn_weights = self.self_attn(\n",
    "            x,\n",
    "            x,\n",
    "            x,\n",
    "            attn_mask=src_mask,\n",
    "            key_padding_mask=src_key_padding_mask,\n",
    "            need_weights=need_weights,\n",
    "            is_causal=is_causal,\n",
    "            average_attn_weights=False,\n",
    "        )\n",
    "\n",
    "        x = x + self.dropout1(x1)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x1 = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        x = x + self.dropout2(x1)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "class EncoderWithAttention(nn.TransformerEncoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_layer: \"EncoderLayerWithAttention\",\n",
    "        num_layers: int,\n",
    "    ) -> None:\n",
    "        super().__init__(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        src_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        need_weights: bool = False,\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        src_key_padding_mask = F._canonical_mask(\n",
    "            mask=src_key_padding_mask,\n",
    "            mask_name=\"src_key_padding_mask\",\n",
    "            other_type=F._none_or_dtype(mask),\n",
    "            other_name=\"mask\",\n",
    "            target_type=src.dtype,\n",
    "        )\n",
    "\n",
    "        mask = F._canonical_mask(\n",
    "            mask=mask,\n",
    "            mask_name=\"mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=src.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "        output = src\n",
    "        attn_weights = []\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output, layer_attn_weights = mod(\n",
    "                output,\n",
    "                src_mask=mask,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                need_weights=need_weights,\n",
    "            )\n",
    "            if need_weights:\n",
    "                attn_weights.append(layer_attn_weights)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output, (torch.stack(attn_weights, dim=1) if need_weights else None)\n",
    "    \n",
    "class ARCVisionEncoder(nn.Module):\n",
    "    def __init__(self, params: ARCTransformerEncoderDecoderParams):\n",
    "        super().__init__()\n",
    "        self.grid_dim = params.grid_dim\n",
    "        self.num_train_pairs = params.num_train_pairs\n",
    "        self.num_classes = params.num_colors + 1\n",
    "        self.d_model = params.d_model\n",
    "        self.num_layers = params.num_encoder_layers\n",
    "        self.num_heads = params.num_heads\n",
    "        self.d_ff = params.d_ff\n",
    "        self.dropout = params.dropout\n",
    "        self.patch_size = 2\n",
    "\n",
    "        self.patch_grid_dim = self.grid_dim // self.patch_size\n",
    "\n",
    "        self.input_seq_len = (\n",
    "            (self.num_train_pairs * 2 + 1) * self.patch_grid_dim * self.patch_grid_dim\n",
    "        )\n",
    "        self.output_seq_len = self.grid_dim * self.grid_dim\n",
    "        self.seq_len = self.input_seq_len + self.output_seq_len\n",
    "\n",
    "        self.embedding = PatchEmbedding(\n",
    "            num_classes=self.num_classes,\n",
    "            patch_size=self.patch_size,\n",
    "            embed_dim=self.d_model,\n",
    "        )\n",
    "        self.tgt_embedding = nn.Embedding(self.num_classes, self.d_model)\n",
    "        self.pos_encoding = ARCPositionalEncodingV2(\n",
    "            d_model=self.d_model,\n",
    "            grid_dim=self.grid_dim,\n",
    "            num_train_pairs=self.num_train_pairs,\n",
    "        )\n",
    "\n",
    "        encoder_layer = EncoderLayerWithAttention(\n",
    "            self.d_model, self.num_heads, self.d_ff, self.dropout\n",
    "        )\n",
    "        self.encoder = EncoderWithAttention(encoder_layer, self.num_layers)\n",
    "\n",
    "        self.output_query = nn.Parameter(\n",
    "            torch.randn(1, 1, self.grid_dim, self.grid_dim, self.d_model)\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.d_model, self.num_classes)\n",
    "\n",
    "    def embed(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        tgt: Optional[torch.Tensor] = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, num_grids, grid_dim, grid_dim = src.shape\n",
    "\n",
    "        pos_emb = self.pos_encoding.forward(\n",
    "            num_grids=num_grids + 1, grid_dim=grid_dim, device=src.device\n",
    "        )\n",
    "\n",
    "        src = src.reshape(batch_size, num_grids * grid_dim, grid_dim)\n",
    "        src_mask = src_mask.reshape(batch_size, num_grids * grid_dim, grid_dim)\n",
    "\n",
    "        src_patched, mask_patched = self.embedding.forward(src, src_mask)\n",
    "\n",
    "        input_pos_emb = pos_emb[:-1, :, :, :]\n",
    "        input_pos_emb_patched = (\n",
    "            input_pos_emb.unfold(1, self.patch_size, self.patch_size)\n",
    "            .unfold(2, self.patch_size, self.patch_size)\n",
    "            .mean(dim=(-2, -1))\n",
    "        )\n",
    "\n",
    "        src_patched = src_patched.reshape(batch_size, -1, self.d_model)\n",
    "        input_pos_emb_patched = input_pos_emb_patched.reshape(-1, self.d_model)\n",
    "        input_seq = src_patched + input_pos_emb_patched\n",
    "\n",
    "        if tgt is not None:\n",
    "            output_query = self.tgt_embedding.forward(tgt).view(\n",
    "                batch_size, 1, self.grid_dim, self.grid_dim, self.d_model\n",
    "            )\n",
    "        else:\n",
    "            output_query = self.output_query.expand(batch_size, -1, -1, -1, -1)\n",
    "        output_pos_emb = pos_emb[-1:, :, :, :]\n",
    "\n",
    "        output_query = output_query.reshape(batch_size, -1, self.d_model)\n",
    "        output_pos_emb = output_pos_emb.reshape(-1, self.d_model)\n",
    "        output_seq = output_query + output_pos_emb\n",
    "\n",
    "        combined_seq = torch.cat([input_seq, output_seq], dim=1)\n",
    "\n",
    "        # Make padding mask\n",
    "        padding_mask = ~mask_patched\n",
    "        padding_mask = torch.cat(\n",
    "            [\n",
    "                padding_mask,\n",
    "                torch.zeros(\n",
    "                    (batch_size, self.grid_dim**2), dtype=torch.bool, device=src.device\n",
    "                ),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return combined_seq, padding_mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        tgt: Optional[torch.Tensor] = None,\n",
    "        temperature: float = 0.0,\n",
    "        need_weights: bool = False,\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        combined_seq, padding_mask = self.embed(src, src_mask, tgt)\n",
    "\n",
    "        # Make causal mask\n",
    "        causal_mask = torch.zeros(self.seq_len, self.seq_len, device=src.device)\n",
    "        causal_mask[: self.input_seq_len, self.input_seq_len :] = 1\n",
    "        causal_mask = causal_mask.bool()\n",
    "\n",
    "        output, attn_weights = self.encoder.forward(\n",
    "            combined_seq,\n",
    "            mask=causal_mask,\n",
    "            src_key_padding_mask=padding_mask,\n",
    "            need_weights=need_weights,\n",
    "        )\n",
    "\n",
    "        output_grid_portion = output[:, -self.output_seq_len :, :]\n",
    "\n",
    "        logits = self.output_layer.forward(output_grid_portion)\n",
    "\n",
    "        output = logits.view(batch_size, self.grid_dim, self.grid_dim, self.num_classes)\n",
    "\n",
    "        if temperature > 0:\n",
    "            output = output / temperature\n",
    "\n",
    "        return (output, attn_weights)\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        tgt: Optional[torch.Tensor] = None,\n",
    "        temperature: float = 0.0,\n",
    "        need_weights: bool = False,\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            (\n",
    "                output,\n",
    "                encoder_attn_weights,\n",
    "            ) = self.forward(src, src_mask, tgt=tgt, temperature=temperature)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(output, dim=-1)\n",
    "            prediction = torch.multinomial(\n",
    "                probs.view(-1, probs.size(-1)),\n",
    "                num_samples=1,\n",
    "                replacement=True,\n",
    "            ).view(-1, *probs.size()[:-1])\n",
    "        else:\n",
    "            prediction = torch.argmax(output, dim=-1)\n",
    "        return prediction, encoder_attn_weights\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ARCKaggleModelState:\n",
    "    model_params: ARCTransformerEncoderDecoderParams\n",
    "    model_state_dict: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "config = ARCDatasetParams(max_grid_size=20, max_train_grids=4, color_offset=1)\n",
    "test_dataset = ARCKaggleDataset(challenges_file=test_challenges_file_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_params = ARCTransformerEncoderDecoderParams(grid_dim=20, num_train_pairs=4, num_colors=10, num_encoder_layers=24, num_decoder_layers=0, num_heads=16, d_model=1024, d_ff=1024*4, dropout=0.1)\n",
    "model = ARCVisionEncoder(model_params).to(device)\n",
    "model.load_state_dict(torch.load(kaggle_model_file_path, map_location=device, weights_only=True))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def unpad_output(output: torch.Tensor) -> torch.Tensor:\n",
    "    output = output - 1\n",
    "    filtered_rows = []\n",
    "    for row in output:\n",
    "        filtered_row = row[row != -1]\n",
    "        if len(filtered_row) > 0:\n",
    "            filtered_rows.append(filtered_row)\n",
    "    # Hack to ensure there's always at least one value\n",
    "    if len(filtered_rows) == 0:\n",
    "        filtered_rows.append(torch.zeros(1, device=device, dtype=torch.long))\n",
    "    max_length = max(len(row) for row in filtered_rows)\n",
    "    padded_rows = [torch.cat([row, torch.zeros(max_length - len(row), dtype=row.dtype, device=device)]) for row in filtered_rows]\n",
    "    return torch.stack(padded_rows)\n",
    "\n",
    "kaggle_output = {}\n",
    "\n",
    "for task in test_dataset:\n",
    "    print(f\"Starting {task['task_id']}\")\n",
    "    task_predictions = []\n",
    "    for grids, masks in zip(task[\"grids\"], task[\"masks\"]):\n",
    "        prediction = model.generate(grids.unsqueeze(0).to(device), masks.unsqueeze(0).to(device), need_weights=False)[0][0]\n",
    "        list_prediction = unpad_output(prediction).cpu().numpy().tolist()\n",
    "        task_predictions.append({\n",
    "            \"attempt_1\":  list_prediction,\n",
    "            \"attempt_2\": list_prediction\n",
    "        })\n",
    "    kaggle_output[task[\"task_id\"]] = task_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(kaggle_submission_file_path, \"w\") as f:\n",
    "  json.dump(kaggle_output, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
