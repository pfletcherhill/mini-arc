{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def evaluate_dataset_dims(folder: str = \"data/re_arc\", cutoff_dim: Optional[int] = None) -> None:\n",
    "    if cutoff_dim is not None:\n",
    "        total_below_cutoff = 0\n",
    "        total_above_cutoff = 0\n",
    "    for i, filename in enumerate(sorted(os.listdir(f\"{folder}\"))):\n",
    "        key = filename.replace(\".json\", \"\")\n",
    "        with open(f\"{folder}/{key}.json\", \"r\") as fp:\n",
    "            generated_task = json.load(fp)\n",
    "\n",
    "            count_by_dim: dict[int, int] = {}\n",
    "            for pair in generated_task:\n",
    "                max_dim = max(\n",
    "                    [\n",
    "                        len(pair[\"input\"]),\n",
    "                        len(pair[\"output\"]),\n",
    "                        len(pair[\"input\"][0]),\n",
    "                        len(pair[\"output\"][0]),\n",
    "                    ]\n",
    "                )\n",
    "                count = count_by_dim.get(max_dim, 0)\n",
    "                count_by_dim[max_dim] = count + 1\n",
    "            \n",
    "            print(key)\n",
    "            if cutoff_dim is not None:\n",
    "                below_cutoff = 0\n",
    "                above_cutoff = 0\n",
    "                for k, v in count_by_dim.items():\n",
    "                    if k <= cutoff_dim:\n",
    "                        below_cutoff += v\n",
    "                    else:\n",
    "                        above_cutoff += v\n",
    "                total_below_cutoff += below_cutoff\n",
    "                total_above_cutoff += above_cutoff\n",
    "                print(\"Cutoff\", cutoff_dim, \"Below\", below_cutoff, \"Above\", above_cutoff)\n",
    "\n",
    "            print(count_by_dim)\n",
    "    \n",
    "    if cutoff_dim is not None:\n",
    "        print(\"Total cutoff\", cutoff_dim, \"Below\", total_below_cutoff, \"Above\", total_above_cutoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from arc_prize.synth_data.utils import ChallengeTask, DatasetTasks, GridInput, GridPair\n",
    "\n",
    "def split_list(input_list: list, min_size: int = 2, max_size: int = 5) -> list[list]:\n",
    "    result = []\n",
    "    while len(input_list) >= min_size:\n",
    "        # Choose a random size between 2 and 5, but not larger than the remaining list\n",
    "        size = min(random.randint(min_size, max_size), len(input_list))\n",
    "        # Append a slice of the input list to the result\n",
    "        result.append(input_list[:size])\n",
    "        # Remove the slice from the input list\n",
    "        input_list = input_list[size:]\n",
    "    return result\n",
    "\n",
    "def make_challenge_and_solution(task_pairs: list[GridPair]) -> tuple[ChallengeTask, list[list]]:\n",
    "    train_pairs = task_pairs[:-1]\n",
    "    test_pair = task_pairs[-1]\n",
    "    return (ChallengeTask(train=train_pairs, test=[GridInput(input=test_pair.input)]), [test_pair.output])\n",
    "\n",
    "def create_examples_with_dataset_dim(folder: str = \"data/re_arc\", cutoff_dim: int = 15, eval_split: float = 0.25, max_per_task: Optional[int] = None, output_folder: str = \"data/re_arc_pruned\") -> None:\n",
    "    train_challenges: dict[str, ChallengeTask] = {}\n",
    "    train_solutions: dict[str, list] = {}\n",
    "    eval_challenges: dict[str, ChallengeTask] = {}\n",
    "    eval_solutions: dict[str, list] = {}\n",
    "    for i, filename in enumerate(sorted(os.listdir(f\"{folder}\"))):\n",
    "        key = filename.replace(\".json\", \"\")\n",
    "        print(f\"Starting {key}\")\n",
    "        with open(f\"{folder}/{key}.json\", \"r\") as fp:\n",
    "            generated_task = json.load(fp)\n",
    "            if max_per_task is not None:\n",
    "                generated_task = generated_task[:max_per_task]\n",
    "            valid_pairs: list[GridPair] = []\n",
    "            for pair in generated_task:\n",
    "                max_dim = max(\n",
    "                    [\n",
    "                        len(pair[\"input\"]),\n",
    "                        len(pair[\"output\"]),\n",
    "                        len(pair[\"input\"][0]),\n",
    "                        len(pair[\"output\"][0]),\n",
    "                    ]\n",
    "                )\n",
    "                if max_dim <= cutoff_dim:\n",
    "                    valid_pairs.append(GridPair(input=pair[\"input\"], output=pair[\"output\"]))\n",
    "            eval_pairs_cutoff = int(len(valid_pairs) * eval_split)\n",
    "            random.shuffle(valid_pairs)\n",
    "            eval_pairs = split_list(valid_pairs[:eval_pairs_cutoff])\n",
    "            train_pairs = split_list(valid_pairs[eval_pairs_cutoff:])\n",
    "            for i, train_pair in enumerate(train_pairs):\n",
    "                challenge, solution = make_challenge_and_solution(train_pair)\n",
    "                train_challenges[f\"{key}_{i}\"] = challenge\n",
    "                train_solutions[f\"{key}_{i}\"] = solution\n",
    "            for i, eval_pair in enumerate(eval_pairs):\n",
    "                challenge, solution = make_challenge_and_solution(eval_pair)\n",
    "                eval_challenges[f\"{key}_{i}\"] = challenge\n",
    "                eval_solutions[f\"{key}_{i}\"] = solution\n",
    "    print(\"Train challenges\", len(train_challenges), \"Eval challenges\", len(eval_challenges))\n",
    "\n",
    "    training_tasks = DatasetTasks(challenges=train_challenges, solutions=train_solutions)\n",
    "    eval_tasks = DatasetTasks(challenges=eval_challenges, solutions=eval_solutions)\n",
    "    \n",
    "    with open(f\"{output_folder}/training_challenges.json\", \"w\") as f:\n",
    "        json.dump(training_tasks.to_dict()[\"challenges\"], f)\n",
    "    with open(f\"{output_folder}/training_solutions.json\", \"w\") as f:\n",
    "        json.dump(training_tasks.to_dict()[\"solutions\"], f)\n",
    "    with open(f\"{output_folder}/evaluation_challenges.json\", \"w\") as f:\n",
    "        json.dump(eval_tasks.to_dict()[\"challenges\"], f)\n",
    "    with open(f\"{output_folder}/evaluation_solutions.json\", \"w\") as f:\n",
    "        json.dump(eval_tasks.to_dict()[\"solutions\"], f)\n",
    "\n",
    "create_examples_with_dataset_dim(folder=\"/Users/pfh/work/arc-data/re_arc_5k\", cutoff_dim=30, max_per_task=2500, eval_split=0.2, output_folder=\"/Users/pfh/work/arc-data/re_arc_dim_30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audited = []\n",
    "\n",
    "with open(\"/Users/pfh/work/arc-data/html/12997ef3.json\", \"r\") as f:\n",
    "  data = json.load(f)  \n",
    "  for puzzle in data:\n",
    "    audit = True\n",
    "    for pair in puzzle:\n",
    "      if len(pair) != 2:\n",
    "        audit = False\n",
    "    if audit is True:\n",
    "      audited.append(puzzle)\n",
    "\n",
    "print(len(audited))\n",
    "\n",
    "with open(\"/Users/pfh/work/arc-data/html/12997ef3.json\", \"w\") as f:\n",
    "  json.dump(audited, f)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from arc_prize.synth_data.utils import ChallengeTask, DatasetTasks, GridInput, GridPair\n",
    "\n",
    "\n",
    "def make_challenge_and_solution_from_html(task_pairs: list[list]) -> tuple[ChallengeTask, list[list]]:\n",
    "    train_pairs = [GridPair(input=input, output=output) for (input, output) in task_pairs[:-1]]\n",
    "    test_pair = GridPair(input=task_pairs[-1][0], output=task_pairs[-1][1])\n",
    "    return (ChallengeTask(train=train_pairs, test=[GridInput(input=test_pair.input)]), [test_pair.output])\n",
    "\n",
    "def create_examples_from_html(folder: str = \"data/html\", max_dim: int = 30, eval_split: float = 0.25, output_folder: str = \"data/re_arc_pruned\") -> None:\n",
    "    train_challenges: dict[str, ChallengeTask] = {}\n",
    "    train_solutions: dict[str, list] = {}\n",
    "    eval_challenges: dict[str, ChallengeTask] = {}\n",
    "    eval_solutions: dict[str, list] = {}\n",
    "    for i, filename in enumerate(sorted(os.listdir(f\"{folder}\"))):\n",
    "        if filename[0] == \".\":\n",
    "            continue\n",
    "        key = filename.replace(\".json\", \"\")\n",
    "        print(f\"Starting {key}\")\n",
    "        with open(f\"{folder}/{key}.json\", \"r\") as fp:\n",
    "            generated_tasks = json.load(fp)\n",
    "            valid_tasks = []\n",
    "            for task in generated_tasks:\n",
    "                # for task in puzzle:\n",
    "                task_dim = 0\n",
    "                for pair in task:\n",
    "                    pair_dim = max(\n",
    "                        [\n",
    "                            len(pair[0]),\n",
    "                            len(pair[1]),\n",
    "                            len(pair[0][0]),\n",
    "                            len(pair[1][0]),\n",
    "                        ]\n",
    "                    )\n",
    "                    if pair_dim > task_dim:\n",
    "                        task_dim = pair_dim\n",
    "                if task_dim <= max_dim:\n",
    "                    valid_tasks.append(task)\n",
    "            random.shuffle(valid_tasks)\n",
    "            eval_cutoff = int(len(valid_tasks) * eval_split)\n",
    "            eval_tasks = valid_tasks[:eval_cutoff]\n",
    "            train_tasks = valid_tasks[eval_cutoff:]\n",
    "            for i, train_pair in enumerate(train_tasks):\n",
    "                challenge, solution = make_challenge_and_solution_from_html(train_pair)\n",
    "                train_challenges[f\"{key}_{i}\"] = challenge\n",
    "                train_solutions[f\"{key}_{i}\"] = solution\n",
    "            for i, eval_pair in enumerate(eval_tasks):\n",
    "                challenge, solution = make_challenge_and_solution_from_html(eval_pair)\n",
    "                eval_challenges[f\"{key}_{i}\"] = challenge\n",
    "                eval_solutions[f\"{key}_{i}\"] = solution\n",
    "    print(\"Train challenges\", len(train_challenges), \"Eval challenges\", len(eval_challenges))\n",
    "\n",
    "    training_tasks = DatasetTasks(challenges=train_challenges, solutions=train_solutions)\n",
    "    eval_tasks = DatasetTasks(challenges=eval_challenges, solutions=eval_solutions)\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    with open(f\"{output_folder}/training_challenges.json\", \"w\") as f:\n",
    "        json.dump(training_tasks.to_dict()[\"challenges\"], f)\n",
    "    with open(f\"{output_folder}/training_solutions.json\", \"w\") as f:\n",
    "        json.dump(training_tasks.to_dict()[\"solutions\"], f)\n",
    "    with open(f\"{output_folder}/evaluation_challenges.json\", \"w\") as f:\n",
    "        json.dump(eval_tasks.to_dict()[\"challenges\"], f)\n",
    "    with open(f\"{output_folder}/evaluation_solutions.json\", \"w\") as f:\n",
    "        json.dump(eval_tasks.to_dict()[\"solutions\"], f)\n",
    "\n",
    "create_examples_from_html(folder=\"/Users/pfh/work/arc-data/html\", max_dim=12, eval_split=0.15, output_folder=\"/Users/pfh/work/arc-data/html_dim_12_20241108\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.synth_data.utils import ChallengeTask, GridInput, GridPair\n",
    "\n",
    "\n",
    "def make_challenge_and_solution_from_dataset(task_pairs: list[list]) -> tuple[ChallengeTask, list[list]]:\n",
    "    train_pairs = [GridPair(input=input, output=output) for (input, output) in task_pairs[:-1]]\n",
    "    test_pair = GridPair(input=task_pairs[-1][0], output=task_pairs[-1][1])\n",
    "    return (ChallengeTask(train=train_pairs, test=[GridInput(input=test_pair.input)]), [test_pair.output])\n",
    "\n",
    "def create_examples_from_dataset(challenges_filename: str, solutions_filename: str, output_folder: str, max_dim: int = 30, eval_split: float = 0.25) -> None:\n",
    "    train_challenges: dict[str, ChallengeTask] = {}\n",
    "    train_solutions: dict[str, list] = {}\n",
    "    eval_challenges: dict[str, ChallengeTask] = {}\n",
    "    eval_solutions: dict[str, list] = {}\n",
    "\n",
    "    with open(challenges_filename, \"r\") as f:\n",
    "        challenges = json.load(f)\n",
    "    with open(solutions_filename, \"r\") as f:\n",
    "        solutions = json.load(f)\n",
    "\n",
    "    valid_tasks: dict[str, list] = {}\n",
    "    for task_id, task in challenges.items():\n",
    "        pair_list = []\n",
    "        task_dim = 0\n",
    "        for pair in task[\"train\"]:\n",
    "            pair_dim = max(\n",
    "                [\n",
    "                    len(pair[\"input\"]),\n",
    "                    len(pair[\"output\"]),\n",
    "                    len(pair[\"input\"][0]),\n",
    "                    len(pair[\"output\"][0]),\n",
    "                ]\n",
    "            )\n",
    "            if pair_dim > task_dim:\n",
    "                task_dim = pair_dim\n",
    "            if pair_dim <= max_dim and len(pair_list) < 4:\n",
    "                pair_list.append([pair[\"input\"], pair[\"output\"]])\n",
    "        for input, output in zip(task[\"test\"][:1], solutions[task_id][:1]):\n",
    "            pair_dim = max([len(input[\"input\"]), len(input[\"input\"][0]), len(output), len(output[0])])\n",
    "            if pair_dim > task_dim:\n",
    "                task_dim = pair_dim\n",
    "            if pair_dim <= max_dim:\n",
    "                pair_list.append([input[\"input\"], output])\n",
    "        if task_dim <= max_dim:\n",
    "            valid_tasks[task_id] = pair_list\n",
    "    task_ids = list(valid_tasks.keys())\n",
    "    random.shuffle(task_ids)\n",
    "    eval_cutoff = int(len(valid_tasks) * eval_split)\n",
    "    eval_tasks = task_ids[:eval_cutoff]\n",
    "    train_tasks = task_ids[eval_cutoff:]\n",
    "    for task_id in train_tasks:\n",
    "        challenge, solution = make_challenge_and_solution_from_dataset(valid_tasks[task_id])\n",
    "        if len(challenge.train) > 4:\n",
    "            print(\"Uh oh\", task_id, challenge.train)\n",
    "        train_challenges[task_id] = challenge\n",
    "        train_solutions[task_id] = solution\n",
    "    for task_id in eval_tasks:\n",
    "        challenge, solution = make_challenge_and_solution_from_dataset(valid_tasks[task_id])\n",
    "        eval_challenges[task_id] = challenge\n",
    "        eval_solutions[task_id] = solution\n",
    "    print(\"Train challenges\", len(train_challenges), \"Eval challenges\", len(eval_challenges))\n",
    "\n",
    "    training_tasks = DatasetTasks(challenges=train_challenges, solutions=train_solutions)\n",
    "    eval_tasks = DatasetTasks(challenges=eval_challenges, solutions=eval_solutions)\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    with open(f\"{output_folder}/training_challenges.json\", \"w\") as f:\n",
    "        json.dump(training_tasks.to_dict()[\"challenges\"], f)\n",
    "    with open(f\"{output_folder}/training_solutions.json\", \"w\") as f:\n",
    "        json.dump(training_tasks.to_dict()[\"solutions\"], f)\n",
    "    with open(f\"{output_folder}/evaluation_challenges.json\", \"w\") as f:\n",
    "        json.dump(eval_tasks.to_dict()[\"challenges\"], f)\n",
    "    with open(f\"{output_folder}/evaluation_solutions.json\", \"w\") as f:\n",
    "        json.dump(eval_tasks.to_dict()[\"solutions\"], f)\n",
    "\n",
    "create_examples_from_dataset(challenges_filename=\"data/arc-agi_evaluation_challenges.json\", solutions_filename=\"data/arc-agi_evaluation_solutions.json\", max_dim=12, eval_split=0.0, output_folder=\"/Users/pfh/work/arc-data/eval_dim_12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_prize.data import convert_to_chunked_format\n",
    "\n",
    "data_dirname = \"barc_2_dim_12\"\n",
    "chunk_size = 500\n",
    "\n",
    "data_dir = f\"/Users/pfh/work/arc-data/{data_dirname}\"\n",
    "output_dir = f\"/Users/pfh/work/arc-data/chunked/{data_dirname}\"\n",
    "\n",
    "challenges_filename = f\"{data_dir}/training_challenges.json\"\n",
    "solutions_filename = f\"{data_dir}/training_solutions.json\"\n",
    "train_output_dir = f\"{output_dir}/training\"\n",
    "\n",
    "val_challenges_filename = f\"{data_dir}/evaluation_challenges.json\"\n",
    "val_solutions_filename = f\"{data_dir}/evaluation_solutions.json\"\n",
    "val_output_dir = f\"{output_dir}/evaluation\"\n",
    "\n",
    "convert_to_chunked_format(challenges_file=challenges_filename, output_dir=train_output_dir, chunk_size=chunk_size, solutions_file=solutions_filename)\n",
    "convert_to_chunked_format(challenges_file=val_challenges_filename, output_dir=val_output_dir, chunk_size=chunk_size, solutions_file=val_solutions_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenges = {}\n",
    "solutions = {}\n",
    "\n",
    "train_challenges_file = \"data/arc/training_challenges.json\"\n",
    "train_solutions_file = \"data/arc/training_solutions.json\"\n",
    "eval_challenges_file = \"data/arc/evaluation_challenges.json\"\n",
    "eval_solutions_file = \"data/arc/evaluation_solutions.json\"\n",
    "\n",
    "with open(train_challenges_file, \"r\") as f:\n",
    "  for k, v in json.load(f).items():\n",
    "    challenges[k] = v\n",
    "with open(train_solutions_file, \"r\") as f:\n",
    "  for k, v in json.load(f).items():\n",
    "    solutions[k] = v\n",
    "with open(eval_challenges_file, \"r\") as f:\n",
    "  for k, v in json.load(f).items():\n",
    "    challenges[k] = v\n",
    "with open(eval_solutions_file, \"r\") as f:\n",
    "  for k, v in json.load(f).items():\n",
    "    solutions[k] = v\n",
    "\n",
    "task_ids = list(challenges.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = {}\n",
    "for task_id in task_ids:\n",
    "  challenge = challenges[task_id]\n",
    "  solution = solutions[task_id]\n",
    "  test = challenge[\"test\"]\n",
    "  for i, test_pair in enumerate(test):\n",
    "    test[i][\"output\"] = solution[i]\n",
    "  challenge[\"test\"] = test\n",
    "  combined[task_id] = challenge\n",
    "\n",
    "combined_file = \"data/arc/combined.json\"\n",
    "with open(combined_file, \"w\") as f:\n",
    "  json.dump(combined, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "keys = set()\n",
    "\n",
    "with open(\"/Users/pfh/work/arc-data/re_arc_dim_12/training_challenges.json\", \"r\") as f:\n",
    "  challenges = json.load(f)\n",
    "  print(len(challenges)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import comb, factorial\n",
    "import itertools\n",
    "\n",
    "n = 4\n",
    "arr = [\"a\", \"b\", \"c\", \"d\"]\n",
    "tasks = []\n",
    "for length in range(3, len(arr) + 1):\n",
    "            for combination in itertools.combinations(arr, length):\n",
    "                for permutation in itertools.permutations(combination):\n",
    "                    tasks.append(list(permutation))\n",
    "\n",
    "print(len(tasks))\n",
    "for item in tasks:\n",
    "       print(item)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
